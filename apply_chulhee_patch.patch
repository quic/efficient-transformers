diff --git a/QEfficient/base/modeling_qeff.py b/QEfficient/base/modeling_qeff.py
index 119ec59..7142e22 100644
--- a/QEfficient/base/modeling_qeff.py
+++ b/QEfficient/base/modeling_qeff.py
@@ -20,7 +20,7 @@ import onnx
 import torch
 
 from QEfficient.base.onnx_transforms import OnnxTransform
-from QEfficient.base.pytorch_transforms import PytorchTransform
+from QEfficient.base.pytorch_transforms import PytorchTransform, append_tranform
 from QEfficient.compile.qnn_compiler import compile as qnn_compile
 from QEfficient.generation.cloud_infer import QAICInferenceSession
 from QEfficient.utils import constants, dump_qconfig
@@ -46,6 +46,7 @@ class QEFFBaseModel(ABC):
     def _transform_names(cls) -> List[str]:
         return [x.__name__ for x in cls._pytorch_transforms + cls._onnx_transforms]
 
+    @append_tranform
     def __init__(self, model: torch.nn.Module) -> None:
         super().__init__()
         self.model = model
diff --git a/QEfficient/base/pytorch_transforms.py b/QEfficient/base/pytorch_transforms.py
index abd19ed..4354e20 100644
--- a/QEfficient/base/pytorch_transforms.py
+++ b/QEfficient/base/pytorch_transforms.py
@@ -9,6 +9,8 @@ from typing import Callable, Dict, Tuple, Type
 
 from torch import nn
 
+from QEfficient.utils.logging_utils import logger
+
 
 class PytorchTransform:
     """
@@ -110,3 +112,66 @@ class ModuleMethodMapperTransform(PytorchTransform):
                     transformed = True
 
         return model, transformed
+
+
+class SplitGateUpWeightsTransform(PytorchTransform):
+    """
+    split fused Gate+Up weights and copy into the model
+
+    For every transformer layer inside `model`:
+      • expects   <PREFIX>.experts.gate_up_proj   in the *source* `sd`
+      • copies halves into
+            <PREFIX>.experts.gate_proj     <-- Gate   [E,H,I]
+            <PREFIX>.experts.up_proj       <-- Up     [E,H,I]
+    """
+
+    @classmethod
+    def apply(cls, model: nn.Module) -> Tuple[nn.Module, bool]:
+        transformed = False
+
+        model = model.language_model if hasattr(model, "language_model") else model
+
+        num_layers = len(model.model.layers)
+        delete_fused_key = True
+        sd = model.state_dict()
+        for layer_idx in range(num_layers):
+            # ---- build the textual prefix once per layer ----------
+            prefix = f"model.layers.{layer_idx}.feed_forward.experts."
+
+            fused_key = prefix + "gate_up_proj"
+            gate_key = prefix + "gate_proj"
+            up_key = prefix + "up_proj"
+
+            # ---- split  [E,H,2I] → two  [E,H,I]  tensors ----------------------
+            fused = sd[fused_key]  # [E, H, 2I]  (no .weight here)
+            E, H, two_I = fused.shape
+            ffn_dim = two_I // 2
+            gate, up = fused.split(ffn_dim, dim=-1)  # views – no copy
+
+            experts = model.model.layers[layer_idx].feed_forward.experts
+            experts.gate_proj.data.copy_(gate)
+            experts.up_proj.data.copy_(up)
+
+            # ---- update the state-dict so load_state_dict sees the right keys
+            sd[gate_key] = gate
+            sd[up_key] = up
+
+            if delete_fused_key:
+                del sd[fused_key]
+
+            logger.info(f"[layer {layer_idx:02d}] loaded gate_proj & up_proj from fused tensor  (shape {fused.shape})")
+            transformed = True
+        return model, transformed
+
+
+VLM_SPLIT_GATE_UP_WEIGHTS = ["Llama4ForConditionalGeneration", "Llama4TextModel"]
+
+
+def append_tranform(func):
+    def wrapper(*args, **kwargs):
+        model_class = args[1].model.__class__.__name__ if hasattr(args[1], "model") else args[1].__class__.__name__
+        if model_class in VLM_SPLIT_GATE_UP_WEIGHTS:
+            args[0]._pytorch_transforms.append(SplitGateUpWeightsTransform)
+        return func(*args, **kwargs)
+
+    return wrapper
diff --git a/QEfficient/transformers/models/llama4/__init__.py b/QEfficient/transformers/models/llama4/__init__.py
new file mode 100644
index 0000000..72ba36c
--- /dev/null
+++ b/QEfficient/transformers/models/llama4/__init__.py
@@ -0,0 +1,6 @@
+# -----------------------------------------------------------------------------
+#
+# Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+# SPDX-License-Identifier: BSD-3-Clause
+#
+# -----------------------------------------------------------------------------
diff --git a/QEfficient/transformers/models/llama4/modeling_llama4.py b/QEfficient/transformers/models/llama4/modeling_llama4.py
new file mode 100644
index 0000000..fde1da3
--- /dev/null
+++ b/QEfficient/transformers/models/llama4/modeling_llama4.py
@@ -0,0 +1,1069 @@
+# -----------------------------------------------------------------------------
+#
+# Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+# SPDX-License-Identifier: BSD-3-Clause
+#
+# -----------------------------------------------------------------------------
+
+import math
+from typing import Callable, List, Optional, Tuple, Union
+
+import torch
+from torch import nn
+from transformers.cache_utils import Cache, DynamicCache
+from transformers.modeling_outputs import (
+    BaseModelOutput,
+    BaseModelOutputWithPast,
+    CausalLMOutputWithPast,
+)
+from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS
+from transformers.models.llama4.modeling_llama4 import (
+    Llama4ForCausalLM,
+    Llama4ForConditionalGeneration,
+    Llama4TextAttention,
+    Llama4TextConfig,
+    Llama4TextDecoderLayer,
+    Llama4TextExperts,
+    Llama4TextModel,
+    Llama4TextMoe,
+    Llama4VisionAttention,
+    Llama4VisionModel,
+    logger,
+    repeat_kv,
+)
+
+from QEfficient.transformers.modeling_attn_mask_utils import _create_causal_mask
+from QEfficient.utils import constants
+from QEfficient.utils._utils import IOInfo, get_padding_shape_from_config
+
+
+def eager_attention_forward_vision(
+    module: nn.Module,
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    attention_mask: Optional[torch.Tensor],
+    scaling: float,
+    dropout: float = 0.0,
+    **kwargs,
+):
+    key_states = repeat_kv(key, module.num_key_value_groups)
+    value_states = repeat_kv(value, module.num_key_value_groups)
+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(module.head_dim)
+    if attention_mask is not None:
+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
+        attn_weights = attn_weights + causal_mask
+    if attention_mask is not None:
+        attn_weights = torch.where(attention_mask, torch.tensor(-10000.0, dtype=torch.float32), attn_weights)
+
+    attn_weights = nn.functional.softmax(attn_weights.float(), dim=-1).to(query.dtype)
+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
+    attn_output = torch.matmul(attn_weights, value_states)
+    attn_output = attn_output.transpose(1, 2).contiguous()
+
+    return attn_output, attn_weights
+
+
+def complex_mul_onnx_safe(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
+    a_real, a_imag = a.unbind(dim=-1)
+    b_real, b_imag = b.unbind(dim=-1)
+
+    real = a_real * b_real - a_imag * b_imag
+    imag = a_real * b_imag + a_imag * b_real
+
+    return torch.stack((real, imag), dim=-1)
+
+
+def qeff_vision_apply_rotary_emb(
+    query: torch.Tensor, key: torch.Tensor, freqs_ci: torch.Tensor
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    query_in = query.view(*query.shape[:-1], -1, 2)
+    key_in = key.view(*key.shape[:-1], -1, 2)
+
+    # freqs_ci: [L, 1, D, 2] => [1, 1, L, D, 2] (broadcasted to query shape)
+    freqs_ci = freqs_ci.unsqueeze(0).unsqueeze(0)  # [1,1,L,D,2]
+
+    # Apply rotary: elementwise complex multiplication
+    query_out = complex_mul_onnx_safe(query_in, freqs_ci)
+    key_out = complex_mul_onnx_safe(key_in, freqs_ci)
+
+    query_out = query_out.reshape(*query.shape)
+    key_out = key_out.reshape(*key.shape)
+
+    return query_out, key_out
+
+
+class QEffLlama4VisionRotaryEmbedding(nn.Module):
+    """
+    Vision RoPE that
+    • caches (cos, sin) tables as a real-valued buffer --► folds into an ONNX initializer
+    • grows automatically if you feed larger images
+    """
+
+    def __init__(self, config):
+        super().__init__()
+
+        self.config = config
+        self.hidden_size = config.hidden_size
+        self.n_heads = config.num_attention_heads
+        self.theta = config.rope_theta
+        self.patch_size = config.patch_size
+
+        # Build the initial cache for the reference image resolution
+        n_patches = (config.image_size // self.patch_size) ** 2
+        self._build_cache(n_patches)  # registers `freqs_cis`
+        self.max_tokens_cached = n_patches + 1  # +1 for CLS row
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        """
+        hidden_states: [B, S, H]   (S = CLS + patches)
+        Returns:      [S, H/num_heads, 2]  (cos,sin).
+        """
+        return self.freqs_cis.unsqueeze(1).to(hidden_states.device)
+
+    def _build_cache(self, n_patches: int) -> None:
+        """
+        Pre-compute cos/sin for every patch plus the CLS token.
+        Produces buffer `freqs_cis`:  [(n_patches+1), head_dim, 2]
+        """
+        # -- 2-D grid coordinates ---------------------------------------- #
+        side = int(math.sqrt(n_patches))
+        assert side * side == n_patches, "Vision RoPE expects a square grid of patches"
+
+        coords = torch.arange(side)
+        y, x = torch.meshgrid(coords, coords, indexing="ij")
+        x = x.reshape(-1, 1)  # [n_patches,1]
+        y = y.reshape(-1, 1)
+
+        # -- rotary base frequencies ------------------------------------- #
+        head_dim = self.hidden_size // self.n_heads // 2  # real+imag split
+        rope_freq = 1.0 / (self.theta ** (torch.arange(0, head_dim, 2).float() / head_dim))
+
+        # angles along x / y; repeat_interleave = [freq0,freq0,freq1,freq1,…]
+        ang_x = ((x + 1) * rope_freq).repeat_interleave(2, dim=-1)
+        ang_y = ((y + 1) * rope_freq).repeat_interleave(2, dim=-1)
+        freqs = torch.cat([ang_x, ang_y], dim=-1).float()[..., ::2]  # [n_patches, head_dim]
+
+        # -- add CLS row = zeros  ---------------------------------------- #
+        freqs = torch.cat([freqs, freqs.new_zeros((1, freqs.shape[1]))], dim=0)
+
+        # -- stack real/imag → shape [tokens, head_dim, 2] --------------- #
+        real, imag = torch.cos(freqs), torch.sin(freqs)
+        freqs_cis = torch.stack([real, imag], dim=-1)
+
+        #    freqs_cis = torch.view_as_complex(freqs_cis.contiguous())
+
+        # store as non-persistent buffer
+        self.register_buffer("freqs_cis", freqs_cis, persistent=False)
+
+
+class QEffLlama4VisionAttention(Llama4VisionAttention):
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        freqs_ci: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Cache] = None,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        input_shape = hidden_states.shape[:-1]
+        hidden_shape = (*input_shape, -1, self.head_dim)
+
+        query_states = self.q_proj(hidden_states).view(hidden_shape)
+        key_states = self.k_proj(hidden_states).view(hidden_shape)
+        value_states = self.v_proj(hidden_states).view(hidden_shape)
+
+        query_states, key_states = qeff_vision_apply_rotary_emb(query_states, key_states, freqs_ci=freqs_ci)
+
+        query_states = query_states.transpose(1, 2)
+        key_states = key_states.transpose(1, 2)
+        value_states = value_states.transpose(1, 2)
+
+        attention_interface: Callable = eager_attention_forward_vision
+
+        attn_output, attn_weights = attention_interface(
+            self,
+            query_states,
+            key_states,
+            value_states,
+            None,
+            dropout=0.0 if not self.training else self.attention_dropout,
+            scaling=None,
+            is_causal=False,  # HAS TO BE ENFORCED
+            **kwargs,
+        )
+
+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
+        attn_output = self.o_proj(attn_output)
+        return attn_output, attn_weights
+
+
+class QEffLlama4VisionModel(Llama4VisionModel):
+    def __init__(self, config):
+        super().__init__(config)
+        # Define the general __qeff_init__() for any changes in the init calls
+        # Set the init in the module mapping pytorch transforms
+        self.config = config
+        self.__qeff_init__()
+
+    def __qeff_init__(self):
+        self.rotary_embedding = QEffLlama4VisionRotaryEmbedding(config=self.config)
+
+    def forward(
+        self,
+        pixel_values: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[BaseModelOutput, Tuple[torch.Tensor, ...]]:
+        r"""
+
+        Example:
+
+        ```python
+        >>> from PIL import Image
+        >>> import requests
+        >>> from transformers import AutoProcessor, MllamaVisionModel
+
+        >>> checkpoint = "meta-llama/Llama-3.2-11B-Vision"
+        >>> model = MllamaVisionModel.from_pretrained(checkpoint)
+        >>> processor = AutoProcessor.from_pretrained(checkpoint)
+
+        >>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
+        >>> image = Image.open(requests.get(url, stream=True).raw)
+        >>> inputs = processor(images=image, return_tensors="pt")
+
+        >>> output = model(**inputs)
+
+        >>> print(output.last_hidden_state.shape)
+        torch.Size([1, 1, 4, 1025, 7680])
+        ```
+        """
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # num_concurrent_media and num_chunks are both currently 1
+        batch_size_times_num_tiles, num_channels, height, width = pixel_values.shape
+        num_concurrent_media = 1
+        num_chunks = 1
+        hidden_state = self.patch_embedding(pixel_values)
+        _, num_patches, hidden_dim = hidden_state.shape
+
+        # Add cls token
+        hidden_state = hidden_state.reshape(
+            batch_size_times_num_tiles * num_concurrent_media * num_chunks, num_patches, hidden_dim
+        )
+        class_embedding = self.class_embedding.expand(hidden_state.shape[0], 1, hidden_state.shape[-1])
+        hidden_state = torch.cat([hidden_state, class_embedding], dim=1)
+        num_patches += 1
+
+        # Position embeddings
+        hidden_state = hidden_state.reshape(
+            batch_size_times_num_tiles * num_concurrent_media, num_chunks, num_patches, hidden_dim
+        )
+        positional_embedding = self.positional_embedding_vlm.to(dtype=hidden_state.dtype, device=hidden_state.device)
+        hidden_state = hidden_state + positional_embedding
+
+        hidden_state = self.layernorm_pre(hidden_state)
+
+        hidden_state = hidden_state.view(batch_size_times_num_tiles, -1, hidden_dim)
+        freqs_ci = self.rotary_embedding(pixel_values)
+        # import ipdb; ipdb.set_trace()
+
+        # import ipdb; ipdb.set_trace()
+        output = self.model(
+            hidden_state,
+            attention_mask=None,
+            output_hidden_states=output_hidden_states,
+            output_attentions=output_attentions,
+            freqs_ci=freqs_ci,
+        )
+
+        hidden_state = output.last_hidden_state
+
+        hidden_state = self.layernorm_post(hidden_state)
+
+        hidden_state = hidden_state[:, :-1, :]
+
+        # now, we use Llama4VisionPixelShuffle + mlp to project embeddings
+        hidden_state = self.vision_adapter(hidden_state)
+
+        hidden_states = output.hidden_states if output_hidden_states else None
+
+        if output_attentions:
+            attentions = output[2]
+        else:
+            attentions = None
+
+        if not return_dict:
+            return tuple(v for v in [hidden_state, hidden_states, attentions] if v is not None)
+
+        return BaseModelOutput(
+            last_hidden_state=hidden_state,
+            hidden_states=hidden_states,
+            attentions=attentions,
+        )
+
+
+class QEffLlama4TextRotaryEmbedding(nn.Module):
+    def __init__(self, config: Llama4TextConfig, device=None):
+        super().__init__()
+        self.config = config
+        self.rope_type = "llama3" if config.rope_scaling is not None else "default"
+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]
+
+        # self.max_seq_len_cached = config.max_position_embeddings
+        # TODO: vbaddi Shouldn't for rope, the max posision_embeddings be original embeddings for rope,
+        # chunk size 8192 always? and Revisit when >8K Chunked attention is enabled.
+        self.max_seq_len_cached = config.rope_scaling["original_max_position_embeddings"]
+        # self.max_seq_len_cached = config.max_position_embeddings
+
+        # Get inverse frequency and scaling function (handles yarn/etc)
+        inv_freq, self.attention_scaling = self.rope_init_fn(config, device)
+        self.register_buffer("inv_freq", inv_freq, persistent=False)
+
+        # Precompute static cache
+        self._set_freqs_cis_cache(self.max_seq_len_cached, device)
+
+    def _set_freqs_cis_cache(self, seq_len, device):
+        # Compute frequencies
+        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)  # [seq_len]
+        freqs = torch.outer(t, self.inv_freq)  # [seq_len, dim/2]
+
+        # Convert to [real, imag] = [cos, sin]
+        cos = torch.cos(freqs)
+        sin = torch.sin(freqs)
+        freqs_cis = torch.stack([cos, sin], dim=-1)  # [seq_len, dim/2, 2]
+
+        self.register_buffer("freqs_cis_cached", freqs_cis * self.attention_scaling, persistent=False)
+
+    def forward(self, seq_len: Optional[int] = None, position_ids: Optional[torch.LongTensor] = None):
+        """
+        Returns: freqs_cis: [batch, seq_len, dim/2, 2] if position_ids given,
+                           [seq_len, dim/2, 2] if only seq_len is given.
+        """
+        if position_ids is not None:
+            # position_ids: [batch, seq_len]
+            return self.freqs_cis_cached[position_ids]  # shape: [batch, seq_len, dim/2, 2]
+        else:
+            assert seq_len is not None, "Either seq_len or position_ids must be provided."
+            return self.freqs_cis_cached[:seq_len]  # shape: [seq_len, dim/2, 2]
+
+
+def qeff_apply_rotary_emb(
+    xq: torch.Tensor,
+    xk: torch.Tensor,
+    freqs_cis: torch.Tensor,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    xq_ = xq.view(*xq.shape[:-1], -1, 2)
+    xk_ = xk.view(*xk.shape[:-1], -1, 2)
+
+    # freqs_cis is already in [..., 2] form (real, imag)
+    freqs_cis_exp = freqs_cis.unsqueeze(2)  # [1,1,L,D,2]
+
+    xq_out = complex_mul_onnx_safe(xq_, freqs_cis_exp)
+    xk_out = complex_mul_onnx_safe(xk_, freqs_cis_exp)
+    xq_out = xq_out.reshape(*xq.shape)
+    xk_out = xk_out.reshape(*xk.shape)
+    return xq_out, xk_out
+
+
+def eager_attention_forward(
+    module: nn.Module,
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    attention_mask: Optional[torch.Tensor],
+    scaling: float,
+    **kwargs,
+):
+    key_states = repeat_kv(key, module.num_key_value_groups)
+    value_states = repeat_kv(value, module.num_key_value_groups)
+
+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
+    if attention_mask is not None:
+        attn_weights = torch.where(attention_mask, torch.tensor(-10000.0, dtype=torch.float32), attn_weights)
+
+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
+    attn_output = torch.matmul(attn_weights, value_states)
+    attn_output = attn_output.transpose(1, 2).contiguous()
+
+    return attn_output, attn_weights
+
+
+class QEffLlama4TextExperts(Llama4TextExperts):
+    def __qeff_init__(self):
+        self.gate_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, self.expert_dim))
+        self.up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, self.expert_dim))
+
+
+class QEffLlama4TextMoe(Llama4TextMoe):
+    def forward(self, hidden: torch.Tensor):
+        B, S, H = hidden.shape
+        T = B * S
+        x = hidden.view(T, H)
+
+        router_logits = self.router(x)
+
+        # *top-k = 1*  → LLama4
+        top_w, top_i = torch.topk(router_logits, self.top_k, dim=-1)  # both [T, K]
+        masked_logits = torch.full_like(router_logits, float("-inf"))
+        masked_logits.scatter_(1, top_i, top_w)
+
+        # ── Book-keeping: create one boolean mask per expert once  ───────────────
+        # routing_weights[e]  ==  True where token routed to that expert. Shape [E, T]
+        routing_weights = torch.sigmoid(masked_logits.float()).to(hidden.dtype)
+
+        # ────────────────── allocate the two big tensors ─────
+        ffn_dim = self.experts.intermediate_size  # = 8/3 · H
+        upgate = x.new_zeros((T, ffn_dim))
+        expert_out = x.new_zeros((T, H))  # accum-out buffer
+
+        # ───────────────────────── Stage-1 : Up-Gate ─────────────────────────────
+        # Loop over experts
+        for e in range(self.num_experts):
+            W_g, W_u = self.experts.gate_proj[e], self.experts.up_proj[e]
+            routing_weight = routing_weights[:, e].unsqueeze(-1)
+            masked_up = torch.where(
+                routing_weights[:, e].unsqueeze(-1) > 0,
+                ((self.experts.act_fn(x @ W_g)) * (x @ W_u)),
+                torch.zeros_like(upgate),
+            )
+            upgate += masked_up
+
+        # At this point  upgate[t]  holds   UpGate(x_t)   for that token’s expert,
+        # and arbitrary (zeros) data for tokens not routed to that expert.
+        # ───────────────────────── Stage-2 : Down ────────────────────────────────
+        for e in range(self.num_experts):
+            routing_weight = routing_weights[:, e].unsqueeze(-1)
+            masked_down = torch.where(
+                routing_weight > 0, (upgate @ self.experts.down_proj[e]) * routing_weight, torch.zeros_like(expert_out)
+            )
+            expert_out += masked_down
+
+        # ───────────────────────── Stage-3 : Shared expert ───────────────────────
+        shared_out = self.shared_expert(x)  # [T, H]
+        final = shared_out + expert_out  # restore [B,S,H]
+        return final.view(B, S, H), router_logits
+
+    # ------------------- Gather based, weights as activation approach ---------------
+    # def forward(self, hidden_states):
+    #     bs, seq_len, _ = hidden_states.shape
+    #     hidden_states = hidden_states.view(bs * seq_len, self.hidden_dim)
+    #     router_logits = self.router(hidden_states).transpose(0, 1)
+    #     router_top_value, router_indices = torch.topk(router_logits.transpose(0, 1), self.top_k, dim=1)
+
+    #     # GATHER
+    #     gate_up_proj = self.experts.gate_up_proj[router_indices.flatten()]
+    #     down_proj = self.experts.down_proj[router_indices.flatten()]
+
+    #     # apply router top value
+    #     expert_in = hidden_states * torch.sigmoid(router_top_value)
+    #     ######################
+    #     # Apply Chosen Experts
+    #     ######################
+    #     expert_in = expert_in.view(bs * seq_len, 1, self.hidden_dim)
+    #     gateup = torch.bmm(expert_in, gate_up_proj)
+    #     gate, up = gateup.chunk(2, dim=-1)
+    #     experts_out = torch.bmm((up * self.experts.act_fn(gate)), down_proj).view(bs * seq_len, self.hidden_dim)
+    #     shared_out = self.shared_expert(hidden_states)
+    #     out = shared_out + experts_out
+    #     return out, router_logits
+
+    # def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
+    #     """
+    #     Implements expert routing and computation as in original LLaMA 4 MoE.
+    #     """
+    #     batch, seq_len, hidden_dim = hidden_states.shape         # B = batch, S = seq_len, H = hidden
+    #     T = batch * seq_len                                      # T = total number of tokens
+    #     hidden_states_flat = hidden_states.view(T, hidden_dim)   # [T, H] = token-wise input
+
+    #     router_logits = self.router(hidden_states_flat)          # [T, E] where E = num_experts
+
+    #     topk_values, topk_indices = torch.topk(router_logits, self.top_k, dim=1)  # both [T, K]
+    #     masked_logits = torch.full_like(router_logits, float("-inf"))  # [T, E]
+    #     masked_logits.scatter_(1, topk_indices, topk_values)           # Set only top-k logits
+    #     routing_weights = torch.sigmoid(masked_logits.float()).to(hidden_states.dtype)  # [T, E]
+
+    #     # -----------------------------------------------------------
+    #     # : Zero tensor for accumulating expert outputs
+    #     # Each expert contributes selectively into this output
+    #     # -----------------------------------------------------------
+    #     expert_output = torch.zeros_like(hidden_states_flat)  # [T, H]
+    #     for expert_idx in range(self.num_experts):
+    #         gate_up = hidden_states_flat @ self.experts.gate_up_proj[expert_idx]  # [T, 2I]
+    #         gate, up = gate_up.chunk(2, dim=-1)                                   # [T, I], [T, I]
+    #         activated = self.experts.act_fn(gate) * up                            # [T, I]
+
+    #         expert_out = activated @ self.experts.down_proj[expert_idx]           # [T, H]
+    #         routing_weight = routing_weights[:, expert_idx].unsqueeze(-1)         # [T, 1]
+
+    #         # Conditionally apply expert output
+    #         #    : cond ? X * w : 0 → ONNX-safe execution
+    #         #    Only routed tokens contribute non-zero values
+    #         masked_out = torch.where(
+    #             routing_weight > 0,                     # condition
+    #             expert_out * routing_weight,            # scaled expert output
+    #             torch.zeros_like(expert_out)            # fallback = 0
+    #         )
+
+    #         expert_output += masked_out  # [T, H]
+
+    #     shared_output = self.shared_expert(hidden_states_flat)  # [T, H]
+    #     final = expert_output + shared_output                   # [T, H]
+    #     final = final.view(batch, seq_len, hidden_dim)          # [B, S, H]
+
+    #     return final, router_logits
+
+
+class QEffLlama4TextAttention(Llama4TextAttention):
+    """Multi-headed attention from 'Attention Is All You Need' paper"""
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.Tensor],
+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Cache] = None,
+        batch_index: Optional[torch.LongTensor] = None,
+        cache_position: Optional[torch.LongTensor] = None,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        input_shape = hidden_states.shape[:-1]
+        hidden_shape = (*input_shape, -1, self.head_dim)
+
+        query_states = self.q_proj(hidden_states).view(hidden_shape)
+        key_states = self.k_proj(hidden_states).view(*input_shape, -1, self.head_dim)
+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
+
+        kv_seq_len = key_states.shape[-2]
+
+        kv_seq_len = past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
+        ##
+        if self.use_rope:  # the 16E model skips rope for long context on certain layers
+            query_states, key_states = qeff_apply_rotary_emb(
+                query_states, key_states, position_embeddings.to(query_states.device)
+            )
+
+        if hasattr(self, "qk_norm"):  # the 128E model does not use qk_norm
+            query_states = self.qk_norm(query_states)
+            key_states = self.qk_norm(key_states)
+
+        # Use temperature tuning from https://arxiv.org/abs/2501.19399) to NoROPE layers
+        if self.attn_temperature_tuning and not self.use_rope:
+            attn_scales = (
+                torch.log(torch.floor((cache_position.float() + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0
+            )
+            attn_scales = attn_scales.view((*input_shape, 1, 1))
+            query_states = (query_states * attn_scales).to(query_states.dtype)
+
+        query_states = query_states.transpose(1, 2)
+        key_states = key_states.transpose(1, 2)
+
+        if past_key_value is not None:
+            # sin and cos are specific to RoPE models; cache_position needed for the static cache
+            cache_kwargs = {"batch_index": batch_index, "position_ids": position_ids}
+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
+
+        attention_interface: Callable = eager_attention_forward
+
+        attn_output, attn_weights = attention_interface(
+            self,
+            query_states,
+            key_states,
+            value_states,
+            attention_mask,
+            scaling=self.scaling,
+            **kwargs,
+        )
+
+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
+        attn_output = self.o_proj(attn_output)
+        return attn_output, attn_weights, past_key_value
+
+
+class QEffLlama4TextDecoderLayer(Llama4TextDecoderLayer):
+    """
+    Copied from LlamaForCausalLM: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama.py
+    The only differences are:
+    - add new args batch idx for the CB models
+    """
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        chunk_causal_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Cache] = None,
+        batch_index: Optional[torch.LongTensor] = None,
+        output_attentions: Optional[bool] = False,
+        output_router_logits: Optional[bool] = False,
+        use_cache: Optional[bool] = False,
+        cache_position: Optional[torch.LongTensor] = None,
+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
+        **kwargs,
+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
+        residual = hidden_states
+
+        # use local attention mask for ROPE layers
+        if self.use_chunked_attention and chunk_causal_mask is not None:
+            attention_mask = chunk_causal_mask
+
+        hidden_states = self.input_layernorm(hidden_states)
+
+        # Self Attention
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            attention_mask=attention_mask,
+            position_embeddings=position_embeddings,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            batch_index=batch_index,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            cache_position=cache_position,
+            **kwargs,
+        )
+        hidden_states = residual + hidden_states
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = self.post_attention_layernorm(hidden_states)
+        hidden_states = self.feed_forward(hidden_states)
+        if self.is_moe_layer:
+            # Change by VB
+            hidden_states, router_logits = hidden_states
+        else:
+            router_logits = None
+        hidden_states = residual + hidden_states.view(residual.shape)
+
+        outputs = (hidden_states,)
+
+        if output_attentions:
+            outputs += (self_attn_weights,)
+
+        if output_router_logits:
+            outputs += (router_logits,)
+
+        return outputs
+
+
+class QEffLlama4TextModel(Llama4TextModel):
+    """
+    Copied from LlamaForCausalLM: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama.py
+    The only differences are:
+    - add new args cache idx for the kv retention
+    """
+
+    def __init__(self, config: Llama4TextModel):
+        super().__init__(config)
+        # Define the general __qeff_init__() for any changes in the init calls
+        # Set the init in the module mapping pytorch transforms
+        self.config = config
+        self.__qeff_init__()
+
+    def __qeff_init__(self):
+        self.rotary_emb = QEffLlama4TextRotaryEmbedding(config=self.config)
+
+    def forward(
+        self,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Cache] = None,
+        batch_index: Optional[torch.LongTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        cache_position: Optional[torch.LongTensor] = None,
+        **kwargs,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        if (input_ids is None) ^ (inputs_embeds is not None):
+            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
+        return_legacy_cache = False
+        if use_cache and not isinstance(past_key_values, Cache):
+            return_legacy_cache = True
+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)
+
+        if cache_position is None:
+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
+            cache_position = torch.arange(
+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
+            )
+        if position_ids is None:
+            position_ids = cache_position.unsqueeze(0)
+
+        causal_mask = _create_causal_mask(position_ids=position_ids, target_length=past_seen_tokens)
+
+        _, chunk_causal_mask = self._update_causal_mask(
+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
+        )
+
+        # embed positions
+        hidden_states = inputs_embeds
+
+        # create position embeddings to be shared across the decoder layers
+        freq_cis = self.rotary_emb(hidden_states, position_ids=position_ids)
+
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+
+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+
+            layer_outputs = decoder_layer(
+                hidden_states,
+                attention_mask=causal_mask,
+                chunk_causal_mask=chunk_causal_mask,
+                position_ids=position_ids,
+                past_key_value=past_key_values,
+                batch_index=batch_index,
+                output_attentions=output_attentions,
+                use_cache=use_cache,
+                cache_position=cache_position,
+                position_embeddings=freq_cis,
+                **kwargs,
+            )
+
+            hidden_states = layer_outputs[0]
+
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
+
+        hidden_states = self.norm(hidden_states)
+
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
+
+        if return_legacy_cache:
+            past_key_values = past_key_values.to_legacy_cache()
+
+        output = BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=past_key_values if use_cache else None,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+        )
+        return output if return_dict else output.to_tuple()
+
+
+class QEffLlama4ForCausalLM(Llama4ForCausalLM):
+    """
+    Copied from Llama4ForCausalLM: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama.py
+    The only differences are:
+    - add new args cache idx for the kv retention
+    """
+
+    def forward(
+        self,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,
+        batch_index: Optional[torch.LongTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        cache_position: Optional[torch.LongTensor] = None,
+        logits_to_keep: Union[int, torch.Tensor] = 0,
+        **kwargs,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        outputs = self.model(
+            input_ids=input_ids,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_values=past_key_values,
+            batch_index=batch_index,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            cache_position=cache_position,
+            **kwargs,
+        )
+
+        # Cast to INT32 to avoid issue while running in ONNXRT
+        logit_index = position_ids.to(torch.int32).argmax(1, keepdim=True)
+        hidden_states = outputs[0][torch.arange(position_ids.shape[0]).view(-1, 1), logit_index]
+
+        logits = self.lm_head(hidden_states)
+        logits = logits.float()
+
+        return CausalLMOutputWithPast(
+            loss=None,
+            logits=logits,
+            past_key_values=outputs.past_key_values,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
+        )
+
+
+class QEffLlama4EncoderWrapper(nn.Module):
+    def __init__(self, model):
+        super().__init__()
+        self.model = model
+
+    def forward(self, input_ids, pixel_values):
+        inputs_embeds = self.model.get_input_embeddings()(input_ids)
+        vision_feature_layer = self.model.config.vision_config.vision_feature_layer
+        vision_feature_select_strategy = self.model.config.vision_config.vision_feature_select_strategy
+        image_features = self.model.get_image_features(
+            pixel_values=pixel_values,
+            vision_feature_layer=vision_feature_layer,
+            vision_feature_select_strategy=vision_feature_select_strategy,
+            image_sizes=None,
+        )
+        vision_flat = image_features.view(-1, image_features.size(-1))
+        projected_vision_flat = self.model.multi_modal_projector(vision_flat)
+        selected = input_ids == self.model.config.image_token_index
+        indices1 = selected.to(torch.int64).cumsum(1) - 1
+        indices0 = torch.arange(selected.unsqueeze(0).shape[0]).view(-1, 1)
+        image_features_expanded = projected_vision_flat.unsqueeze(0)[indices0, indices1]
+        image_input_embeds = torch.where(selected.unsqueeze(-1), image_features_expanded, inputs_embeds)
+        return image_input_embeds
+
+
+class QEffLlama4DecoderWrapper(nn.Module):
+    def __init__(self, model):
+        super().__init__()
+        self.model = model
+        self.language_model = self.model.language_model
+        self.config = self.model.config
+
+    def forward(self, input_ids, vision_embeds, position_ids, past_key_values):
+        image_embeds = vision_embeds[:, : input_ids.shape[1], :]
+        inputs_embeds = self.model.language_model.get_input_embeddings()(input_ids)
+        inputs_embeds = torch.where(input_ids.shape[1] == torch.tensor(1), inputs_embeds, image_embeds)
+        outputs = self.model.language_model(
+            inputs_embeds=inputs_embeds, position_ids=position_ids, past_key_values=past_key_values, use_cache=True
+        )
+        return outputs.logits, vision_embeds, outputs.past_key_values
+
+
+class QEffLlama4ForConditionalGeneration(Llama4ForConditionalGeneration):
+    def get_qeff_vision_encoder(self):
+        return QEffLlama4EncoderWrapper(self)
+
+    def get_qeff_language_decoder(self):
+        return QEffLlama4DecoderWrapper(self)
+
+    def get_specializations(
+        self,
+        batch_size: int,
+        prefill_seq_len: int,
+        ctx_len: int,
+        img_size: int,
+        kv_offload: bool = False,
+        **compiler_options,
+    ):
+        # TODO: check if this should be named num_patches or something else
+        batch_size_times_num_tiles = compiler_options.pop("batch_size_times_num_tiles", None)
+        if batch_size_times_num_tiles is None:
+            logger.warning(
+                "User should pass `batch_size_times_num_tiles` to compile API to fix the dynamic axes `pixel_values`, you can get more info by calling get_inputs_info function!, Since its not found setting its value to 17"
+            )
+            batch_size_times_num_tiles = 17
+
+        vision_seq_len = compiler_options.pop("vision_seq_len", None)
+        if vision_seq_len is None:
+            vision_seq_len = 2560
+
+        prefill_seq_len = prefill_seq_len if prefill_seq_len else 32
+        ctx_len = ctx_len if ctx_len else constants.INTERN_CTX_LEN
+        if img_size is None and hasattr(self.config.vision_config, "image_size"):
+            img_size = getattr(self.config.vision_config, "image_size")
+        elif img_size is None:
+            img_size = 336  # FIXME based on llama4 Image size
+            logger.warning("Setting img_size to be 336, as it was neither passed nor found in vision_config")
+
+        vision = [
+            {
+                "batch_size": batch_size,
+                "batch_size_times_num_tiles": batch_size_times_num_tiles,
+                "img_size": img_size,
+                "seq_len": vision_seq_len,
+                "ctx_len": ctx_len,
+            }
+        ]
+        lang = [
+            {
+                "batch_size": batch_size,
+                "seq_len": prefill_seq_len,
+                "ctx_len": ctx_len,
+                "batch_size_times_num_tiles": batch_size_times_num_tiles,
+                "img_size": img_size,
+                "chunk_length": prefill_seq_len,
+            },
+            {
+                "batch_size": batch_size,
+                "seq_len": "1",
+                "ctx_len": ctx_len,
+                "batch_size_times_num_tiles": batch_size_times_num_tiles,
+                "img_size": img_size,
+                "chunk_length": prefill_seq_len,
+            },
+        ]
+
+        specializations = {}
+
+        if kv_offload:
+            specializations["vision"] = vision
+            specializations["lang"] = lang
+            return specializations, compiler_options
+        else:
+            return lang, compiler_options
+
+    def get_onnx_dynamic_axes(self, kv_offload: bool = False):
+        # Define dynamic axes
+        vision_dynamic_axes = {}
+        lang_dynamic_axes = {}
+        lang_dynamic_axes["input_ids"] = {0: "batch_size", 1: "seq_len"}
+        lang_dynamic_axes["position_ids"] = {0: "batch_size", 1: "seq_len"}
+        lang_dynamic_axes["vision_embeds"] = {0: "batch_size", 1: "chunk_length"}
+        vision_dynamic_axes["pixel_values"] = {0: "batch_size_times_num_tiles", 2: "img_size", 3: "img_size"}
+        vision_dynamic_axes["input_ids"] = {0: "batch_size", 1: "seq_len"}
+
+        pkv_dynamic_axes = {0: "batch_size", 2: "ctx_len"}
+        for i in range(self.language_model.config.num_hidden_layers):
+            for kv in ["key", "value"]:
+                lang_dynamic_axes[f"past_{kv}.{i}"] = pkv_dynamic_axes
+
+        dynamic_axes = {}
+        if kv_offload:
+            dynamic_axes["vision"] = vision_dynamic_axes
+            dynamic_axes["lang"] = lang_dynamic_axes
+        else:
+            dynamic_axes = {**vision_dynamic_axes, **lang_dynamic_axes}
+        return dynamic_axes
+
+    def get_output_names(self, kv_offload: bool = False):
+        vision_output_names = ["vision_embeds"]
+        lang_output_names = ["logits"]
+        for i in range(self.language_model.config.num_hidden_layers):
+            for kv in ["key", "value"]:
+                lang_output_names.append(f"past_{kv}.{i}_RetainedState")
+
+        output_names = {}
+        if kv_offload:
+            lang_output_names.insert(1, "vision_embeds_RetainedState")
+            output_names["vision"] = vision_output_names
+            output_names["lang"] = lang_output_names
+        else:
+            lang_output_names.insert(1, "pixel_values_RetainedState")
+            return lang_output_names
+        return output_names
+
+    def get_dummy_inputs(self, kv_offload: bool = False):
+        if vis_cfg := getattr(self.config, "vision_config", None):
+            img_size = getattr(vis_cfg, "image_size", 336)
+        else:
+            img_size = 336
+        # if img_size != constants.INTERN_IMG_SIZE and kv_offload:
+        #     raise NotImplementedError("Image Size other than 448 is not supported for Intern models yet.")
+
+        # patch_size = getattr(self.config.vision_config, "patch_size", None)
+        # downsample_ratio = getattr(self.config, "downsample_ratio", None)
+        # if patch_size and downsample_ratio:
+        #     computed_feature_size = int(((img_size / patch_size) * downsample_ratio) ** 2)
+        #     if computed_feature_size != constants.INTERN_FEATURE_SIZE:
+        #         logger.warning(
+        #             "Discrepancy detected between estimated and actual feature sizes. Could impact on functionality or accuracy"
+        #         )
+
+        # Define shapes
+        inputs_shapes = {}
+        inputs_shapes["input_ids"] = (constants.ONNX_EXPORT_EXAMPLE_BATCH_SIZE, constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN)
+        inputs_shapes["vision_embeds"] = (
+            1,  # constants.INTERN_NUM_PATCHES,
+            constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN,  # constants.INTERN_FEATURE_SIZE,
+            self.language_model.config.hidden_size,  # 5120
+        )
+        inputs_shapes["position_ids"] = (
+            constants.ONNX_EXPORT_EXAMPLE_BATCH_SIZE,
+            constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN,
+        )
+        inputs_shapes["pixel_values"] = (
+            17,  # constants.INTERN_NUM_PATCHES,
+            constants.INTERN_NUM_CHANNELS,
+            img_size,
+            img_size,
+        )
+
+        # Define inputs
+        vision_inputs = {}
+        lang_inputs = {}
+        vision_inputs["pixel_values"] = torch.zeros((inputs_shapes["pixel_values"]), dtype=torch.float32)
+        vision_inputs["input_ids"] = torch.zeros((inputs_shapes["input_ids"]), dtype=torch.int64)
+        lang_inputs["input_ids"] = torch.zeros((inputs_shapes["input_ids"]), dtype=torch.int64)
+        lang_inputs["vision_embeds"] = torch.zeros((inputs_shapes["vision_embeds"]), dtype=torch.float32)
+        lang_inputs["position_ids"] = (
+            torch.arange(constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN, dtype=torch.int64)
+            .view(1, constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN)
+            .repeat(constants.ONNX_EXPORT_EXAMPLE_BATCH_SIZE, 1)
+        )
+
+        # Add data for KV
+        kv_cache_shape = get_padding_shape_from_config(
+            config=self.language_model.config,
+            batch_size=constants.ONNX_EXPORT_EXAMPLE_BATCH_SIZE,
+            seq_len=constants.ONNX_EXPORT_EXAMPLE_SEQ_LEN,
+        )
+
+        lang_inputs["past_key_values"] = [[] for _ in range(self.language_model.config.num_hidden_layers)]
+        for i in range(self.language_model.config.num_hidden_layers):
+            for kv in ["key", "value"]:
+                lang_inputs["past_key_values"][i].append(torch.zeros(kv_cache_shape, dtype=torch.float32))
+
+        inputs = {}
+        if kv_offload:
+            inputs["vision"] = vision_inputs
+            inputs["lang"] = lang_inputs
+        else:
+            lang_inputs.pop("vision_embeds")
+            inputs = {**vision_inputs, **lang_inputs}
+
+        return inputs
+
+    def get_inputs_info(self):
+        return [
+            IOInfo(name="input_ids", datatype=torch.int64, shape=("batch_size", "seq_len")),
+            IOInfo(name="attention_mask", datatype=torch.int64, shape=("batch_size", "seq_len")),
+            IOInfo(
+                name="pixel_values",
+                datatype=torch.float32,
+                shape=("batch_size_times_num_tiles", 3, "img_size", "img_size"),
+            ),
+        ]
diff --git a/QEfficient/transformers/models/modeling_auto.py b/QEfficient/transformers/models/modeling_auto.py
index 6b5deb8..10b3313 100644
--- a/QEfficient/transformers/models/modeling_auto.py
+++ b/QEfficient/transformers/models/modeling_auto.py
@@ -751,16 +751,13 @@ class _QEffAutoModelForImageTextToTextDualQPC:
         input_len = inputs["attention_mask"].sum(1, keepdims=True)
         input_ids_length = inputs["input_ids"].shape[1]
         num_chunks = -(input_ids_length // -prefill_seq_len)  # ceil divide without float
-        padded_len = num_chunks * prefill_seq_len  # Convert to a multiple of prompt_len
-
+        # padded_len = num_chunks * prefill_seq_len  # Convert to a multiple of prompt_len
+        padded_len = vision_session.bindings[0].dims[1]
         if generation_len is None:
             generation_len = ctx_len - input_len.max()
         assert generation_len > 0, "generation length should be greater than zero"
         generated_ids = np.full((batch_size, generation_len + 1), pad_token_id)
 
-        # Prepare inputs for prefill
-        prefill_start = perf_counter()
-
         inputs["input_ids"] = torch.nn.functional.pad(
             inputs["input_ids"],
             (0, padded_len - input_ids_length),
@@ -783,9 +780,14 @@ class _QEffAutoModelForImageTextToTextDualQPC:
         }
 
         vision_inputs["pixel_values"] = vision_inputs["pixel_values"].astype("float16")
+        vision_inputs["input_ids"] = inputs["input_ids"]
+        vision_start = perf_counter()
+
         vision_outputs = vision_session.run(vision_inputs)
+        vision_end = perf_counter()
 
         lang_inputs = {k: v for k, v in inputs.items() if k not in vision_inputs}
+        lang_inputs["input_ids"] = inputs["input_ids"]
         lang_inputs["position_ids"] = np.where(
             lang_inputs.pop("attention_mask"), np.arange(padded_len), -1
         )  # Need to use -1 as position_ids for invalid tokens
@@ -793,7 +795,10 @@ class _QEffAutoModelForImageTextToTextDualQPC:
         vision_session.deactivate()
         lang_session.activate()
 
-        lang_session.set_buffers(vision_outputs)
+        # lang_session.set_buffers(vision_outputs)
+        lang_inputs["vision_embeds"] = vision_outputs["vision_embeds"]
+        # Prepare inputs for prefill
+        prefill_start = perf_counter()
 
         # Run prefill
         for i in range(num_chunks):
@@ -802,10 +807,14 @@ class _QEffAutoModelForImageTextToTextDualQPC:
             chunk_inputs["position_ids"] = lang_inputs["position_ids"][
                 :, i * prefill_seq_len : (i + 1) * prefill_seq_len
             ]
+            chunk_inputs["vision_embeds"] = lang_inputs["vision_embeds"][
+                :, i * prefill_seq_len : (i + 1) * prefill_seq_len
+            ]
             outputs = lang_session.run(chunk_inputs)
 
-        prefill_time = perf_counter() - prefill_start
+        prefill_time = perf_counter() - prefill_start + vision_end - vision_start
         # Skip inputs/outputs again
+        lang_inputs["vision_embeds"] = lang_inputs["vision_embeds"][:, :prefill_seq_len]
         lang_session.skip_buffers(
             [x for x in lang_session.input_names + lang_session.output_names if x.startswith("past_")]
         )
@@ -838,7 +847,7 @@ class _QEffAutoModelForImageTextToTextDualQPC:
             streamer.end()
 
         decode_perf = (num_token - 1) / (decode_end - decode_start)
-        total_time = decode_end - prefill_start
+        total_time = decode_end - decode_start + prefill_time
         total_perf = num_token / total_time
 
         return CloudAI100ExecInfoNew(
@@ -1298,7 +1307,7 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
         self,
         model: nn.Module,
         continuous_batching: bool = False,
-        qaic_config: Optional[dict] = None,
+        is_tlm: bool = False,
         **kwargs,
     ):
         model_class_name = model.__class__.__name__
@@ -1324,8 +1333,11 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
         self.model.config.use_cache = True
         self.num_layers = model.config.num_hidden_layers
         self.continuous_batching = continuous_batching
-        self.model, transformed = SpDTransform.apply(self.model, qaic_config, **kwargs)
-        self.is_tlm = transformed
+
+        if is_tlm:
+            # TODO: It is possible to always apply this transform and make value of indices as last indices by default in PyTorch
+            self.model, transformed = SpDTransform.apply(self.model)
+        self.is_tlm = is_tlm
 
     @property
     def model_name(self) -> str:
@@ -1340,12 +1352,7 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
     @classmethod
     @with_replaced_quantizers
     def from_pretrained(
-        cls,
-        pretrained_model_name_or_path,
-        continuous_batching: bool = False,
-        qaic_config: Optional[dict] = None,
-        *args,
-        **kwargs,
+        cls, pretrained_model_name_or_path, continuous_batching: bool = False, is_tlm: bool = False, *args, **kwargs
     ):
         """
         This method serves as the easiest entry point into using QEfficient. The interface is designed to be similar to transformers.AutoModelForCausalLM.
@@ -1390,8 +1397,6 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
 
         kwargs.update({"attn_implementation": "eager", "low_cpu_mem_usage": False})
         model = cls._hf_auto_class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)
-        if qaic_config is not None:
-            qaic_config["pretrained_model_name_or_path"] = pretrained_model_name_or_path
 
         # This is support models that should be classified to in a different auto class but transformers load them via this class
 
@@ -1400,12 +1405,7 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
                 model, kv_offload=kv_offload
             )
 
-        return cls(
-            model,
-            continuous_batching=continuous_batching,
-            qaic_config=qaic_config,
-            **kwargs,
-        )
+        return cls(model, is_tlm=is_tlm, continuous_batching=continuous_batching)
 
     @property
     def model_hash(self) -> str:
@@ -1580,7 +1580,15 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
             raise TypeError("`prefill_only` must be a boolean.")
 
         if self.is_tlm:
-            num_speculative_tokens = self.check_and_get_num_speculative_tokens(num_speculative_tokens, prefill_seq_len)
+            if num_speculative_tokens is None:
+                raise TypeError("`num_speculative_tokens` is required when `is_tlm=True`.")
+            if not isinstance(num_speculative_tokens, int) or num_speculative_tokens < 2:
+                raise ValueError("`num_speculative_tokens` must be an integer >= 2.")
+            if prefill_seq_len < (num_speculative_tokens + 1):
+                raise ValueError(
+                    f"`prefill_seq_len` must be at least `num_speculative_tokens + 1` "
+                    f"({num_speculative_tokens + 1}), got {prefill_seq_len}."
+                )
 
         if self.continuous_batching and full_batch_size is None:
             raise TypeError("`full_batch_size` is required when `continuous_batching=True`.")
@@ -1675,29 +1683,6 @@ class QEFFAutoModelForCausalLM(QEFFBaseModel):
         else:
             raise NotImplementedError("Only AI_100 runtime is supported right now via generate API")
 
-    def check_and_get_num_speculative_tokens(self, num_speculative_tokens: Optional[int], prefill_seq_len: int):
-        if hasattr(self.model.config, "speculative_config"):
-            num_speculative_tokens_ = self.model.config.speculative_config["num_speculative_tokens"]
-            if num_speculative_tokens is not None:
-                logger.warning(
-                    f"arg `num_speculative_tokens` is a fixed value of {num_speculative_tokens_} for this model."
-                    f" Passed value of {num_speculative_tokens} will be ignored."
-                )
-            num_speculative_tokens = num_speculative_tokens_
-        elif num_speculative_tokens is None:
-            raise TypeError("missing required argument `num_speculative_tokens` as `is_tlm` is True.")
-
-        if not isinstance(num_speculative_tokens, int) and num_speculative_tokens < 2:
-            ValueError(
-                f"`num_speculative_tokens` arg should be an integer greater than 1, got {num_speculative_tokens}"
-            )
-        num_logits_to_keep = num_speculative_tokens + 1
-        if prefill_seq_len < num_logits_to_keep:
-            raise ValueError(
-                f"sequence length ({prefill_seq_len}) must be at least `num_speculative_tokens+1` ({num_logits_to_keep})"
-            )
-        return num_speculative_tokens
-
 
 class QEFFAutoModelForSpeechSeq2Seq(QEFFTransformersBase, MultimodalUtilityMixin):
     """
diff --git a/QEfficient/transformers/models/pytorch_transforms.py b/QEfficient/transformers/models/pytorch_transforms.py
index 333c734..c2a8c99 100644
--- a/QEfficient/transformers/models/pytorch_transforms.py
+++ b/QEfficient/transformers/models/pytorch_transforms.py
@@ -1,13 +1,14 @@
 # -----------------------------------------------------------------------------
 #
-# Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 #
 # -----------------------------------------------------------------------------
 
 from types import MethodType
-from typing import Optional, Tuple
+from typing import Tuple
 
+import transformers
 from torch import nn
 from transformers.models.codegen.modeling_codegen import (
     CodeGenAttention,
@@ -49,16 +50,6 @@ from transformers.models.granite.modeling_granite import (
     GraniteModel,
     GraniteRMSNorm,
 )
-from transformers.models.granitemoe.modeling_granitemoe import (
-    GraniteMoeAttention,
-    GraniteMoeForCausalLM,
-    GraniteMoeModel,
-    GraniteMoeMoE,
-    GraniteMoeParallelExperts,
-    GraniteMoeRMSNorm,
-    GraniteMoeRotaryEmbedding,
-    GraniteMoeTopKGating,
-)
 from transformers.models.llama.modeling_llama import (
     LlamaAttention,
     LlamaDecoderLayer,
@@ -66,12 +57,21 @@ from transformers.models.llama.modeling_llama import (
     LlamaModel,
     LlamaRMSNorm,
 )
+from transformers.models.llama4.modeling_llama4 import (
+    Llama4ForCausalLM,
+    Llama4ForConditionalGeneration,
+    Llama4TextAttention,
+    Llama4TextDecoderLayer,
+    Llama4TextExperts,
+    Llama4TextModel,
+    Llama4TextMoe,
+    Llama4TextRMSNorm,
+    Llama4VisionAttention,
+    Llama4VisionModel,
+)
 from transformers.models.llava.modeling_llava import (
     LlavaForConditionalGeneration,
 )
-from transformers.models.llava_next.modeling_llava_next import (
-    LlavaNextForConditionalGeneration,
-)
 from transformers.models.mistral.modeling_mistral import (
     MistralAttention,
     MistralDecoderLayer,
@@ -133,6 +133,7 @@ from transformers.models.whisper.modeling_whisper import (
 
 from QEfficient.base.pytorch_transforms import ModuleMappingTransform, ModuleMethodMapperTransform
 from QEfficient.customop import CustomRMSNormAIC, GemmaCustomRMSNormAIC
+from QEfficient.transformers.cache_utils import QEffDynamicCache
 from QEfficient.transformers.models.codegen.modeling_codegen import (
     QEffCodeGenAttention,
     QeffCodeGenBlock,
@@ -180,28 +181,30 @@ from QEfficient.transformers.models.granite.modeling_granite import (
     QEffGraniteForCausalLM,
     QEffGraniteModel,
 )
-from QEfficient.transformers.models.granitemoe.modeling_granitemoe import (
-    QEffGraniteMoeAttention,
-    QEffGraniteMoeForCausalLM,
-    QEffGraniteMoeModel,
-    QEffGraniteMoeMoE,
-    QEffGraniteMoeParallelExperts,
-    QEffGraniteMoeRotaryEmbedding,
-    QEffGraniteMoeTopKGating,
+from QEfficient.transformers.models.internvl.modeling_internvl import (
+    QEffInternVisionEmbeddings,
+    QEffInternVLModel,
 )
-from QEfficient.transformers.models.internvl.modeling_internvl import QEffInternVisionEmbeddings, QEffInternVLModel
 from QEfficient.transformers.models.llama.modeling_llama import (
     QEffLlamaAttention,
     QEffLlamaDecoderLayer,
     QEffLlamaForCausalLM,
     QEffLlamaModel,
 )
+from QEfficient.transformers.models.llama4.modeling_llama4 import (
+    QEffLlama4ForCausalLM,
+    QEffLlama4ForConditionalGeneration,
+    QEffLlama4TextAttention,
+    QEffLlama4TextDecoderLayer,
+    QEffLlama4TextExperts,
+    QEffLlama4TextModel,
+    QEffLlama4TextMoe,
+    QEffLlama4VisionAttention,
+    QEffLlama4VisionModel,
+)
 from QEfficient.transformers.models.llava.modeling_llava import (
     QEffLlavaForConditionalGeneration,
 )
-from QEfficient.transformers.models.llava_next.modeling_llava_next import (
-    QEffLlavaNextForConditionalGeneration,
-)
 from QEfficient.transformers.models.mistral.modeling_mistral import (
     QEffMistralAttention,
     QEffMistralDecoderLayer,
@@ -266,10 +269,7 @@ from QEfficient.transformers.models.whisper.modeling_whisper import (
     QEffWhisperModel,
     QEffWhisperPositionalEmbedding,
 )
-from QEfficient.transformers.post_processing import build_and_attach_mlp, model_type_registry
-from QEfficient.transformers.spd.spd_transform_forward import tlm_forward
-
-SPD_TARGET = "target"
+from QEfficient.transformers.spd.causal_lm_forward import tlm_forward
 
 
 class CustomOpsTransform(ModuleMappingTransform):
@@ -277,13 +277,13 @@ class CustomOpsTransform(ModuleMappingTransform):
         GemmaRMSNorm: GemmaCustomRMSNormAIC,
         Gemma2RMSNorm: GemmaCustomRMSNormAIC,
         LlamaRMSNorm: CustomRMSNormAIC,
+        Llama4TextRMSNorm: CustomRMSNormAIC,
         MistralRMSNorm: CustomRMSNormAIC,
         MixtralRMSNorm: CustomRMSNormAIC,
         Phi3RMSNorm: CustomRMSNormAIC,
         Qwen2RMSNorm: CustomRMSNormAIC,
         MllamaTextRMSNorm: CustomRMSNormAIC,
         GraniteRMSNorm: CustomRMSNormAIC,
-        GraniteMoeRMSNorm: CustomRMSNormAIC,
     }
 
 
@@ -314,10 +314,18 @@ class KVCacheTransform(ModuleMappingTransform):
         LlamaDecoderLayer: QEffLlamaDecoderLayer,
         LlamaModel: QEffLlamaModel,
         LlamaForCausalLM: QEffLlamaForCausalLM,
+        # Llama4
+        Llama4TextAttention: QEffLlama4TextAttention,
+        Llama4ForCausalLM: QEffLlama4ForCausalLM,
+        Llama4TextDecoderLayer: QEffLlama4TextDecoderLayer,
+        Llama4TextModel: QEffLlama4TextModel,
+        Llama4TextMoe: QEffLlama4TextMoe,
+        Llama4ForConditionalGeneration: QEffLlama4ForConditionalGeneration,
+        Llama4VisionAttention: QEffLlama4VisionAttention,
+        Llama4VisionModel: QEffLlama4VisionModel,
+        Llama4TextExperts: QEffLlama4TextExperts,
         # Llava
         LlavaForConditionalGeneration: QEffLlavaForConditionalGeneration,
-        # Llava Next
-        LlavaNextForConditionalGeneration: QEffLlavaNextForConditionalGeneration,
         # Gemma
         GemmaAttention: QEffGemmaAttention,
         GemmaDecoderLayer: QEffGemmaDecoderLayer,
@@ -332,14 +340,6 @@ class KVCacheTransform(ModuleMappingTransform):
         GraniteModel: QEffGraniteModel,
         GraniteForCausalLM: QEffGraniteForCausalLM,
         GraniteAttention: QEffGraniteAttention,
-        # GraniteMoe
-        GraniteMoeModel: QEffGraniteMoeModel,
-        GraniteMoeForCausalLM: QEffGraniteMoeForCausalLM,
-        GraniteMoeAttention: QEffGraniteMoeAttention,
-        GraniteMoeRotaryEmbedding: QEffGraniteMoeRotaryEmbedding,
-        GraniteMoeParallelExperts: QEffGraniteMoeParallelExperts,
-        GraniteMoeTopKGating: QEffGraniteMoeTopKGating,
-        GraniteMoeMoE: QEffGraniteMoeMoE,
         # mllama
         MllamaTextRMSNorm: CustomRMSNormAIC,
         MllamaTextSelfAttention: QEffMllamaTextSelfAttention,
@@ -404,6 +404,8 @@ class KVCacheTransform(ModuleMappingTransform):
     @classmethod
     def apply(cls, model: nn.Module) -> Tuple[nn.Module, bool]:
         model, transformed = super().apply(model)
+        # FIXME: see if we can merge into _module_mapping dict
+        transformers.cache_utils.DynamicCache.update = QEffDynamicCache.update
         return model, transformed
 
 
@@ -426,33 +428,19 @@ class SpDTransform:
     _module_mapping = {
         # Llama
         QEffLlamaForCausalLM,
-        QEffQwen2ForCausalLM,
     }
 
     @classmethod
-    def apply(cls, model: nn.Module, qaic_config: Optional[dict] = None, **kwargs) -> Tuple[nn.Module, bool]:
+    def apply(cls, model: nn.Module) -> Tuple[nn.Module, bool]:
         transformed = False
-        if qaic_config is None or (speculative_model_type := qaic_config.get("speculative_model_type")) is None:
-            return model, transformed
-        elif speculative_model_type not in (
-            supported_spd_model_types := [SPD_TARGET] + list(model_type_registry.keys())
-        ):
-            raise ValueError(
-                f"Specualtive model type {speculative_model_type} is not supported. we currently only support {supported_spd_model_types}"
-            )
-        elif (model_class := model.__class__) in cls._module_mapping:
+        if (model_class := model.__class__) in cls._module_mapping:
             model.forward = MethodType(tlm_forward, model)
-            if speculative_model_type != SPD_TARGET:
-                # build and attach draft mlp
-                pretrained_model_name_or_path = qaic_config["pretrained_model_name_or_path"]
-                model = build_and_attach_mlp(
-                    model, pretrained_model_name_or_path, speculative_model_type=speculative_model_type, **kwargs
-                )
             transformed = True
         else:
             raise NotImplementedError(
                 f"model class {model_class} does not yet support returning multiple logits to keep."
             )
+
         return model, transformed
 
 
diff --git a/examples/llama4_lm_example.py b/examples/llama4_lm_example.py
new file mode 100644
index 0000000..2c13ade
--- /dev/null
+++ b/examples/llama4_lm_example.py
@@ -0,0 +1,50 @@
+# -----------------------------------------------------------------------------
+#
+# Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+# SPDX-License-Identifier: BSD-3-Clause
+#
+# -----------------------------------------------------------------------------
+
+import torch
+from transformers import Llama4ForCausalLM
+
+from QEfficient import QEFFAutoModelForCausalLM
+from QEfficient.utils._utils import load_hf_tokenizer
+from QEfficient.utils.constants import Constants
+from QEfficient.utils.run_utils import ApiRunner
+
+model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"
+model = Llama4ForCausalLM.from_pretrained(
+    model_id, torch_dtype=torch.float32, use_cache=True, attn_implementation="eager"
+)
+model.eval()
+
+tokenizer = load_hf_tokenizer(pretrained_model_name_or_path=model_id)
+config = model.config
+batch_size = len(Constants.INPUT_STR)
+api_runner = ApiRunner(
+    batch_size,
+    tokenizer,
+    config,
+    Constants.INPUT_STR,
+    Constants.PROMPT_LEN,
+    Constants.CTX_LEN,
+)
+
+qeff_model = QEFFAutoModelForCausalLM(model)
+
+qpc_path = qeff_model.compile(
+    prefill_seq_len=128,
+    ctx_len=2048,
+    num_cores=16,
+    mxfp6_matmul=True,
+    mxint8_kv_cache=True,
+    num_devices=8,
+    mos=1,
+    aic_enable_depth_first=True,
+    num_speculative_tokens=None,
+)
+print(f"qpc path is {qpc_path}")
+exec_info = qeff_model.generate(
+    tokenizer, prompts=Constants.INPUT_STR, generation_len=32, device_ids=[0, 1, 2, 3, 4, 5, 6, 7]
+)
diff --git a/examples/llama4_mm_single.py b/examples/llama4_mm_single.py
new file mode 100644
index 0000000..3fe92da
--- /dev/null
+++ b/examples/llama4_mm_single.py
@@ -0,0 +1,67 @@
+# -----------------------------------------------------------------------------
+#
+# Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+# SPDX-License-Identifier: BSD-3-Clause
+#
+# -----------------------------------------------------------------------------
+
+import torch
+import transformers
+from transformers import AutoConfig, AutoModelForImageTextToText, AutoProcessor, TextStreamer
+
+from QEfficient import QEFFAutoModelForImageTextToText
+
+model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"
+config = AutoConfig.from_pretrained(model_id)
+# For Testing Purpose Only
+# config.text_config.num_hidden_layers = 1
+# config.vision_config.num_hidden_layers = 2
+
+model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation="eager", config=config)
+model.eval()
+
+qeff_model = QEFFAutoModelForImageTextToText(model, kv_offload=True)
+
+# TODO: Map the Vision Encoder to FP16 Only and Disable MXFP6 For Better Accuracy.
+qeff_model.compile(
+    prefill_seq_len=128,
+    ctx_len=3072,
+    img_size=336,
+    num_cores=16,
+    num_devices=8,
+    batch_size_times_num_tiles=17,
+    mxfp6_matmul=True,
+    mxint8_kv_cache=True,
+    aic_enable_depth_first=True,
+    mos=1,
+)
+
+image_url = (
+    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png"
+)
+
+messages = [
+    {
+        "role": "user",
+        "content": [
+            {"type": "image", "url": image_url},
+            {"type": "text", "text": "Can you describe the image in detail."},
+        ],
+    },
+]
+
+tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
+processor = AutoProcessor.from_pretrained(model_id)
+inputs = processor.apply_chat_template(
+    messages,
+    add_generation_prompt=True,
+    tokenize=True,
+    return_dict=True,
+    return_tensors="pt",
+)
+inputs["pixel_values"] = inputs["pixel_values"].to(torch.float32)
+streamer = TextStreamer(tokenizer)
+output = qeff_model.generate(inputs=inputs, device_ids=[0, 1, 2, 3, 4, 5, 6, 7], generation_len=100)
+print(output.generated_ids)
+print(tokenizer.batch_decode(output.generated_ids))
+print(output)
diff --git a/pyproject.toml b/pyproject.toml
index 648d2ce..99b7c30 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -19,8 +19,8 @@ classifiers = [
 ]
 requires-python = ">=3.8,<3.11"
 dependencies = [
-    "transformers==4.50.0",
-    "huggingface-hub==0.27.0",
+    "transformers==4.51.0",
+    "huggingface-hub==0.30.0",
     "hf_transfer==0.1.9",
     "peft==0.13.2",
     "datasets==2.20.0",

