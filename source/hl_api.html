<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>High Level API &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/my_theme.css?v=547657ed" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Low Level API" href="ll_api.html" />
    <link rel="prev" title="QEfficient.cloud.infer" href="cli_api.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Upgrade Efficient-Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Using GitHub Repository</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Transformed models and QPC storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html#python-api">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Command Line Interface Use (CLI)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.infer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html#module-QEfficient.cloud.execute.main"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.execute</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html#qefficient-cloud-compile"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html#qefficient-cloud-export"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.export</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">High Level API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#qeffautomodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautopeftmodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.load_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.load_adapter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.active_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.active_adapter</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.set_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.set_adapter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautoloramodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.download_adapter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.load_adapter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.unload_adapter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100"><code class="docutils literal notranslate"><span class="pre">export</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter"><code class="docutils literal notranslate"><span class="pre">qualcomm_efficient_converter()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.compile.compile_helper"><code class="docutils literal notranslate"><span class="pre">compile</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.compile.compile_helper.compile"><code class="docutils literal notranslate"><span class="pre">compile()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.generation.text_generation_inference"><code class="docutils literal notranslate"><span class="pre">Execute</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfo"><code class="docutils literal notranslate"><span class="pre">CloudAI100ExecInfo</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv"><code class="docutils literal notranslate"><span class="pre">cloud_ai_100_exec_kv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.fix_prompts"><code class="docutils literal notranslate"><span class="pre">fix_prompts()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ll_api.html">Low Level API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">High Level API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/hl_api.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><strong>This page give you an overview about the all the APIs that you might need to integrate the <code class="docutils literal notranslate"><span class="pre">QEfficient</span></code> into your python applications.</strong></p>
<section id="high-level-api">
<h1>High Level API<a class="headerlink" href="#high-level-api" title="Permalink to this heading"></a></h1>
<section id="qeffautomodelforcausallm">
<h2><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code><a class="headerlink" href="#qeffautomodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM" title="Permalink to this definition"></a></dt>
<dd><p>The QEFF class is designed for manipulating any causal language model from the HuggingFace hub.
Although it is possible to initialize the class directly, we highly recommend using the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
<dt class="field-even">continuous_batching (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">QEfficient</span> <span class="kn">import</span> <span class="n">QEFFAutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ctx_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>This method serves as the easiest entry point into using QEfficient. The interface is designed to be similar to transformers.AutoModelForCausalLM.
Once the model is initialized, you can use other methods such as export, compile, and generate on the same object.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">pretrained_name_or_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model card name from HuggingFace or local path to model directory.</p>
</dd>
<dt class="field-even">continuous_batching (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</p>
</dd>
<dt class="field-odd">args, kwargs<span class="colon">:</span></dt>
<dd class="field-odd"><p>Additional arguments to pass to transformers.AutoModelForCausalLM.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">QEfficient</span> <span class="kn">import</span> <span class="n">QEFFAutoModelForCausalLM</span>

<span class="c1"># Initialize the model using from_pretrained similar to transformers.AutoModelForCausalLM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># Now you can directly compile the model for Cloud AI 100</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Considering you have a Cloud AI 100 Standard SKU</span>

<span class="c1"># You can now execute the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.
We currently don’t support exporting non-transformed models. Please refer to the <code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle</span></code> function in the <strong>Low-Level API</strong> for a legacy function that supports this.”</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>does not any arguments.</p>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile" title="Permalink to this definition"></a></dt>
<dd><p>This method compiles the exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using the Cloud AI 100 Platform SDK compiler binary found at <code class="docutils literal notranslate"><span class="pre">/opt/qti-aic/exec/qaic-exec</span></code> and generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.
If the model has not been exported yet, this method will handle the export process.
You can pass any other arguments that the <cite>qaic-exec</cite> takes as extra kwargs.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to pre-exported onnx model.</p>
</dd>
<dt class="field-even">compile_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving the qpc generated.</p>
</dd>
<dt class="field-odd">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of cores used to compile the model.</p>
</dd>
<dt class="field-even">num_devices (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of devices for tensor-slicing is invoked, defaults to None, and automatically chooses suitable device.</p>
</dd>
<dt class="field-odd">batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Batch size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-even">prefill_seq_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>The length of the Prefill prompt should be less that <code class="docutils literal notranslate"><span class="pre">prefill_seq_len</span></code>. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">32</span></code>.</p>
</dd>
<dt class="field-odd">ctx_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum <code class="docutils literal notranslate"><span class="pre">ctx</span></code> that the compiled model can remember. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">128</span></code>.</p>
</dd>
<dt class="field-even">full_batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Continuous batching batch size.</p>
</dd>
<dt class="field-odd">mxfp6_matmul (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxfp6</span></code> compression for weights. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">mxint8_kv_cache (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxint8</span></code> compression for KV cache. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">mos (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">-1</span></code>.</p>
</dd>
<dt class="field-even">aic_enable_depth_first (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enables DFS with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'AI_100'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">prompts (List[str])<span class="colon">:</span></dt>
<dd class="field-odd"><p>List of prompts to run the execution.</p>
</dd>
<dt class="field-even">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">runtime (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Only <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> runtime is supported as of now; <code class="docutils literal notranslate"><span class="pre">ONNXRT</span></code> and <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> coming soon. Defaults to “AI_100”.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautopeftmodelforcausallm">
<h2><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code><a class="headerlink" href="#qeffautopeftmodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoPeftModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM" title="Permalink to this definition"></a></dt>
<dd><p>QEff class for loading models with PEFT adapters (Only LoRA is supported currently).
Once exported and compiled for an adapter, the same can be utilized for another adapter with same base model and adapter config.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">QEfficient</span> <span class="kn">import</span> <span class="n">QEffAutoPeftModelForCausalLM</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ctx_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># A coding prompt</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># A math prompt</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.load_adapter">
<span class="sig-name descname"><span class="pre">load_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.load_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.load_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Loads a new adapter from huggingface hub or local path</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to set this adapter as current</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.active_adapter">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">active_adapter</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.active_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Currently active adapter to be used for inference</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.set_adapter">
<span class="sig-name descname"><span class="pre">set_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.set_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.set_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Sets active adapter from one of the loaded adapters</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">pretrained_name_or_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model card name from huggingface or local path to model directory.</p>
</dd>
<dt class="field-even">finite_adapters (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>set True to enable finite adapter mode with QEffAutoLoraModelForCausalLM class. Please refer to QEffAutoLoraModelForCausalLM for API specification.</p>
</dd>
<dt class="field-odd">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Name used to identify loaded adapter.</p>
</dd>
<dt class="field-even">args, kwargs<span class="colon">:</span></dt>
<dd class="field-even"><p>Additional arguments to pass to peft.AutoPeftModelForCausalLM.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">export_dir (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Specify the export directory. The export_dir will be suffixed with a hash corresponding to current model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Path<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported onnx to run on AI100.
If the model has not been exported yet, this method will handle the export process.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Onnx file to compile</p>
</dd>
<dt class="field-even">compile_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Directory path to compile the qpc. A suffix is added to the directory path to avoid reusing same qpc for different parameters.</p>
</dd>
<dt class="field-odd">num_devices (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of devices to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-even">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of cores to utilize in each device <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">16</span></code>.</p>
</dd>
<dt class="field-odd">mxfp6_matmul (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Use MXFP6 to compress weights for MatMul nodes to run faster on device. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">mxint8_kv_cache (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Use MXINT8 to compress KV-cache on device to access and update KV-cache faster. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">compiler_options<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pass any compiler option as input. Any flag that is supported by <code class="docutils literal notranslate"><span class="pre">qaic-exec</span></code> can be passed. Params are converted to flags as below:
- aic_num_cores=16 -&gt; -aic-num-cores=16
- convert_to_fp16=True -&gt; -convert-to-fp16</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">full_batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Full batch size to allocate cache lines.</p>
</dd>
<dt class="field-even">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Batch size to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-odd">prefill_seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Prefill sequence length to compile for. Prompt will be chunked according to this length.</p>
</dd>
<dt class="field-even">ctx_len (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Context length to allocate space for KV-cache tensors.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">GenerationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">StoppingCriteria</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streamer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BaseStreamer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate tokens from compiled binary. This method takes same parameters as HuggingFace transformers model.generate() method.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids</p>
</dd>
<dt class="field-even">generation_config<span class="colon">:</span></dt>
<dd class="field-even"><p>Merge this generation_config with model-specific for the current generation.</p>
</dd>
<dt class="field-odd">stopping_criteria<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pass custom stopping_criteria to stop at a specific point in generation.</p>
</dd>
<dt class="field-even">streamer<span class="colon">:</span></dt>
<dd class="field-even"><p>Streamer to put the generated tokens into.</p>
</dd>
<dt class="field-odd">kwargs<span class="colon">:</span></dt>
<dd class="field-odd"><p>Additional parameters for generation_config or to be passed to the model while generating.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautoloramodelforcausallm">
<h2><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code><a class="headerlink" href="#qeffautoloramodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.lora.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoLoraModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM" title="Permalink to this definition"></a></dt>
<dd><p>QEff class for loading models with multiple LoRA adapters. Currently only Mistral and Llama model are supported.
Once exported and compiled, the qpc can perform mixed batch inference with provided <cite>prompt_to_adapter_mapping</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
<dt class="field-even">continuous_batching (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">QEfficient.peft.lora</span> <span class="kn">import</span> <span class="n">QEffAutoLoraModelForCausalLM</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;code prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;math prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;generic&quot;</span><span class="p">]</span>
<span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prompt_to_adapter_mapping</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;magicoder&quot;</span><span class="p">,</span><span class="s2">&quot;gsm8k_id&quot;</span><span class="p">,</span><span class="s2">&quot;base&quot;</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter">
<span class="sig-name descname"><span class="pre">download_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.download_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Loads a new adapter from huggingface hub or local path into CPU cache</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to downloaded this adapter</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_weight (dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter weight tensors in dictionary format</p>
</dd>
<dt class="field-even">adapter_config (PeftConfig)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter config in the format of PeftConfig</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter">
<span class="sig-name descname"><span class="pre">load_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.load_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Load adapter into CPU cache and set it as active</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to load this adapter</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_weight (dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter weight tensors in dictionary format</p>
</dd>
<dt class="field-even">adapter_config (PeftConfig)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter config in the format of PeftConfig</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter">
<span class="sig-name descname"><span class="pre">unload_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.unload_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter" title="Permalink to this definition"></a></dt>
<dd><p>Deactivate adpater and remove it from CPU cache</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter name to be unloaded</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.
We currently don’t support exporting non-transformed models. Please refer to the <code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle</span></code> function in the <strong>Low-Level API</strong> for a legacy function that supports this.”</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>does not any arguments.</p>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_to_adapter_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'AI_100'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">tokenizer (PreTrainedTokenizerFast or PreTrainedTokenizer)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The tokenizer used in the inference</p>
</dd>
<dt class="field-even">prompts (List[str])<span class="colon">:</span></dt>
<dd class="field-even"><p>List of prompts to run the execution.</p>
</dd>
<dt class="field-odd">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</p>
</dd>
<dt class="field-even">prompt_to_adapter_mapping (List[str])<span class="colon">:</span></dt>
<dd class="field-even"><p>The sequence of the adapter names will be matched with sequence of prompts and corresponding adapters will be used for the prompts.”base” for base model (no adapter).</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">runtime (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Only <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> runtime is supported as of now; <code class="docutils literal notranslate"><span class="pre">ONNXRT</span></code> and <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> coming soon. Defaults to “AI_100”.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-QEfficient.exporter.export_hf_to_cloud_ai_100">
<span id="export"></span><h2><code class="docutils literal notranslate"><span class="pre">export</span></code><a class="headerlink" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">qualcomm_efficient_converter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">QEFFBaseModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_model_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">form_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cloud'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#qualcomm_efficient_converter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter" title="Permalink to this definition"></a></dt>
<dd><p>This method is an alias for <code class="docutils literal notranslate"><span class="pre">QEfficient.export</span></code>.</p>
<p>Usage 1: This method can be used by passing <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">local_model_dir</span></code> or <code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> if required for loading from local dir.
This will download the model from <code class="docutils literal notranslate"><span class="pre">HuggingFace</span></code> and export it to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph and returns generated files path check below.</p>
<p>Usage 2: You can pass <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">model_kv</span></code> as an object of <code class="docutils literal notranslate"><span class="pre">QEfficient.QEFFAutoModelForCausalLM</span></code>, In this case will directly export the <code class="docutils literal notranslate"><span class="pre">model_kv.model</span></code> to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code></p>
<p>We will be deprecating this function and it will be replaced by <code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.export</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The name of the model to be used.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_kv (torch.nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Transformed <code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">torch</span> <span class="pre">model</span></code> to be used. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">local_model_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of local model. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">cache_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of the <code class="docutils literal notranslate"><span class="pre">cache</span></code> directory. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">onnx_dir_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to store <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">hf_token (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>HuggingFace token to access gated models. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The length of the sequence. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">128</span></code>.</p>
</dd>
<dt class="field-even">kv (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If false, it will export to Bert style. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">True</span></code>.</p>
</dd>
<dt class="field-odd">form_factor (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Form factor of the hardware, currently only <code class="docutils literal notranslate"><span class="pre">cloud</span></code> is accepted. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">cloud</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Tuple[str, str]<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to Base <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> dir and path to generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version This: </span>function will be deprecated in version 1.19, please use QEFFAutoModelForCausalLM.export instead</p>
</div>
</section>
<section id="module-QEfficient.compile.compile_helper">
<span id="compile"></span><h2><code class="docutils literal notranslate"><span class="pre">compile</span></code><a class="headerlink" href="#module-QEfficient.compile.compile_helper" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.compile.compile_helper.compile">
<span class="sig-prename descclassname"><span class="pre">QEfficient.compile.compile_helper.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aic_enable_depth_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_io_file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/compile/compile_helper.html#compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.compile.compile_helper.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compiles the given <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using Cloud AI 100 platform SDK compiler and saves the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package at <code class="docutils literal notranslate"><span class="pre">qpc_path</span></code>.
Generates tensor-slicing configuration if multiple devices are passed in <code class="docutils literal notranslate"><span class="pre">device_group</span></code>.</p>
<p>This function will be deprecated soon and will be replaced by <code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.compile</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> Model Path.</p>
</dd>
<dt class="field-even">qpc_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving compiled qpc binaries.</p>
</dd>
<dt class="field-odd">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of cores to compile the model on.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">device_group (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Used for finding the number of devices to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span></code></p>
</dd>
<dt class="field-even">aic_enable_depth_first (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enables <code class="docutils literal notranslate"><span class="pre">DFS</span></code> with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-odd">mos (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Effort level to reduce the on-chip memory. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">-1.</span></code></p>
</dd>
<dt class="field-even">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Batch size to compile the model for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1.</span></code></p>
</dd>
<dt class="field-odd">full_batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Set full batch size to enable continuous batching mode. <code class="docutils literal notranslate"><span class="pre">Default</span> <span class="pre">to</span> <span class="pre">None</span></code></p>
</dd>
<dt class="field-even">prompt_len (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Prompt length for the model to compile. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">32</span></code></p>
</dd>
<dt class="field-odd">ctx_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum context length to compile the model. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">128</span></code></p>
</dd>
<dt class="field-even">mxfp6 (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enable compilation for <code class="docutils literal notranslate"><span class="pre">MXFP6</span></code> precision.  <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">True.</span></code></p>
</dd>
<dt class="field-odd">mxint8 (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Compress Present/Past KV to <code class="docutils literal notranslate"><span class="pre">MXINT8</span></code> using <code class="docutils literal notranslate"><span class="pre">CustomIO</span></code> config. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-even">custom_io_file_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to <code class="docutils literal notranslate"><span class="pre">customIO</span></code> file (formatted as a string). <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">qpc_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="s2">&quot;qpc&quot;</span><span class="p">),</span> <span class="n">num_cores</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version This: </span>function will be deprecated in version 1.19, please use QEFFAutoModelForCausalLM.compile instead</p>
</div>
</section>
<section id="module-QEfficient.generation.text_generation_inference">
<span id="execute"></span><h2><code class="docutils literal notranslate"><span class="pre">Execute</span></code><a class="headerlink" href="#module-QEfficient.generation.text_generation_inference" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.CloudAI100ExecInfo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">CloudAI100ExecInfo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decode_perf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_perf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#CloudAI100ExecInfo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfo" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Holds all the information about Cloud AI 100 execution</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Batch size of the QPC compilation.</p>
</dd>
<dt class="field-even">generated_texts (Union[List[List[str]], List[str]])<span class="colon">:</span></dt>
<dd class="field-even"><p>Generated text(s).</p>
</dd>
<dt class="field-odd">generated_ids (Union[List[np.ndarray], np.ndarray])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated IDs.</p>
</dd>
<dt class="field-even">prefill_time (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>Time for prefilling.</p>
</dd>
<dt class="field-odd">decode_perf (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Decoding performance.</p>
</dd>
<dt class="field-even">total_perf (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>Total performance.</p>
</dd>
<dt class="field-odd">total_time (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Total time.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">cloud_ai_100_exec_kv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts_txt_file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_debug_logs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_io_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">automation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_to_lora_id_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#cloud_ai_100_exec_kv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv" title="Permalink to this definition"></a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer.</p>
</dd>
<dt class="field-even">qpc_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the saved generated binary file after compilation.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">prompt (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Sample prompt for the model text generation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">prompts_txt_file_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of the prompt text file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">generation_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum context length for the model during compilation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Device IDs to be used for execution. If <code class="docutils literal notranslate"><span class="pre">len(device_id)</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, it enables multiple card setup. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, auto-device-picker will be used. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">enable_debug_logs (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, it enables debugging logs. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">stream (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If True, enable streamer, which returns tokens one by one as the model generates them. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">True</span></code>.</p>
</dd>
<dt class="field-odd">Write_io_dir (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to write the input and output files. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">automation (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If true, it prints input, output, and performance stats. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">CloudAI100ExecInfo<span class="colon">:</span></dt>
<dd class="field-odd"><p>Object holding execution output and performance details.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">qpc_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="s2">&quot;qpc&quot;</span><span class="p">),</span> <span class="n">num_cores</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">execinfo</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">cloud_ai_100_exec_kv</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.fix_prompts">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">fix_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#fix_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.fix_prompts" title="Permalink to this definition"></a></dt>
<dd><p>Adjusts the list of prompts to match the required batch size.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><p>prompt (List[str]): List of input prompts.
batch_size (int): The batch size to process at a time.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>full_batch_size (Optional[int]): The full batch size if different from batch_size.</p>
</dd>
<dt>Returns:</dt><dd><p>List[str]: Adjusted list of prompts.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cli_api.html" class="btn btn-neutral float-left" title="QEfficient.cloud.infer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ll_api.html" class="btn btn-neutral float-right" title="Low Level API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>