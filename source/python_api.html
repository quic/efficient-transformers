<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Python API &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/my_theme.css?v=547657ed" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Finetune Infra" href="finetune.html" />
    <link rel="prev" title="Command Line Interface Use (CLI)" href="cli_api.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Release Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="release_docs.html">üöÄ Efficient Transformer Library - Release 1.20.0 (Beta)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Upgrade Efficient-Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Using GitHub Repository</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Cloud AI 100</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html">Command Line Interface Use (CLI)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#high-level-api">High Level API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qeffautomodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qeffautomodel"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qeffautopeftmodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qeffautoloramodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qeffautomodelforimagetexttotext"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qeffautomodelforspeechseq2seq"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100"><code class="docutils literal notranslate"><span class="pre">export</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter"><code class="docutils literal notranslate"><span class="pre">qualcomm_efficient_converter()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.compile.compile_helper"><code class="docutils literal notranslate"><span class="pre">compile</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.compile.compile_helper.compile"><code class="docutils literal notranslate"><span class="pre">compile()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.generation.text_generation_inference"><code class="docutils literal notranslate"><span class="pre">Execute</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfo"><code class="docutils literal notranslate"><span class="pre">CloudAI100ExecInfo</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfoNew"><code class="docutils literal notranslate"><span class="pre">CloudAI100ExecInfoNew</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.PerfMetrics"><code class="docutils literal notranslate"><span class="pre">PerfMetrics</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.calculate_latency"><code class="docutils literal notranslate"><span class="pre">calculate_latency()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv"><code class="docutils literal notranslate"><span class="pre">cloud_ai_100_exec_kv()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.fix_prompt_to_lora_id_mapping"><code class="docutils literal notranslate"><span class="pre">fix_prompt_to_lora_id_mapping()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.fix_prompts"><code class="docutils literal notranslate"><span class="pre">fix_prompts()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.get_compilation_dims"><code class="docutils literal notranslate"><span class="pre">get_compilation_dims()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#low-level-api">Low Level API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_kvstyle</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_kvstyle()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-QEfficient.utils.device_utils"><code class="docutils literal notranslate"><span class="pre">utils</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.device_utils.get_available_device_id"><code class="docutils literal notranslate"><span class="pre">get_available_device_id()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.generate_inputs.InputHandler"><code class="docutils literal notranslate"><span class="pre">InputHandler</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.generate_inputs.InputHandlerInternVL"><code class="docutils literal notranslate"><span class="pre">InputHandlerInternVL</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.generate_inputs.InputHandlerVLM"><code class="docutils literal notranslate"><span class="pre">InputHandlerVLM</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.run_utils.ApiRunner"><code class="docutils literal notranslate"><span class="pre">ApiRunner</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.run_utils.ApiRunnerInternVL"><code class="docutils literal notranslate"><span class="pre">ApiRunnerInternVL</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.utils.run_utils.ApiRunnerVlm"><code class="docutils literal notranslate"><span class="pre">ApiRunnerVlm</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm¬Æ Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models ‚Äì Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Python API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/python_api.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="python-api">
<h1>Python API<a class="headerlink" href="#python-api" title="Permalink to this heading">ÔÉÅ</a></h1>
<p><strong>This page give you an overview about the all the APIs that you might need to integrate the <code class="docutils literal notranslate"><span class="pre">QEfficient</span></code> into your python applications.</strong></p>
<section id="high-level-api">
<h2>High Level API<a class="headerlink" href="#high-level-api" title="Permalink to this heading">ÔÉÅ</a></h2>
<section id="qeffautomodelforcausallm">
<h3><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code><a class="headerlink" href="#qeffautomodelforcausallm" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qaic_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>The QEFF class is designed for manipulating any causal language model from the HuggingFace hub.
Although it is possible to initialize the class directly, we highly recommend using the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
<dt class="field-even">continuous_batching (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">qaic_config (dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>QAIC config dictionary with the following supported keys:
:speculative_model_type (str): To specify Speculative Decoding Target Language Models.
:include_sampler (bool): Enable/Disable sampling of next tokens.
:return_pdfs (bool): Return probability distributions along with sampled
next tokens. For Speculative Decoding Target Language Model,
<cite>return_pdfs`=True always. Otherwise, `return_pdfs`=True for Speculative
Decoding Draft Language Model and `return_pdfs`=False for regular model.
:max_top_k_ids (int): Specify the maximum number of top K tokens
(&lt;= vocab size) to consider during sampling. The values provided in
`top_ks</cite> tensor must be less than this maximum limit.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">ctx_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">export_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The directory path to store ONNX-graph.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.get_sampling_inputs_and_outputs">
<span class="sig-name descname"><span class="pre">get_sampling_inputs_and_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic_axes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.get_sampling_inputs_and_outputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.get_sampling_inputs_and_outputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update the example inputs and outputs with respect to the On Device Sampler
for the ONNX export.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_cache_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_speculative_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method compiles the exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using the Cloud AI 100 Platform SDK compiler binary found at <code class="docutils literal notranslate"><span class="pre">/opt/qti-aic/exec/qaic-exec</span></code> and generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.
If the model has not been exported yet, this method will handle the export process.
You can pass any other arguments that the <cite>qaic-exec</cite> takes as extra kwargs.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list">
<dt class="field-odd">onnx_path (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to pre-exported onnx model.</p>
</dd>
<dt class="field-even">compile_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving the qpc generated.</p>
</dd>
<dt class="field-odd">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of cores used to compile the model.</p>
</dd>
<dt class="field-even">num_devices (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of devices the model needs to be compiled for. Defaults to 1.</p>
</dd>
<dt class="field-odd">batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Batch size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-even">prefill_seq_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>The length of the Prefill prompt should be less that <code class="docutils literal notranslate"><span class="pre">prefill_seq_len</span></code>. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">32</span></code>.</p>
</dd>
<dt class="field-odd">ctx_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum <code class="docutils literal notranslate"><span class="pre">ctx</span></code> that the compiled model can remember. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">128</span></code>.</p>
</dd>
<dt class="field-even">full_batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Continuous batching batch size.</p>
</dd>
<dt class="field-odd">mxfp6_matmul (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxfp6</span></code> compression for weights. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">mxint8_kv_cache (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxint8</span></code> compression for KV cache. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">num_speculative_tokens (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of speculative tokens to take as input for Speculative Decoding Target Language Model.</p>
</dd>
<dt class="field-even">prefill_only (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code> compile for prefill only and if <code class="docutils literal notranslate"><span class="pre">False</span></code> compile for decode only. Defaults to None, which compiles for both <code class="docutils literal notranslate"><span class="pre">prefill</span> <span class="pre">and</span> <span class="pre">``decode</span></code>.</p>
</dd>
<dt class="field-odd">compiler_options (dict, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Additional compiler options. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.
For QAIC Compiler: Extra arguments for qaic-exec can be passed.</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">mos (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">-1</span></code>.</p>
</dd>
<dt class="field-even">aic_enable_depth_first (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enables DFS with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">allow_mxint8_mdp_io (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Allows MXINT8 compression of MDP IO traffic. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
</dl>
<p>Params are converted to flags as below:
- aic_num_cores=16 -&gt; -aic-num-cores=16
- convert_to_fp16=True -&gt; -convert-to-fp16</p>
</div></blockquote>
<dl class="simple">
<dt>For QNN Compiler: Following arguments can be passed.</dt><dd><dl class="field-list simple">
<dt class="field-odd">enable_qnn (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Enables QNN Compilation.</p>
</dd>
<dt class="field-even">qnn_config (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime_ai100</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizerFast, PreTrainedTokenizer])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pass tokenizer of the model.</p>
</dd>
<dt class="field-even">prompts (List[str])<span class="colon">:</span></dt>
<dd class="field-even"><p>List of prompts to run the execution.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</p>
</dd>
<dt class="field-even">runtime_ai100 (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">AI_100</span></code> and <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> runtime is supported as of now. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> for <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> runtime.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautomodel">
<span id="id1"></span><h3><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel</span></code><a class="headerlink" href="#qeffautomodel" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>The QEFFAutoModel class is designed for manipulating any transformer model from the HuggingFace hub.
Although it is possible to initialize the class directly, we highly recommend using the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Initialize the model using from_pretrained similar to transformers.AutoModel.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;model_name&quot;</span><span class="p">)</span>

<span class="c1"># Now you can directly compile the model for Cloud AI 100</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>  <span class="c1"># Considering you have a Cloud AI 100 SKU</span>

<span class="c1">#prepare input</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;My name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># You can now execute the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">export_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The directory path to store ONNX-graph.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method compiles the exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using the Cloud AI 100 Platform SDK compiler binary found at <code class="docutils literal notranslate"><span class="pre">/opt/qti-aic/exec/qaic-exec</span></code> and generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.
If the model has not been exported yet, this method will handle the export process.
You can pass any other arguments that the <cite>qaic-exec</cite> takes as extra kwargs.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list">
<dt class="field-odd">onnx_path (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to pre-exported onnx model.</p>
</dd>
<dt class="field-even">compile_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving the qpc generated.</p>
</dd>
<dt class="field-odd">seq_len (Union[int, List[int]])<span class="colon">:</span></dt>
<dd class="field-odd"><p>The length of the prompt should be less that <code class="docutils literal notranslate"><span class="pre">seq_len</span></code>. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">32</span></code>.</p>
</dd>
<dt class="field-even">batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Batch size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-odd">num_devices (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of devices the model needs to be compiled for. Defaults to 1.</p>
</dd>
<dt class="field-even">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of cores used to compile the model.</p>
</dd>
<dt class="field-odd">mxfp6_matmul (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxfp6</span></code> compression for weights. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">compiler_options (dict, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Additional compiler options.
For QAIC Compiler: Extra arguments for qaic-exec can be passed.</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">aic_enable_depth_first (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Enables DFS with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">allow_mxint8_mdp_io (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Allows MXINT8 compression of MDP IO traffic. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>For QNN Compiler: Following arguments can be passed.</dt><dd><dl class="field-list simple">
<dt class="field-odd">enable_qnn (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Enables QNN Compilation.</p>
</dd>
<dt class="field-even">qnn_config (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime_ai100</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method generates output by executing PyTorch runtime or the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
<code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">inputs (Union[torch.Tensor, np.ndarray])<span class="colon">:</span></dt>
<dd class="field-odd"><p>inputs to run the execution.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</p>
</dd>
<dt class="field-even">runtime_ai100 (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">AI_100</span></code> and <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> runtime is supported as of now. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> for <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> runtime.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Output from the <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> or <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> runtime.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.cloud_ai_100_feature_generate">
<span class="sig-name descname"><span class="pre">cloud_ai_100_feature_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[0]</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.cloud_ai_100_feature_generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.cloud_ai_100_feature_generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Generates features with list of prompts using AI 100 runtime.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs (Union[torch.Tensor, np.ndarray])<span class="colon">:</span></dt>
<dd class="field-odd"><p>inputs to run the execution.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>device_ids (List[int], optional): A list of device IDs to use for the session. Defaults to [0].</p>
</dd>
<dt>Returns:</dt><dd><p>np.ndarray: A list of dictionaries containing the generated output features.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.pytorch_feature_generate">
<span class="sig-name descname"><span class="pre">pytorch_feature_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.pytorch_feature_generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.pytorch_feature_generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Generates features from a list of text prompts using a PyTorch model.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model<span class="colon">:</span></dt>
<dd class="field-odd"><p>The transformed PyTorch model used for generating features.</p>
</dd>
<dt class="field-even">inputs (Union[torch.Tensor, np.ndarray])<span class="colon">:</span></dt>
<dd class="field-even"><p>inputs to run the execution.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>torch.Tensor: A list of output features generated by the model for each prompt.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautopeftmodelforcausallm">
<span id="id2"></span><h3><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code><a class="headerlink" href="#qeffautopeftmodelforcausallm" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoPeftModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>QEff class for loading models with PEFT adapters (Only LoRA is supported currently).
Once exported and compiled for an adapter, the same can be utilized for another adapter with same base model and adapter config.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEffAutoPeftModelForCausalLM</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ctx_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># A coding prompt</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># A math prompt</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.load_adapter">
<span class="sig-name descname"><span class="pre">load_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.load_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.load_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Loads a new adapter from huggingface hub or local path</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to set this adapter as current</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.active_adapter">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">active_adapter</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.active_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Currently active adapter to be used for inference</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.set_adapter">
<span class="sig-name descname"><span class="pre">set_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.set_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.set_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Sets active adapter from one of the loaded adapters</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">pretrained_name_or_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model card name from huggingface or local path to model directory.</p>
</dd>
<dt class="field-even">finite_adapters (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>set True to enable finite adapter mode with QEffAutoLoraModelForCausalLM class. Please refer to QEffAutoLoraModelForCausalLM for API specification.</p>
</dd>
<dt class="field-odd">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Name used to identify loaded adapter.</p>
</dd>
<dt class="field-even">args, kwargs<span class="colon">:</span></dt>
<dd class="field-even"><p>Additional arguments to pass to peft.AutoPeftModelForCausalLM.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">export_dir (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Specify the export directory. The export_dir will be suffixed with a hash corresponding to current model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Path<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Compile the exported onnx to run on AI100.
If the model has not been exported yet, this method will handle the export process.</p>
<dl>
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Onnx file to compile</p>
</dd>
<dt class="field-even">compile_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Directory path to compile the qpc. A suffix is added to the directory path to avoid reusing same qpc for different parameters.</p>
</dd>
<dt class="field-odd">num_devices (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of devices to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-even">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of cores to utilize in each device <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">16</span></code>.</p>
</dd>
<dt class="field-odd">mxfp6_matmul (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Use MXFP6 to compress weights for MatMul nodes to run faster on device. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">mxint8_kv_cache (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Use MXINT8 to compress KV-cache on device to access and update KV-cache faster. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">compiler_options<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pass any compiler option as input.</p>
</dd>
</dl>
<dl class="simple">
<dt>Following flag can be passed in compiler_options to enable QNN Compilation path.</dt><dd><dl class="field-list simple">
<dt class="field-odd">enable_qnn (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Enables QNN Compilation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span> <span class="pre">if</span> <span class="pre">not</span> <span class="pre">passed.</span></code></p>
</dd>
<dt class="field-even">qnn_config (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of QNN Config parameters file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span> <span class="pre">if</span> <span class="pre">not</span> <span class="pre">passed</span></code></p>
</dd>
</dl>
</dd>
<dt>for QAIC compilation path, any flag that is supported by <code class="docutils literal notranslate"><span class="pre">qaic-exec</span></code> can be passed. Params are converted to flags as below:</dt><dd><ul class="simple">
<li><p>aic_num_cores=16 -&gt; -aic-num-cores=16</p></li>
<li><p>convert_to_fp16=True -&gt; -convert-to-fp16</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">full_batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Full batch size to allocate cache lines.</p>
</dd>
<dt class="field-even">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Batch size to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-odd">prefill_seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Prefill sequence length to compile for. Prompt will be chunked according to this length.</p>
</dd>
<dt class="field-even">ctx_len (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Context length to allocate space for KV-cache tensors.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">GenerationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">StoppingCriteria</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streamer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BaseStreamer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Generate tokens from compiled binary. This method takes same parameters as HuggingFace transformers model.generate() method.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids</p>
</dd>
<dt class="field-even">generation_config<span class="colon">:</span></dt>
<dd class="field-even"><p>Merge this generation_config with model-specific for the current generation.</p>
</dd>
<dt class="field-odd">stopping_criteria<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pass custom stopping_criteria to stop at a specific point in generation.</p>
</dd>
<dt class="field-even">streamer<span class="colon">:</span></dt>
<dd class="field-even"><p>Streamer to put the generated tokens into.</p>
</dd>
<dt class="field-odd">kwargs<span class="colon">:</span></dt>
<dd class="field-odd"><p>Additional parameters for generation_config or to be passed to the model while generating.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautoloramodelforcausallm">
<span id="id3"></span><h3><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code><a class="headerlink" href="#qeffautoloramodelforcausallm" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.lora.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoLoraModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>QEff class for loading models with multiple LoRA adapters. Currently only Mistral and Llama model are supported.
Once exported and compiled, the qpc can perform mixed batch inference with provided <cite>prompt_to_adapter_mapping</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
<dt class="field-even">continuous_batching (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.peft.lora</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEffAutoLoraModelForCausalLM</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;code prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;math prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;generic&quot;</span><span class="p">]</span>
<span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prompt_to_adapter_mapping</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;magicoder&quot;</span><span class="p">,</span><span class="s2">&quot;gsm8k_id&quot;</span><span class="p">,</span><span class="s2">&quot;base&quot;</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter">
<span class="sig-name descname"><span class="pre">download_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.download_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Loads a new adapter from huggingface hub or local path into CPU cache</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to downloaded this adapter</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_weight (dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter weight tensors in dictionary format</p>
</dd>
<dt class="field-even">adapter_config (PeftConfig)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter config in the format of PeftConfig</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter">
<span class="sig-name descname"><span class="pre">load_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_model_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adapter_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.load_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Load adapter into CPU cache and set it as active</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_model_id (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter model ID from huggingface hub or local path</p>
</dd>
<dt class="field-even">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter name to be used to load this adapter</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_weight (dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter weight tensors in dictionary format</p>
</dd>
<dt class="field-even">adapter_config (PeftConfig)<span class="colon">:</span></dt>
<dd class="field-even"><p>Adapter config in the format of PeftConfig</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter">
<span class="sig-name descname"><span class="pre">unload_adapter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adapter_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.unload_adapter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Deactivate adpater and remove it from CPU cache</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">adapter_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Adapter name to be unloaded</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.
We currently don‚Äôt support exporting non-transformed models. Please refer to the <code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle</span></code> function in the <strong>Low-Level API</strong> for a legacy function that supports this.‚Äù</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>does not any arguments.</p>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_to_adapter_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'AI_100'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">tokenizer (PreTrainedTokenizerFast or PreTrainedTokenizer)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The tokenizer used in the inference</p>
</dd>
<dt class="field-even">prompts (List[str])<span class="colon">:</span></dt>
<dd class="field-even"><p>List of prompts to run the execution.</p>
</dd>
<dt class="field-odd">prompt_to_adapter_mapping (List[str])<span class="colon">:</span></dt>
<dd class="field-odd"><p>The sequence of the adapter names will be matched with sequence of prompts and corresponding adapters will be used for the prompts.‚Äùbase‚Äù for base model (no adapter).</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Device IDs to be used for execution. If <code class="docutils literal notranslate"><span class="pre">len(device_id)</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, it enables multiple card setup. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, auto-device-picker will be used. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">runtime (str, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Only <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> runtime is supported as of now; <code class="docutils literal notranslate"><span class="pre">ONNXRT</span></code> and <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> coming soon. Defaults to ‚ÄúAI_100‚Äù.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="qeffautomodelforimagetexttotext">
<span id="id4"></span><h3><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText</span></code><a class="headerlink" href="#qeffautomodelforimagetexttotext" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForImageTextToText</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_offload</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForImageTextToText"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>The QEFFAutoModelForImageTextToText class is used to work with multimodal language models from the HuggingFace hub.
While you can initialize the class directly, it‚Äôs best to use the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for this purpose. This class supports both single and dual QPC approaches.
Attributes:</p>
<blockquote>
<div><p>_hf_auto_class (class): The Hugging Face AutoModel class for ImageTextToText models.</p>
</div></blockquote>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">pretrained_model_name_or_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model card name from HuggingFace or local path to model directory.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">kv_offload (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Flag to toggle between single and dual QPC approaches. If set to False, the Single QPC approach will be used; otherwise, the dual QPC approach will be applied. Defaults to True.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForImageTextToText</span>

<span class="c1"># Add HuggingFace Token to access the model</span>
<span class="n">HF_TOKEN</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-11B-Vision-Instruct&quot;</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Describe this image.&quot;</span>
<span class="n">image_url</span> <span class="o">=</span> <span class="s2">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg&quot;</span>

<span class="c1">## STEP - 1 Load the Processor and Model, and kv_offload=True/False for dual and single qpc</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">HF_TOKEN</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForImageTextToText</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">HF_TOKEN</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="n">kv_offload</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">## STEP - 2 Export &amp; Compile the Model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">ctx_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">img_size</span><span class="o">=</span><span class="mi">560</span><span class="p">,</span>
    <span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">## STEP - 3 Load and process the inputs for Inference</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">]</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
    <span class="n">text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span>
    <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">## STEP - 4 Run Inference on the compiled model</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="qeffautomodelforspeechseq2seq">
<span id="id5"></span><h3><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></code><a class="headerlink" href="#qeffautomodelforspeechseq2seq" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>The QEFFAutoModelForSpeechSeq2Seq class is designed for transformers models with a sequence-to-sequence speech-to-text modeling head, including Whisper and other Encoder-Decoder speech models.
Although it is possible to initialize the class directly, we highly recommend using the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model (nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForSpeechSeq2Seq</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span>

<span class="c1"># Initialize the model using from_pretrained similar to transformers.AutoModelForSpeechSeq2Seq.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForSpeechSeq2Seq</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;model_name&quot;</span><span class="p">)</span>

<span class="c1"># Now you can directly compile the model for Cloud AI 100</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Considering you have a Cloud AI 100 SKU</span>

<span class="c1">#prepare inputs</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">input_audio</span><span class="p">,</span> <span class="n">sample_rate</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="c1"># audio data loaded in via some external audio package, such as librosa or soundfile</span>
<span class="n">input_features</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">processor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_features</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
<span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">decoder_position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
    <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
    <span class="n">decoder_position_ids</span><span class="o">=</span><span class="n">decoder_position_ids</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># You can now execute the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Exports the model to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:
:export_dir (str, optional): The directory path to store ONNX-graph.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">150</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_cache_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_speculative_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method compiles the exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using the Cloud AI 100 Platform SDK compiler binary found at <code class="docutils literal notranslate"><span class="pre">/opt/qti-aic/exec/qaic-exec</span></code> and generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.
If the model has not been exported yet, this method will handle the export process.
You can pass any other arguments that the <cite>qaic-exec</cite> takes as extra kwargs.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to pre-exported onnx model.</p>
</dd>
<dt class="field-even">compile_dir (str, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving the qpc generated.</p>
</dd>
<dt class="field-odd">encoder_ctx_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The maximum length of context for encoder, based on the AutoProcessor output. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">checking</span> <span class="pre">config,</span> <span class="pre">if</span> <span class="pre">None</span> <span class="pre">in</span> <span class="pre">config</span> <span class="pre">then</span> <span class="pre">1500</span></code></p>
</dd>
<dt class="field-even">ctx_len (int, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>The maximum length of context to keep for decoding. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">150</span></code>.</p>
</dd>
<dt class="field-odd">batch_size (int, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Batch size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1</span></code>.</p>
</dd>
<dt class="field-even">num_devices (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of devices the model needs to be compiled for. Defaults to 1.</p>
</dd>
<dt class="field-odd">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of cores used to compile the model.</p>
</dd>
<dt class="field-even">mxfp6_matmul (bool, optional)<span class="colon">:</span></dt>
<dd class="field-even"><p>Whether to use <code class="docutils literal notranslate"><span class="pre">mxfp6</span></code> compression for weights. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">aic_enable_depth_first (bool, optional)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Enables DFS with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
</dl>
<p>Other args are not yet implemented for AutoModelForSpeechSeq2Seq</p>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streamer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TextStreamer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">endoftranscript</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of audio tensor passed.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">processor<span class="colon">:</span></dt>
<dd class="field-odd"><p>autoprocessor to process inputs and decode logits</p>
</dd>
<dt class="field-even">inputs (torch.Tensor)<span class="colon">:</span></dt>
<dd class="field-even"><p>inputs to run the execution.</p>
</dd>
<dt class="field-odd">generation_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>length upto which to generate</p>
</dd>
<dt class="field-even">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Output from the <code class="docutils literal notranslate"><span class="pre">AI_100</span></code> or <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> runtime.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-QEfficient.exporter.export_hf_to_cloud_ai_100">
<span id="export"></span><h3><code class="docutils literal notranslate"><span class="pre">export</span></code><a class="headerlink" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">qualcomm_efficient_converter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">QEFFBaseModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_model_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">form_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cloud'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#qualcomm_efficient_converter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method is an alias for <code class="docutils literal notranslate"><span class="pre">QEfficient.export</span></code>.</p>
<p>Usage 1: This method can be used by passing <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">local_model_dir</span></code> or <code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> if required for loading from local dir.
This will download the model from <code class="docutils literal notranslate"><span class="pre">HuggingFace</span></code> and export it to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> graph and returns generated files path check below.</p>
<p>Usage 2: You can pass <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">model_kv</span></code> as an object of <code class="docutils literal notranslate"><span class="pre">QEfficient.QEFFAutoModelForCausalLM</span></code>, In this case will directly export the <code class="docutils literal notranslate"><span class="pre">model_kv.model</span></code> to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code></p>
<p>We will be deprecating this function and it will be replaced by <code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.export</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The name of the model to be used.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_kv (torch.nn.Module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Transformed <code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">torch</span> <span class="pre">model</span></code> to be used. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">local_model_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of local model. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">cache_dir (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of the <code class="docutils literal notranslate"><span class="pre">cache</span></code> directory. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">onnx_dir_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to store <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">hf_token (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>HuggingFace token to access gated models. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The length of the sequence. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">128</span></code>.</p>
</dd>
<dt class="field-even">kv (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If false, it will export to Bert style. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">is</span> <span class="pre">True</span></code>.</p>
</dd>
<dt class="field-odd">form_factor (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Form factor of the hardware, currently only <code class="docutils literal notranslate"><span class="pre">cloud</span></code> is accepted. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">cloud</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Tuple[str, str]<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to Base <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> dir and path to generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version This: </span>function will be deprecated in version 1.19, please use QEFFAutoModelForCausalLM.export instead</p>
</div>
</section>
<section id="module-QEfficient.compile.compile_helper">
<span id="compile"></span><h3><code class="docutils literal notranslate"><span class="pre">compile</span></code><a class="headerlink" href="#module-QEfficient.compile.compile_helper" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.compile.compile_helper.compile">
<span class="sig-prename descclassname"><span class="pre">QEfficient.compile.compile_helper.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aic_enable_depth_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_io_file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_mxint8_mdp_io</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_qnn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qnn_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/compile/compile_helper.html#compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.compile.compile_helper.compile" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Compiles the given <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model using Cloud AI 100 platform SDK compiler and saves the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package at <code class="docutils literal notranslate"><span class="pre">qpc_path</span></code>.
Generates tensor-slicing configuration if multiple devices are passed in <code class="docutils literal notranslate"><span class="pre">device_group</span></code>.</p>
<p>This function will be deprecated soon and will be replaced by <code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.compile</span></code>.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">onnx_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> Model Path.</p>
</dd>
<dt class="field-even">qpc_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path for saving compiled qpc binaries.</p>
</dd>
<dt class="field-odd">num_cores (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of cores to compile the model on.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">device_group (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Used for finding the number of devices to compile for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span></code></p>
</dd>
<dt class="field-even">aic_enable_depth_first (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enables <code class="docutils literal notranslate"><span class="pre">DFS</span></code> with default memory size. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-odd">mos (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Effort level to reduce the on-chip memory. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">-1.</span></code></p>
</dd>
<dt class="field-even">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Batch size to compile the model for. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">1.</span></code></p>
</dd>
<dt class="field-odd">full_batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Set full batch size to enable continuous batching mode. <code class="docutils literal notranslate"><span class="pre">Default</span> <span class="pre">to</span> <span class="pre">None</span></code></p>
</dd>
<dt class="field-even">prompt_len (int)<span class="colon">:</span></dt>
<dd class="field-even"><p>Prompt length for the model to compile. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">32</span></code></p>
</dd>
<dt class="field-odd">ctx_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum context length to compile the model. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">128</span></code></p>
</dd>
<dt class="field-even">mxfp6 (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enable compilation for <code class="docutils literal notranslate"><span class="pre">MXFP6</span></code> precision.  <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">True.</span></code></p>
</dd>
<dt class="field-odd">mxint8 (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Compress Present/Past KV to <code class="docutils literal notranslate"><span class="pre">MXINT8</span></code> using <code class="docutils literal notranslate"><span class="pre">CustomIO</span></code> config. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-even">custom_io_file_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to <code class="docutils literal notranslate"><span class="pre">customIO</span></code> file (formatted as a string). <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span></code></p>
</dd>
<dt class="field-odd">allow_mxint8_mdp_io (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Allows MXINT8 compression of MDP IO traffic <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-even">enable_qnn (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>Enables QNN Compilation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False.</span></code></p>
</dd>
<dt class="field-odd">qnn_config (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of QNN Config parameters file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None.</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">qpc_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="s2">&quot;qpc&quot;</span><span class="p">),</span> <span class="n">num_cores</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version This: </span>function will be deprecated in version 1.19, please use QEFFAutoModelForCausalLM.compile instead</p>
</div>
</section>
<section id="module-QEfficient.generation.text_generation_inference">
<span id="execute"></span><h3><code class="docutils literal notranslate"><span class="pre">Execute</span></code><a class="headerlink" href="#module-QEfficient.generation.text_generation_inference" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.CloudAI100ExecInfo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">CloudAI100ExecInfo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perf_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.PerfMetrics" title="QEfficient.generation.text_generation_inference.PerfMetrics"><span class="pre">PerfMetrics</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#CloudAI100ExecInfo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfo" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Holds all the information about Cloud AI 100 execution</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">batch_size (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Batch size of the QPC compilation.</p>
</dd>
<dt class="field-even">generated_texts (Union[List[List[str]], List[str]])<span class="colon">:</span></dt>
<dd class="field-even"><p>Generated text(s).</p>
</dd>
<dt class="field-odd">generated_ids (Union[List[np.ndarray], np.ndarray])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated IDs.</p>
</dd>
<dt class="field-even">perf_metrics (PerfMetrics)<span class="colon">:</span></dt>
<dd class="field-even"><p>Performance metrics.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.CloudAI100ExecInfoNew">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">CloudAI100ExecInfoNew</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.ndarray</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perf_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.PerfMetrics" title="QEfficient.generation.text_generation_inference.PerfMetrics"><span class="pre">QEfficient.generation.text_generation_inference.PerfMetrics</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#CloudAI100ExecInfoNew"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.CloudAI100ExecInfoNew" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.PerfMetrics">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">PerfMetrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefill_time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decode_perf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_perf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#PerfMetrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.PerfMetrics" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Holds all performance metrics</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">prefill_time (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Time for prefilling.</p>
</dd>
<dt class="field-even">decode_perf (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoding performance.</p>
</dd>
<dt class="field-odd">total_perf (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Total performance.</p>
</dd>
<dt class="field-even">total_time (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>Total time.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.calculate_latency">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">calculate_latency</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_decoded_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loop_start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decode_pause_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#calculate_latency"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.calculate_latency" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Method will calculate the latency metrics using the time loops and based on the total decoded token count.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">total_decoded_tokens (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of tokens generated in decode stage.</p>
</dd>
<dt class="field-even">loop_start (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>Start time of decode loop.</p>
</dd>
<dt class="field-odd">start (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Start time.</p>
</dd>
<dt class="field-even">end (float)<span class="colon">:</span></dt>
<dd class="field-even"><p>End time.</p>
</dd>
<dt class="field-odd">decode_pause_time (float)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Total decode pause time in continuous batching decode stage.</p>
</dd>
</dl>
</dd>
</dl>
<p>Returns:
:tuple: prefill time, decode performance, total performance, total time</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">cloud_ai_100_exec_kv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts_txt_file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_debug_logs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_io_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">automation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_to_lora_id_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_tlm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#cloud_ai_100_exec_kv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This method generates output until <code class="docutils literal notranslate"><span class="pre">eos</span></code> or <code class="docutils literal notranslate"><span class="pre">generation_len</span></code> by executing the compiled <code class="docutils literal notranslate"><span class="pre">qpc</span></code> on <code class="docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">AI</span> <span class="pre">100</span></code> Hardware cards.
This is a sequential execution based on the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of the compiled model and the number of prompts passed.
If the number of prompts cannot be divided by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the last unfulfilled batch will be dropped.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer.</p>
</dd>
<dt class="field-even">qpc_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the saved generated binary file after compilation.</p>
</dd>
</dl>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">prompt (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Sample prompt for the model text generation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">prompts_txt_file_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path of the prompt text file. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">generation_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Maximum context length for the model during compilation. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">device_id (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Device IDs to be used for execution. If <code class="docutils literal notranslate"><span class="pre">len(device_id)</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, it enables multiple card setup. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, auto-device-picker will be used. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-odd">enable_debug_logs (bool)<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, it enables debugging logs. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">stream (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If True, enable streamer, which returns tokens one by one as the model generates them. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">True</span></code>.</p>
</dd>
<dt class="field-odd">Write_io_dir (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to write the input and output files. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">None</span></code>.</p>
</dd>
<dt class="field-even">automation (bool)<span class="colon">:</span></dt>
<dd class="field-even"><p>If true, it prints input, output, and performance stats. <code class="docutils literal notranslate"><span class="pre">Defaults</span> <span class="pre">to</span> <span class="pre">False</span></code>.</p>
</dd>
<dt class="field-odd">prompt_to_lora_id_mapping (List[int])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Mapping to associate prompts with their respective LoRA adapter.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">CloudAI100ExecInfo<span class="colon">:</span></dt>
<dd class="field-odd"><p>Object holding execution output and performance details.</p>
</dd>
</dl>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">QEfficient</span>
<span class="n">base_path</span><span class="p">,</span> <span class="n">onnx_model_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">qpc_path</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="s2">&quot;qpc&quot;</span><span class="p">),</span> <span class="n">num_cores</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">device_group</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">exec_info</span> <span class="o">=</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">cloud_ai_100_exec_kv</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">qpc_path</span><span class="o">=</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.fix_prompt_to_lora_id_mapping">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">fix_prompt_to_lora_id_mapping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt_to_lora_id_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#fix_prompt_to_lora_id_mapping"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.fix_prompt_to_lora_id_mapping" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Adjusts the list of prompt_to_lora_id_mapping to match the required batch size.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><p>prompt_to_lora_id_mapping (Optional[List[int]]): Mapping to associate prompts with their respective LoRA adapter.
batch_size (int): The batch size to process at a time.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>full_batch_size (Optional[int]): The full batch size if different from batch_size.</p>
</dd>
<dt>Returns:</dt><dd><p>List[int]: Adjusted list of prompt_to_lora_id_mapping.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.fix_prompts">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">fix_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#fix_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.fix_prompts" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Adjusts the list of prompts to match the required batch size.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><p>prompt (List[str]): List of input prompts.
batch_size (int): The batch size to process at a time.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional</span></code> Args:</dt><dd><p>full_batch_size (Optional[int]): The full batch size if different from batch_size.</p>
</dd>
<dt>Returns:</dt><dd><p>List[str]: Adjusted list of prompts.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.get_compilation_dims">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">get_compilation_dims</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#get_compilation_dims"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.get_compilation_dims" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function to fetch compilation dimensions from specializations.json.
Uses qpc path to compute path to specializations.json.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>qpc_path (str): Path to directory comprising generated binary file after compilation.</p>
</dd>
</dl>
<p>Returns:
:tuple: compilation batch size, compilation context length, compilation full batch size</p>
</dd></dl>

</section>
</section>
<section id="low-level-api">
<h2>Low Level API<a class="headerlink" href="#low-level-api" title="Permalink to this heading">ÔÉÅ</a></h2>
<section id="module-QEfficient.exporter.export_hf_to_cloud_ai_100">
<span id="convert-to-cloud-kvstyle"></span><h3><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_kvstyle</span></code><a class="headerlink" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">convert_to_cloud_kvstyle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qeff_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM" title="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM"><span class="pre">QEFFAutoModelForCausalLM</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#convert_to_cloud_kvstyle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>API to convert model with kv retention and export to ONNX.
KV Style Approach-</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>This architecture is particularly suitable for auto-regressive tasks.</p></li>
<li><p>where sequence generation involves processing one token at a time.</p></li>
<li><p>And contextual information from earlier tokens is crucial for predicting the next token.</p></li>
<li><p>The inclusion of a kV cache enhances the efficiency of the decoding process, making it more computationally efficient.</p></li>
</ol>
</div></blockquote>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Hugging Face Model Card name, Example: <cite>gpt2</cite>.</p>
</dd>
<dt class="field-even">qeff_model (QEFFAutoModelForCausalLM)<span class="colon">:</span></dt>
<dd class="field-even"><p>Transformed KV torch model to be used.</p>
</dd>
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer.</p>
</dd>
<dt class="field-even">onnx_dir_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to save exported ONNX file.</p>
</dd>
<dt class="field-odd">seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The length of the sequence.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-QEfficient.exporter.export_hf_to_cloud_ai_100">
<span id="convert-to-cloud-bertstyle"></span><h3><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle</span></code><a class="headerlink" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">convert_to_cloud_bertstyle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qeff_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM" title="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM"><span class="pre">QEFFAutoModelForCausalLM</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#convert_to_cloud_bertstyle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>API to convert model to Bertstyle approach.
Bertstyle Approach:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>No Prefill/Decode separably compiled.</p></li>
<li><p>No KV retention logic.</p></li>
<li><p>KV is every time computed for all the tokens until EOS/max_length.</p></li>
</ol>
</div></blockquote>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_name (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Hugging Face Model Card name, Example: <cite>gpt2</cite>.</p>
</dd>
<dt class="field-even">qeff_model (QEFFAutoModelForCausalLM)<span class="colon">:</span></dt>
<dd class="field-even"><p>Transformed KV torch model to be used.</p>
</dd>
<dt class="field-odd">tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast])<span class="colon">:</span></dt>
<dd class="field-odd"><p>Model tokenizer.</p>
</dd>
<dt class="field-even">onnx_dir_path (str)<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to save exported ONNX file.</p>
</dd>
<dt class="field-odd">seq_len (int)<span class="colon">:</span></dt>
<dd class="field-odd"><p>The length of the sequence.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">str<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path of exported <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-QEfficient.utils.device_utils">
<span id="utils"></span><h3><code class="docutils literal notranslate"><span class="pre">utils</span></code><a class="headerlink" href="#module-QEfficient.utils.device_utils" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.utils.device_utils.get_available_device_id">
<span class="sig-prename descclassname"><span class="pre">QEfficient.utils.device_utils.</span></span><span class="sig-name descname"><span class="pre">get_available_device_id</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/device_utils.html#get_available_device_id"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.device_utils.get_available_device_id" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>API to check available device id.</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">int<span class="colon">:</span></dt>
<dd class="field-odd"><p>Available device id.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-QEfficient.utils.generate_inputs"></span><dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.generate_inputs.</span></span><span class="sig-name descname"><span class="pre">InputHandler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler.prepare_ort_inputs">
<span class="sig-name descname"><span class="pre">prepare_ort_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler.prepare_ort_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler.prepare_ort_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for creating Prefill stage numpy inputs for ONNX model to be run on ONNXRT.</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids, position_ids, past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler.prepare_pytorch_inputs">
<span class="sig-name descname"><span class="pre">prepare_pytorch_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler.prepare_pytorch_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler.prepare_pytorch_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for creating Prefill stage tensor inputs for PyTorch model.</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids, position_ids, past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler.update_ort_inputs">
<span class="sig-name descname"><span class="pre">update_ort_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ort_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler.update_ort_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler.update_ort_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for ONNX model to be run on ONNXRT.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>NumPy inputs of Onnx model from previous iteration</p>
</dd>
<dt class="field-even">ort_outputs (Dict)<span class="colon">:</span></dt>
<dd class="field-even"><p>Numpy outputs of Onnx model from previous iteration</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Updated input_ids, position_ids and past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler.update_ort_outputs">
<span class="sig-name descname"><span class="pre">update_ort_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ort_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler.update_ort_outputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler.update_ort_outputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for updating ONNXRT session outputs.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">ort_outputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Numpy outputs of Onnx model from current iteration</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>updated_outputs (Dict): Updated past_key_values, logits</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandler.update_pytorch_inputs">
<span class="sig-name descname"><span class="pre">update_pytorch_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pt_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandler.update_pytorch_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandler.update_pytorch_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for updating Prefill stage inputs to create decode stage inputs for PyTorch model.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pytorch inputs from previous iteration</p>
</dd>
<dt class="field-even">pt_outputs (Dict)<span class="colon">:</span></dt>
<dd class="field-even"><p>Pytorch outputs from previous iteration</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Updated input_ids, position_ids and past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerInternVL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.generate_inputs.</span></span><span class="sig-name descname"><span class="pre">InputHandlerInternVL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerInternVL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerInternVL" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <a class="reference internal" href="#QEfficient.utils.generate_inputs.InputHandlerVLM" title="QEfficient.utils.generate_inputs.InputHandlerVLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">InputHandlerVLM</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerInternVL.prepare_pytorch_inputs">
<span class="sig-name descname"><span class="pre">prepare_pytorch_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerInternVL.prepare_pytorch_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerInternVL.prepare_pytorch_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for creating Prefill stage tensor inputs for PyTorch model.</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids, position_ids, past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerInternVL.prepare_vlm_ort_inputs">
<span class="sig-name descname"><span class="pre">prepare_vlm_ort_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerInternVL.prepare_vlm_ort_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerInternVL.prepare_vlm_ort_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerVLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.generate_inputs.</span></span><span class="sig-name descname"><span class="pre">InputHandlerVLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conversation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerVLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerVLM" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerVLM.prepare_pytorch_inputs">
<span class="sig-name descname"><span class="pre">prepare_pytorch_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerVLM.prepare_pytorch_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerVLM.prepare_pytorch_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for creating Prefill stage tensor inputs for PyTorch model.</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>input_ids, position_ids, past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerVLM.prepare_vlm_ort_inputs">
<span class="sig-name descname"><span class="pre">prepare_vlm_ort_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerVLM.prepare_vlm_ort_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerVLM.prepare_vlm_ort_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerVLM.update_vlm_ort_inputs">
<span class="sig-name descname"><span class="pre">update_vlm_ort_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ort_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerVLM.update_vlm_ort_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerVLM.update_vlm_ort_inputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for ONNX model to be run on ONNXRT.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>NumPy inputs of Onnx model from previous iteration</p>
</dd>
<dt class="field-even">ort_outputs (Dict)<span class="colon">:</span></dt>
<dd class="field-even"><p>Numpy outputs of Onnx model from previous iteration</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Updated input_ids, position_ids, pixel_values and past_key_values</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.generate_inputs.InputHandlerVLM.update_vlm_ort_outputs">
<span class="sig-name descname"><span class="pre">update_vlm_ort_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ort_outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/generate_inputs.html#InputHandlerVLM.update_vlm_ort_outputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.generate_inputs.InputHandlerVLM.update_vlm_ort_outputs" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for updating ONNXRT session outputs.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">ort_outputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Numpy outputs of Onnx model from current iteration</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>updated_outputs (Dict): Updated past_key_values, logits, pixel_values</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-QEfficient.utils.run_utils"></span><dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.run_utils.</span></span><span class="sig-name descname"><span class="pre">ApiRunner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<section id="apirunner-class-is-responsible-for-running">
<h4>ApiRunner class is responsible for running:<a class="headerlink" href="#apirunner-class-is-responsible-for-running" title="Permalink to this heading">ÔÉÅ</a></h4>
<ol class="arabic simple">
<li><p>HuggingFace <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p></li>
<li><p>Transformed KV Pytorch Model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on ONNXRT</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on Cloud AI 100</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_hf_model_on_pytorch">
<span class="sig-name descname"><span class="pre">run_hf_model_on_pytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_hf</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_hf_model_on_pytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_hf_model_on_pytorch" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running HuggingFace <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model and return the output tokens</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_hf (torch.nn.module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Original <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">numpy.ndarray<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated output tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_hf_model_on_pytorch_CB">
<span class="sig-name descname"><span class="pre">run_hf_model_on_pytorch_CB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_hf</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_hf_model_on_pytorch_CB"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_hf_model_on_pytorch_CB" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running HuggingFace <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model and return the output tokens</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_hf (torch.nn.module)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Original <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">numpy.ndarray<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated output tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_cloud_ai_100">
<span class="sig-name descname"><span class="pre">run_kv_model_on_cloud_ai_100</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_kv_model_on_cloud_ai_100"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_cloud_ai_100" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on Cloud AI 100 and return the output tokens</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">qpc_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>path to qpc generated after compilation</p>
</dd>
<dt class="field-even">device_group (List[int])<span class="colon">:</span></dt>
<dd class="field-even"><p>Device Ids to be used for compilation. if len(device_group) &gt; 1. Multiple Card setup is enabled.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">numpy.ndarray<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated output tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_ort">
<span class="sig-name descname"><span class="pre">run_kv_model_on_ort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_tlm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_kv_model_on_ort"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_ort" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on onnxruntime and return the output tokens</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">model_path (str)<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path to the Onnx model.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">numpy.ndarray<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated output tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_pytorch">
<span class="sig-name descname"><span class="pre">run_kv_model_on_pytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_kv_model_on_pytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_kv_model_on_pytorch" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running KV <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model and return the output tokens</p>
<p><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:
:model (torch.nn.module): Transformed <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p>
<dl class="simple">
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">numpy.ndarray<span class="colon">:</span></dt>
<dd class="field-odd"><p>Generated output tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunner.run_ort_session">
<span class="sig-name descname"><span class="pre">run_ort_session</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">session</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunner.run_ort_session"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunner.run_ort_session" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Function responsible for running onnxrt session with given inputs and passing retained state outputs to be used for next iteration inputs</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Mandatory</span></code> Args:</dt><dd><dl class="field-list simple">
<dt class="field-odd">inputs (Dict)<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">session (onnxruntime.capi.onnxruntime_inference_collection.InferenceSession)<span class="colon">:</span></dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
<dt>Return:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Dict<span class="colon">:</span></dt>
<dd class="field-odd"><p>Numpy outputs of Onnx model</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerInternVL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.run_utils.</span></span><span class="sig-name descname"><span class="pre">ApiRunnerInternVL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerInternVL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerInternVL" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <a class="reference internal" href="#QEfficient.utils.run_utils.ApiRunnerVlm" title="QEfficient.utils.run_utils.ApiRunnerVlm"><code class="xref py py-class docutils literal notranslate"><span class="pre">ApiRunnerVlm</span></code></a></p>
<section id="apirunner-for-internvl-vision-models">
<h4>ApiRunner for InternVL Vision models:<a class="headerlink" href="#apirunner-for-internvl-vision-models" title="Permalink to this heading">ÔÉÅ</a></h4>
<ol class="arabic simple">
<li><p>HuggingFace <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p></li>
<li><p>Transformed KV Pytorch Model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on ONNXRT</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on Cloud AI 100</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerInternVL.run_vlm_hf_model_on_pytorch">
<span class="sig-name descname"><span class="pre">run_vlm_hf_model_on_pytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerInternVL.run_vlm_hf_model_on_pytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerInternVL.run_vlm_hf_model_on_pytorch" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.utils.run_utils.</span></span><span class="sig-name descname"><span class="pre">ApiRunnerVlm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conversation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<section id="apirunnervlm-class-is-responsible-for-running-vision-models">
<h4>ApiRunnerVlm class is responsible for running Vision models:<a class="headerlink" href="#apirunnervlm-class-is-responsible-for-running-vision-models" title="Permalink to this heading">ÔÉÅ</a></h4>
<ol class="arabic simple">
<li><p>HuggingFace <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> model</p></li>
<li><p>Transformed KV Pytorch Model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on ONNXRT</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model on Cloud AI 100</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm.run_ort_session">
<span class="sig-name descname"><span class="pre">run_ort_session</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">session</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm.run_ort_session"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm.run_ort_session" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_hf_model_on_pytorch">
<span class="sig-name descname"><span class="pre">run_vlm_hf_model_on_pytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm.run_vlm_hf_model_on_pytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_hf_model_on_pytorch" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_kv_model_on_ort">
<span class="sig-name descname"><span class="pre">run_vlm_kv_model_on_ort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm.run_vlm_kv_model_on_ort"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_kv_model_on_ort" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_kv_model_on_pytorch">
<span class="sig-name descname"><span class="pre">run_vlm_kv_model_on_pytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm.run_vlm_kv_model_on_pytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm.run_vlm_kv_model_on_pytorch" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.utils.run_utils.ApiRunnerVlm.setup_ort_session">
<span class="sig-name descname"><span class="pre">setup_ort_session</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/utils/run_utils.html#ApiRunnerVlm.setup_ort_session"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.utils.run_utils.ApiRunnerVlm.setup_ort_session" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

</section>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cli_api.html" class="btn btn-neutral float-left" title="Command Line Interface Use (CLI)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="finetune.html" class="btn btn-neutral float-right" title="Finetune Infra" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
        <dd><a href="release/v1.19/index.html">release/v1.19</a></dd>
        <dd><a href="release/v1.20/index.html">release/v1.20(beta)</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>