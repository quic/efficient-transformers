<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QEfficient.transformers.models.modeling_auto &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/my_theme.css?v=f6ee2d30" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Release Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/release_docs.html">Efficient Transformer Library - 1.20.0 Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/supported_features.html">Supported Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Cloud AI 100</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/features_enablement.html">Fetaures Enablement Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/qeff_autoclasses.html">QEfficient Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html">CLI API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">QEfficient.transformers.models.modeling_auto</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for QEfficient.transformers.models.modeling_auto</h1><div class="highlight"><pre>
<span></span><span class="c1"># -----------------------------------------------------------------------------</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="c1">#</span>
<span class="c1"># ----------------------------------------------------------------------------</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">time</span><span class="w"> </span><span class="kn">import</span> <span class="n">perf_counter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModel</span><span class="p">,</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoModelForImageTextToText</span><span class="p">,</span>
    <span class="n">AutoModelForSpeechSeq2Seq</span><span class="p">,</span>
    <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
    <span class="n">PreTrainedTokenizerFast</span><span class="p">,</span>
    <span class="n">TextStreamer</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">QEfficient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.base.modeling_qeff</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFBaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.base.onnx_transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.base.pytorch_transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">SplitGateUpWeightsTransform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.generation.cloud_infer</span><span class="w"> </span><span class="kn">import</span> <span class="n">QAICInferenceSession</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.generation.text_generation_inference</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">CloudAI100ExecInfoNew</span><span class="p">,</span>
    <span class="n">PerfMetrics</span><span class="p">,</span>
    <span class="n">calculate_latency</span><span class="p">,</span>
    <span class="n">get_compilation_dims</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.transformers.modeling_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">DYNAMIC_SEQ_LEN_SUPPORTED_MODEL_ARCH</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.transformers.models.pytorch_transforms</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">CustomOpsTransform</span><span class="p">,</span>
    <span class="n">KVCacheExternalModuleMapperTransform</span><span class="p">,</span>
    <span class="n">KVCacheTransform</span><span class="p">,</span>
    <span class="n">PoolingTransform</span><span class="p">,</span>
    <span class="n">SamplerTransform</span><span class="p">,</span>
    <span class="n">SpDTransform</span><span class="p">,</span>
    <span class="n">VlmKVOffloadTransform</span><span class="p">,</span>
    <span class="n">VlmNoKVOffloadTransform</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.transformers.quantizers.auto</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFF_AUTO_QUANTIZATION_CONFIG_MAPPING</span><span class="p">,</span> <span class="n">with_replaced_quantizers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.transformers.quantizers.quant_transforms</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span>
    <span class="n">FP8DeQuantLinearToLinearTransform</span><span class="p">,</span>
    <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">constants</span><span class="p">,</span>
    <span class="n">get_padding_shape_from_config</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils.logging_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>


<span class="k">class</span><span class="w"> </span><span class="nc">QEFFTransformersBase</span><span class="p">(</span><span class="n">QEFFBaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for QEfficient wrappers around HuggingFace transformer models.</span>

<span class="sd">    This class provides common functionality for loading, representing, and managing</span>
<span class="sd">    HuggingFace models within the QEfficient framework. It serves as a parent</span>
<span class="sd">    for specific model types like `AutoModel`, `AutoModelForCausalLM`, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span><span class="p">:</span> <span class="nb">type</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;quantization_config&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_config</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">QEFF_AUTO_QUANTIZATION_CONFIG_MAPPING</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Please use `from_pretrained` method to load quantized models&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient transformer model from a pretrained HuggingFace model or local path.</span>

<span class="sd">        This is the recommended way to initialize any QEfficient transformer model.</span>
<span class="sd">        The interface is similar to ``transformers.AutoModel.from_pretrained``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        *args :</span>
<span class="sd">            Positional arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Keyword arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>

<span class="sd">            **Note:** `attn_implementation` and `low_cpu_mem_usage` are automatically set to &quot;eager&quot; and False respectively to ensure compatibility.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        QEFFTransformersBase</span>
<span class="sd">            An instance of the specific QEFFAutoModel subclass, initialized with the pretrained weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying HuggingFace model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MultimodalUtilityMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mixin for multimodal models providing utilities like input auto-correction.</span>

<span class="sd">    This mixin ensures that inputs to multimodal models conform to the expected</span>
<span class="sd">    names, shapes, and dtypes defined by the model&#39;s `get_inputs_info` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">MultimodalUtilityMixin</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;only children of &#39;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39; may be instantiated&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">auto_correct_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validates and corrects model inputs to match expected specifications.</span>

<span class="sd">        Checks if the provided inputs dictionary contains all required keys and</span>
<span class="sd">        if the data types of the tensors match the model&#39;s specifications.</span>
<span class="sd">        It then filters the input dictionary to only include expected inputs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, torch.Tensor]</span>
<span class="sd">            A dictionary of input tensors, where keys are input names and values are `torch.Tensor` objects.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Dict[str, torch.Tensor]</span>
<span class="sd">            A filtered dictionary of input tensors that match the model&#39;s expected inputs.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            If any expected input is missing or has a mismatched data type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checked</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">inputs_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_inputs_info</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">valid_input_info</span> <span class="ow">in</span> <span class="n">inputs_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">valid_input_info</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
                <span class="n">checked</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="n">inputs</span><span class="p">[</span><span class="n">valid_input_info</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">valid_input_info</span><span class="o">.</span><span class="n">datatype</span><span class="p">:</span>
                <span class="n">checked</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">checked</span><span class="p">:</span>
            <span class="n">err_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Expected following input names and shapes to be passed</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">val</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">inputs_info</span><span class="p">])</span>
                <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">got&quot;</span>
                <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="p">[(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">err_str</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="n">iinfo</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">iinfo</span> <span class="ow">in</span> <span class="n">inputs_info</span><span class="p">]}</span>


<div class="viewcode-block" id="QEFFAutoModel"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">QEFFAutoModel</span><span class="p">(</span><span class="n">QEFFTransformersBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient class for general transformer models from the HuggingFace hub (e.g., BERT, Sentence Transformers).</span>

<span class="sd">    This class provides a unified interface for loading, exporting, compiling, and running</span>
<span class="sd">    various encoder-only transformer models on Cloud AI 100 hardware. It supports pooling</span>
<span class="sd">    for embedding extraction.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        from QEfficient import QEFFAutoModel</span>
<span class="sd">        from transformers import AutoTokenizer</span>

<span class="sd">        model = QEFFAutoModel.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, pooling=&quot;mean&quot;)</span>
<span class="sd">        model.compile(num_cores=16)</span>
<span class="sd">        tokenizer = AutoTokenizer.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)</span>
<span class="sd">        inputs = tokenizer(&quot;My name is&quot;, return_tensors=&quot;pt&quot;)</span>
<span class="sd">        output = model.generate(inputs)</span>
<span class="sd">        print(output) # Output will be a dictionary containing extracted features.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModel</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">CustomOpsTransform</span><span class="p">,</span> <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a QEFFAutoModel instance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The underlying HuggingFace PyTorch model.</span>
<span class="sd">        pooling : str or Callable, optional</span>
<span class="sd">            The pooling method to use for feature extraction.</span>
<span class="sd">            Options include: &quot;mean&quot;, &quot;max&quot;, &quot;cls&quot;, &quot;avg&quot;, or a custom Callable.</span>
<span class="sd">            Default is None (no pooling applied).</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the base class constructor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Make Embedding specific transforms like appending pooling</span>
        <span class="k">if</span> <span class="n">pooling</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">PoolingTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">pooling</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

<div class="viewcode-block" id="QEFFAutoModel.from_pretrained"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient transformer model from a pretrained HuggingFace model or local path.</span>

<span class="sd">        This is the recommended way to initialize a QEfficient transformer model. The interface is similar to</span>
<span class="sd">        ``transformers.AutoModel.from_pretrained``. Once initialized, you can use methods such as ``export``, ``compile``, and ``generate``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        pooling : str or Callable, optional</span>
<span class="sd">            The pooling method to use. Options include:</span>
<span class="sd">            - &quot;mean&quot;: Mean pooling</span>
<span class="sd">            - &quot;max&quot;: Max pooling</span>
<span class="sd">            - &quot;cls&quot;: CLS token pooling</span>
<span class="sd">            - &quot;avg&quot;: Average pooling</span>
<span class="sd">            - Callable: A custom pooling function</span>
<span class="sd">            - None: No pooling applied. Default is None.</span>
<span class="sd">        *args :</span>
<span class="sd">            Positional arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>

<span class="sd">            **Note:** `attn_implementation` and `low_cpu_mem_usage` are automatically</span>
<span class="sd">            set to &quot;eager&quot; and False respectively to ensure compatibility.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        QEFFAutoModel</span>
<span class="sd">            An instance initialized with the pretrained weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># This is support models that should be classified to in a different auto class but transformers load them via this class</span>
        <span class="n">kv_offload</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;kv_offload&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">MISCLASSIFIED_CAUSAL_LM_TO_QEFF_AUTO_CLASS_MAP</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">MISCLASSIFIED_CAUSAL_LM_TO_QEFF_AUTO_CLASS_MAP</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">](</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">kv_offload</span><span class="o">=</span><span class="n">kv_offload</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="n">pooling</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the model configuration as a dictionary.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary of the underlying HuggingFace model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>

<div class="viewcode-block" id="QEFFAutoModel.export"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export the model to ONNX format using ``torch.onnx.export``.</span>

<span class="sd">        This method prepares example inputs and dynamic axes based on the model configuration,</span>
<span class="sd">        then exports the model to an ONNX graph suitable for compilation and deployment on Cloud AI 100 hardware.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved. If not provided,</span>
<span class="sd">            the default export directory is used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span>

        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">}}</span>

        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">output_names</span><span class="p">,</span>
            <span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModel.compile"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">seq_len</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</span>

<span class="sd">        This method generates a ``qpc`` package. If the model has not been exported yet,</span>
<span class="sd">        this method will handle the export process. Additional arguments for the `qaic-exec`</span>
<span class="sd">        compiler can be passed as keyword arguments.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX model. If not provided, the model will be exported first.</span>
<span class="sd">        compile_dir : str, optional</span>
<span class="sd">            Directory to save the generated QPC package. If not provided, a default directory is used.</span>
<span class="sd">        seq_len : int or list of int, optional</span>
<span class="sd">            The length(s) of the prompt(s) to compile for. Can be a single integer or a list of integers</span>
<span class="sd">            to create multiple specializations. Default is 32.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size. Default is 1.</span>
<span class="sd">        num_devices : int, optional</span>
<span class="sd">            Number of devices to compile for. Default is 1.</span>
<span class="sd">        num_cores : int, optional</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        mxfp6_matmul : bool, optional</span>
<span class="sd">            Use MXFP6 compression for weights. Default is False.</span>
<span class="sd">        **compiler_options : dict</span>
<span class="sd">            Additional compiler options for QAIC or QNN compilers. These are passed directly</span>
<span class="sd">            to the underlying compilation command.</span>

<span class="sd">            **For QAIC Compiler:** Extra arguments for qaic-exec can be passed. Some common options include:</span>

<span class="sd">            - mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</span>
<span class="sd">            - aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</span>
<span class="sd">            - allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</span>

<span class="sd">            Params are converted to flags as below:</span>

<span class="sd">            - ``aic_num_cores=16`` -&gt; ``-aic-num-cores=16``</span>
<span class="sd">            - ``convert_to_fp16=True`` -&gt; ``-convert-to-fp16``</span>

<span class="sd">            **For QNN Compiler:** Following arguments can be passed as:</span>

<span class="sd">            - enable_qnn (bool): Enables QNN Compilation.</span>
<span class="sd">            - qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">15</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Recommended: `seq_len` should contain fewer than 15 items.&quot;</span><span class="p">)</span>

        <span class="n">specializations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">sl</span><span class="p">}</span> <span class="k">for</span> <span class="n">sl</span> <span class="ow">in</span> <span class="p">(</span><span class="n">seq_len</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">seq_len</span><span class="p">])</span>
        <span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModel.generate"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate output by executing the compiled QPC on Cloud AI 100 hardware or using PyTorch runtime.</span>

<span class="sd">        This method runs sequential execution based on the compiled model&#39;s batch size and the number of prompts.</span>
<span class="sd">        If the number of prompts is not divisible by the batch size, the last batch will be dropped.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : torch.Tensor or np.ndarray</span>
<span class="sd">            Input data for the model. For AI 100 runtime, this typically includes</span>
<span class="sd">            `input_ids` and `attention_mask`.</span>
<span class="sd">        device_ids : list of int, optional</span>
<span class="sd">            Device IDs for running the QPC. Defaults to `[0]` if not specified and `runtime_ai100` is True.</span>
<span class="sd">        runtime_ai100 : bool, optional</span>
<span class="sd">            Whether to use the AI 100 runtime for inference. If False, the PyTorch</span>
<span class="sd">            runtime will be used. Default is True.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or np.ndarray</span>
<span class="sd">            Output from the AI 100 or PyTorch runtime. The type depends on the runtime and model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># AI_100 runtime</span>
        <span class="k">if</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cloud_ai_100_feature_generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="c1"># PyTorch runtime</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pytorch_feature_generate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">cloud_ai_100_feature_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate features for a batch of inputs using the Cloud AI 100 hardware runtime.</span>

<span class="sd">        This method runs inference on the compiled QPC using the Cloud AI 100 accelerator.</span>
<span class="sd">        It automatically pads input tensors to match the compiled sequence length and handles session setup.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : torch.Tensor or np.ndarray</span>
<span class="sd">            Input tensors for feature extraction. Must be a dictionary-like object</span>
<span class="sd">            including `input_ids` and `attention_mask`.</span>
<span class="sd">        device_ids : List[int], optional</span>
<span class="sd">            List of device IDs to use for inference. Defaults to [0].</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.ndarray</span>
<span class="sd">            Array containing the generated output features for each input in the batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Dynamic switching to closest seq_Len based on input_ids_len</span>
        <span class="n">input_ids_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">allowed_shape</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">:</span>
            <span class="n">seq_len_allowed</span> <span class="o">=</span> <span class="n">allowed_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">seq_len_allowed</span> <span class="o">&gt;=</span> <span class="n">input_ids_len</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len_allowed</span>
                <span class="k">break</span>

        <span class="c1"># To handle single seq_len as we can&#39;t fetch allowed shapes for single seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">input_ids_len</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># TODO: Remove try and catch after compiler fix</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">pytorch_feature_generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate features from a batch of inputs using the PyTorch model.</span>

<span class="sd">        This method runs the model in PyTorch (CPU/GPU) mode for feature extraction.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The PyTorch model to use for inference.</span>
<span class="sd">        inputs : torch.Tensor or np.ndarray</span>
<span class="sd">            Input tensors for feature extraction. Expected to be a dictionary-like object.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of output features generated by the model for each input.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">QEffVisionEncoderForTextImageToTextModel</span><span class="p">(</span><span class="n">QEFFBaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient wrapper for the Vision Encoder component of a Text-to-Image-to-Text model.</span>

<span class="sd">    This class handles the export and compilation of the vision encoder part</span>
<span class="sd">    of multimodal models for optimal performance on Cloud AI 100 hardware.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">CustomOpsTransform</span><span class="p">,</span>
        <span class="n">KVCacheTransform</span><span class="p">,</span>
        <span class="n">KVCacheExternalModuleMapperTransform</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the vision encoder component for multimodal models.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The full HuggingFace multimodal model from which the vision encoder is extracted.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the base class constructor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_qeff_vision_encoder</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offload_pt_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the vision encoder component to ONNX format.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, torch.Tensor]</span>
<span class="sd">            Example inputs for the ONNX export.</span>
<span class="sd">        output_names : List[str]</span>
<span class="sd">            List of output names for the ONNX graph.</span>
<span class="sd">        dynamic_axes : Dict[str, Dict[int, str]]</span>
<span class="sd">            Dynamic axes configuration for the ONNX graph.</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved. Default is None.</span>
<span class="sd">        offload_pt_weights : bool, optional</span>
<span class="sd">            If True, PyTorch weights will be offloaded after export. Default is True.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file for the vision encoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">offload_pt_weights</span><span class="o">=</span><span class="n">offload_pt_weights</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">,</span>
        <span class="n">compile_only</span><span class="p">,</span>
        <span class="n">specializations</span><span class="p">,</span>
        <span class="n">convert_to_fp16</span><span class="p">,</span>
        <span class="n">mxfp6_matmul</span><span class="p">,</span>
        <span class="n">mdp_ts_num_devices</span><span class="p">,</span>
        <span class="n">aic_num_cores</span><span class="p">,</span>
        <span class="n">custom_io</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles the vision encoder component to a QPC package.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        compile_dir : str</span>
<span class="sd">            Directory to save the generated QPC package.</span>
<span class="sd">        compile_only : bool</span>
<span class="sd">            If True, only compilation occurs without running inference.</span>
<span class="sd">        specializations : List[Dict[str, Union[int, str]]]</span>
<span class="sd">            List of dictionaries, each specifying a compilation specialization.</span>
<span class="sd">        convert_to_fp16 : bool</span>
<span class="sd">            If True, converts model to FP16 precision during compilation.</span>
<span class="sd">        mxfp6_matmul : bool</span>
<span class="sd">            If True, uses MXFP6 compression for MatMul weights.</span>
<span class="sd">        mdp_ts_num_devices : int</span>
<span class="sd">            Number of devices for multi-device (tensor slicing) compilation.</span>
<span class="sd">        aic_num_cores : int</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        custom_io : Dict[str, str]</span>
<span class="sd">            Custom I/O configurations for the compiler.</span>
<span class="sd">        **compiler_options :</span>
<span class="sd">            Additional compiler options passed to the underlying compilation command.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package for the vision encoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="n">compile_only</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="n">convert_to_fp16</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">mdp_ts_num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">aic_num_cores</span><span class="p">,</span>
            <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying vision encoder model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the configuration dictionary of the underlying HuggingFace vision model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>


<span class="k">class</span><span class="w"> </span><span class="nc">QEffCausalLMForTextImageToTextModel</span><span class="p">(</span><span class="n">QEFFBaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient wrapper for the Causal Language Model (decoder) component of a Text-to-Image-to-Text model.</span>

<span class="sd">    This class handles the export and compilation of the language decoder part</span>
<span class="sd">    of multimodal models for optimal performance on Cloud AI 100 hardware.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">CustomOpsTransform</span><span class="p">,</span>
        <span class="n">KVCacheTransform</span><span class="p">,</span>
        <span class="n">VlmKVOffloadTransform</span><span class="p">,</span>
        <span class="n">SplitGateUpWeightsTransform</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the language decoder component for multimodal models.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The full HuggingFace multimodal model from which the language decoder is extracted.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the base class constructor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_qeff_language_decoder</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offload_pt_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the language decoder component to ONNX format.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, torch.Tensor]</span>
<span class="sd">            Example inputs for the ONNX export.</span>
<span class="sd">        output_names : List[str]</span>
<span class="sd">            List of output names for the ONNX graph.</span>
<span class="sd">        dynamic_axes : Dict[str, Dict[int, str]]</span>
<span class="sd">            Dynamic axes configuration for the ONNX graph.</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved. Default is None.</span>
<span class="sd">        offload_pt_weights : bool, optional</span>
<span class="sd">            If True, PyTorch weights will be offloaded after export. Default is True.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file for the language decoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">offload_pt_weights</span><span class="o">=</span><span class="n">offload_pt_weights</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">,</span>
        <span class="n">compile_only</span><span class="p">,</span>
        <span class="n">specializations</span><span class="p">,</span>
        <span class="n">convert_to_fp16</span><span class="p">,</span>
        <span class="n">mxfp6_matmul</span><span class="p">,</span>
        <span class="n">mdp_ts_num_devices</span><span class="p">,</span>
        <span class="n">aic_num_cores</span><span class="p">,</span>
        <span class="n">custom_io</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles the language decoder component to a QPC package.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        compile_dir : str</span>
<span class="sd">            Directory to save the generated QPC package.</span>
<span class="sd">        compile_only : bool</span>
<span class="sd">            If True, only compilation occurs without running inference.</span>
<span class="sd">        specializations : List[Dict[str, Union[int, str]]]</span>
<span class="sd">            List of dictionaries, each specifying a compilation specialization.</span>
<span class="sd">        convert_to_fp16 : bool</span>
<span class="sd">            If True, converts model to FP16 precision during compilation.</span>
<span class="sd">        mxfp6_matmul : bool</span>
<span class="sd">            If True, uses MXFP6 compression for MatMul weights.</span>
<span class="sd">        mdp_ts_num_devices : int</span>
<span class="sd">            Number of devices for multi-device (tensor slicing) compilation.</span>
<span class="sd">        aic_num_cores : int</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        custom_io : Dict[str, str]</span>
<span class="sd">            Custom I/O configurations for the compiler.</span>
<span class="sd">        **compiler_options :</span>
<span class="sd">            Additional compiler options passed to the underlying compilation command.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package for the language decoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="n">compile_only</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="n">convert_to_fp16</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">mdp_ts_num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">aic_num_cores</span><span class="p">,</span>
            <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying language decoder model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the configuration dictionary of the underlying HuggingFace language model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_QEffAutoModelForImageTextToTextDualQPC</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal class handling multimodal image-text-to-text models using a dual QPC approach.</span>

<span class="sd">    In this approach, the vision encoder and language model decoder are compiled</span>
<span class="sd">    into separate QPC packages. The vision encoder&#39;s KV cache might be offloaded</span>
<span class="sd">    to CPU or managed differently from the language model&#39;s KV cache.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForImageTextToText</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the dual QPC multimodal model wrapper.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The full HuggingFace multimodal model.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments. `full_batch_size` is not supported here.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `full_batch_size` is provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Continuous batching is not supported for image-text-to-text models yet.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span> <span class="o">=</span> <span class="n">QEffVisionEncoderForTextImageToTextModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span> <span class="o">=</span> <span class="n">QEffCausalLMForTextImageToTextModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_shapes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_names</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying multimodal model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient multimodal model for dual QPC from a pretrained HuggingFace model or local path.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">            Note: `attn_implementation` and `low_cpu_mem_usage` are automatically</span>
<span class="sd">            set to &quot;eager&quot; and False respectively to ensure compatibility.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        _QEffAutoModelForImageTextToTextDualQPC</span>
<span class="sd">            An instance initialized with the pretrained weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">onnx_path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the ONNX paths for the vision and language model components.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            A list containing the ONNX paths of the vision model and the language model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">onnx_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">onnx_path</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">qpc_path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the QPC paths for the vision and language model components.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[List[str], str, None]</span>
<span class="sd">            A list containing both QPC paths if both are compiled, or just one if only one is,</span>
<span class="sd">            or None if neither is compiled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports both the vision encoder and language decoder components to ONNX format.</span>

<span class="sd">        This method exports the vision component (optionally without offloading PyTorch weights)</span>
<span class="sd">        and the language component (with offloading PyTorch weights).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graphs will be saved. Default is None.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            A list containing the paths to the generated ONNX graph files for both components.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_dummy_inputs</span><span class="p">(</span><span class="n">kv_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_onnx_dynamic_axes</span><span class="p">(</span><span class="n">kv_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">(</span><span class="n">kv_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;vision&quot;</span><span class="p">],</span>
            <span class="n">output_names</span><span class="p">[</span><span class="s2">&quot;vision&quot;</span><span class="p">],</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;vision&quot;</span><span class="p">],</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
            <span class="n">offload_pt_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">],</span> <span class="n">output_names</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">],</span> <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">],</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">offload_pt_weights</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">onnx_path</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">img_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vision_onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lang_onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mxint8_kv_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">skip_vision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">skip_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles both the vision encoder and language decoder components into QPC packages.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        img_size : int, optional</span>
<span class="sd">            The image size to compile the vision model for. Default is None.</span>
<span class="sd">        vision_onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX file for the vision encoder. If None, it will be exported.</span>
<span class="sd">        lang_onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX file for the language decoder. If None, it will be exported.</span>
<span class="sd">        compile_dir : str, optional</span>
<span class="sd">            Directory to save the generated QPC packages.</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Length of the prefill prompt for the language model. Default is None.</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum context length for the language model. Default is None.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size. Default is 1.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        num_devices : int, optional</span>
<span class="sd">            Number of devices to compile for. Default is 1.</span>
<span class="sd">        num_cores : int, optional</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        mxfp6_matmul : bool, optional</span>
<span class="sd">            Use MXFP6 compression for weights in the language model. Default is False.</span>
<span class="sd">        mxint8_kv_cache : bool, optional</span>
<span class="sd">            Use MXINT8 compression for KV cache. Default is False.</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        skip_vision : bool, optional</span>
<span class="sd">            If True, skips compilation of the vision encoder. Default is False.</span>
<span class="sd">        skip_lang : bool, optional</span>
<span class="sd">            If True, skips compilation of the language decoder. Default is False.</span>
<span class="sd">        **compiler_options : dict</span>
<span class="sd">            Additional compiler options for QAIC or QNN compilers.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[List[str], str, None]</span>
<span class="sd">            A list of paths to the compiled QPC packages, or a single path if only</span>
<span class="sd">            one component is compiled, or None if neither is compiled.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If `full_batch_size`, `kv_cache_batch_size`, or `num_speculative_tokens` are not None.</span>
<span class="sd">            If both `skip_lang` and `skip_vision` are True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="p">[</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="n">kv_cache_batch_size</span><span class="p">,</span> <span class="n">num_speculative_tokens</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected &#39;full_batch_size&#39;, &#39;kv_cache_batch_size&#39;, &#39;num_speculative_tokens&#39; to be None but got: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;full_batch_size=</span><span class="si">{</span><span class="n">full_batch_size</span><span class="si">}</span><span class="s2">, kv_cache_batch_size=</span><span class="si">{</span><span class="n">kv_cache_batch_size</span><span class="si">}</span><span class="s2">, num_speculative_tokens=</span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">skip_lang</span> <span class="ow">and</span> <span class="n">skip_vision</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected at least one of &#39;skip_lang&#39; or &#39;skip_vision&#39; to be False&quot;</span><span class="p">)</span>

        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">(</span><span class="n">kv_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">specializations</span><span class="p">,</span> <span class="n">compiler_options</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_specializations</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">prefill_seq_len</span><span class="o">=</span><span class="n">prefill_seq_len</span><span class="p">,</span>
            <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
            <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span>
            <span class="n">kv_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">custom_io_vision</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="s2">&quot;mxint8&quot;</span> <span class="k">if</span> <span class="n">mxint8_kv_cache</span> <span class="k">else</span> <span class="s2">&quot;float16&quot;</span>
        <span class="n">custom_io_vision</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>
        <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">[</span><span class="s2">&quot;vision&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">):</span>
                <span class="n">custom_io_vision</span><span class="p">[</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">custom_io_vision</span><span class="p">[</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>

        <span class="k">if</span> <span class="n">vision_onnx_path</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">onnx_path</span> <span class="o">=</span> <span class="n">vision_onnx_path</span>
        <span class="k">if</span> <span class="n">lang_onnx_path</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">onnx_path</span> <span class="o">=</span> <span class="n">lang_onnx_path</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">onnx_path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">vision_onnx_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">onnx_path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">lang_onnx_path</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_vision</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
                <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
                <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">[</span><span class="s2">&quot;vision&quot;</span><span class="p">],</span>
                <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">VISION_MXFP6_MATMUL</span><span class="p">,</span>
                <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
                <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
                <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io_vision</span><span class="p">,</span>
                <span class="n">mxint8_kv_cache</span><span class="o">=</span><span class="n">mxint8_kv_cache</span><span class="p">,</span>
                <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_lang</span><span class="p">:</span>
            <span class="n">custom_io_lang</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># Inputs</span>
            <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                    <span class="n">custom_io_lang</span><span class="p">[</span><span class="n">output_name</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)]]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;float16&quot;</span> <span class="k">if</span> <span class="s2">&quot;vision_embeds&quot;</span> <span class="ow">in</span> <span class="n">output_name</span> <span class="k">else</span> <span class="n">kv_cache_dtype</span>
                    <span class="p">)</span>

            <span class="c1"># outputs</span>
            <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                    <span class="n">custom_io_lang</span><span class="p">[</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span> <span class="k">if</span> <span class="s2">&quot;vision_embeds&quot;</span> <span class="ow">in</span> <span class="n">output_name</span> <span class="k">else</span> <span class="n">kv_cache_dtype</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
                <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
                <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">retained_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">[</span><span class="s2">&quot;lang&quot;</span><span class="p">],</span>
                <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
                <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
                <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
                <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io_lang</span><span class="p">,</span>
                <span class="n">mxint8_kv_cache</span><span class="o">=</span><span class="n">mxint8_kv_cache</span><span class="p">,</span>
                <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates output by executing the compiled QPC(s) on Cloud AI 100 Hardware cards.</span>

<span class="sd">        This method coordinates inference between the vision encoder and language model decoder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, Union[torch.Tensor, np.ndarray]]</span>
<span class="sd">            Inputs to run the execution, typically includes `pixel_values`, `input_ids`,</span>
<span class="sd">            `attention_mask`, etc.</span>
<span class="sd">        streamer : TextStreamer, optional</span>
<span class="sd">            A streamer object to display generated tokens in real-time. Default is None.</span>
<span class="sd">        device_ids : List[int], optional</span>
<span class="sd">            IDs of devices for running the QPC. E.g., `[0]` for a single device or</span>
<span class="sd">            `[0, 1, 2, 3]` for tensor slicing. Defaults to `[0]` if not specified.</span>
<span class="sd">        runtime_ai100 : bool, optional</span>
<span class="sd">            If True, uses the AI 100 runtime. PyTorch runtime is not supported for this model.</span>
<span class="sd">            Default is True.</span>
<span class="sd">        generation_len : int, optional</span>
<span class="sd">            The maximum number of tokens to generate. If None, it&#39;s inferred from `ctx_len`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew or np.ndarray</span>
<span class="sd">            Output from the AI 100 runtime, including generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `runtime_ai100` is False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;PyTorch execution is not supported yet for this model!&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_offload_generate</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kv_offload_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs generation for multimodal models with KV offloading to CPU.</span>

<span class="sd">        This method orchestrates the inference by running the vision encoder (if compiled)</span>
<span class="sd">        and then iteratively running the language decoder, managing KV cache states.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, Union[torch.Tensor, np.ndarray]]</span>
<span class="sd">            Input tensors for the multimodal model.</span>
<span class="sd">        streamer : TextStreamer, optional</span>
<span class="sd">            A streamer object to display generated tokens in real-time. Default is None.</span>
<span class="sd">        device_ids : List[int], optional</span>
<span class="sd">            IDs of devices for running the QPC. Defaults to `[0]` if not specified.</span>
<span class="sd">        generation_len : int, optional</span>
<span class="sd">            The maximum number of tokens to generate. If None, it&#39;s inferred from `ctx_len`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew</span>
<span class="sd">            Execution information including generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If the language model QPC is not compiled.</span>
<span class="sd">        AssertionError</span>
<span class="sd">            If `generation_len` is not greater than zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API for language model first!&quot;</span><span class="p">)</span>

        <span class="n">lang_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">activate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="n">vision_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">fbs</span> <span class="o">=</span> <span class="n">get_compilation_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lang_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">)</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Skip inputs/outputs</span>
        <span class="n">lang_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">input_names</span> <span class="o">+</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">output_names</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Read prompt and ctx len from session</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">prefill_seq_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="n">lang_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">input_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">input_ids_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">input_ids_length</span> <span class="o">//</span> <span class="o">-</span><span class="n">prefill_seq_len</span><span class="p">)</span>  <span class="c1"># ceil divide without float</span>
        <span class="n">padded_len</span> <span class="o">=</span> <span class="n">num_chunks</span> <span class="o">*</span> <span class="n">prefill_seq_len</span>  <span class="c1"># Convert to a multiple of prompt_len</span>

        <span class="k">if</span> <span class="n">generation_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">generation_len</span> <span class="o">=</span> <span class="n">ctx_len</span> <span class="o">-</span> <span class="n">input_len</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">generation_len</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;generation length should be greater than zero&quot;</span>
        <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pad_token_id</span><span class="p">)</span>

        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">),</span>
            <span class="s2">&quot;constant&quot;</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;cross_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">vision_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">,</span> <span class="s2">&quot;aspect_ratio_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;aspect_ratio_mask&quot;</span><span class="p">}</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">vision_inputs</span><span class="p">:</span>
            <span class="n">vision_inputs</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vision_inputs</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">)</span>
        <span class="n">vision_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">vision_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">vision_inputs</span><span class="p">:</span>
            <span class="n">vision_outputs</span> <span class="o">=</span> <span class="n">vision_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">vision_inputs</span><span class="p">)</span>
        <span class="n">vision_end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">lang_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vision_inputs</span><span class="p">}</span>
        <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">lang_inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">padded_len</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># Need to use -1 as position_ids for invalid tokens</span>

        <span class="n">not_mllama</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;model_type&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">!=</span> <span class="s2">&quot;mllama&quot;</span>
        <span class="k">if</span> <span class="n">not_mllama</span><span class="p">:</span>
            <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;image_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="n">vision_session</span><span class="o">.</span><span class="n">deactivate</span><span class="p">()</span>
        <span class="n">lang_session</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>

        <span class="n">lang_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">(</span><span class="n">vision_outputs</span><span class="p">)</span>

        <span class="c1"># Prepare inputs for prefill</span>
        <span class="n">chunk_inputs</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">prefill_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="c1"># Run prefill</span>
        <span class="n">chunk_inputs</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks</span><span class="p">):</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prefill_seq_len</span><span class="p">]</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][</span>
                <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prefill_seq_len</span>
            <span class="p">]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">chunk_inputs</span><span class="p">)</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;image_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;image_idx_output&quot;</span><span class="p">]</span>

        <span class="n">prefill_time</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">prefill_start</span> <span class="o">+</span> <span class="n">vision_end</span> <span class="o">-</span> <span class="n">vision_start</span>
        <span class="c1"># Skip inputs/outputs again</span>
        <span class="n">lang_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">input_names</span> <span class="o">+</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">output_names</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Get first token</span>
        <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_len</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;cross_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">lang_inputs</span><span class="p">:</span>
            <span class="n">bs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">img_tiles</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">img_tiles</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">generated_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Decode loop</span>
        <span class="n">decode_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">num_token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">lang_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">lang_inputs</span><span class="p">)</span>

            <span class="c1"># Prepare inputs for next iteration</span>
            <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">num_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">lang_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">decode_end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="n">decode_perf</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_token</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">decode_end</span> <span class="o">-</span> <span class="n">decode_start</span><span class="p">)</span>
        <span class="n">total_time</span> <span class="o">=</span> <span class="n">decode_end</span> <span class="o">-</span> <span class="n">decode_start</span> <span class="o">+</span> <span class="n">prefill_time</span>
        <span class="n">total_perf</span> <span class="o">=</span> <span class="n">num_token</span> <span class="o">/</span> <span class="n">total_time</span>

        <span class="k">return</span> <span class="n">CloudAI100ExecInfoNew</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">generated_ids</span><span class="o">=</span><span class="n">generated_ids</span><span class="p">,</span>
            <span class="n">perf_metrics</span><span class="o">=</span><span class="n">PerfMetrics</span><span class="p">(</span>
                <span class="n">prefill_time</span><span class="o">=</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="o">=</span><span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="o">=</span><span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="o">=</span><span class="n">total_time</span>
            <span class="p">),</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_QEFFAutoModelForImageTextToTextSingleQPC</span><span class="p">(</span><span class="n">QEFFTransformersBase</span><span class="p">,</span> <span class="n">MultimodalUtilityMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal class handling multimodal image-text-to-text models using a single QPC approach.</span>

<span class="sd">    In this approach, the entire multimodal model (vision encoder + language model decoder)</span>
<span class="sd">    is compiled into a single QPC package.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForImageTextToText</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">CustomOpsTransform</span><span class="p">,</span>
        <span class="n">KVCacheTransform</span><span class="p">,</span>
        <span class="n">KVCacheExternalModuleMapperTransform</span><span class="p">,</span>
        <span class="n">VlmNoKVOffloadTransform</span><span class="p">,</span>
        <span class="n">SplitGateUpWeightsTransform</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the single QPC multimodal model wrapper.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The full HuggingFace multimodal model.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments. `full_batch_size` is not supported here.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `full_batch_size` is provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Continuous batching is not supported for image-text-to-text models yet.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># to handle internvl models</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;llm_config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;vision_config&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">llm_config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">llm_config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vision_config</span><span class="o">.</span><span class="n">use_flash_attn</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient multimodal model for single QPC from a pretrained HuggingFace model or local path.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        *args :</span>
<span class="sd">            Positional arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">            Note: `attn_implementation` and `low_cpu_mem_usage` are automatically</span>
<span class="sd">            set to &quot;eager&quot; and False respectively to ensure compatibility.</span>
<span class="sd">            Also, `_attn_implementation` and `use_flash_attn` are configured for VLM models.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        _QEFFAutoModelForImageTextToTextSingleQPC</span>
<span class="sd">            An instance initialized with the pretrained weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoConfig</span>

        <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>
        <span class="n">config</span><span class="o">.</span><span class="n">vision_config</span><span class="o">.</span><span class="n">use_flash_attn</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the entire multimodal model to ONNX format.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved. Default is None.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_dummy_inputs</span><span class="p">()</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_onnx_dynamic_axes</span><span class="p">()</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">img_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mxint8_kv_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles the exported ONNX model (single QPC) using the Cloud AI 100 Platform SDK compiler.</span>

<span class="sd">        This method generates a single ``qpc`` package for the entire multimodal model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX model. If not provided, the model will be exported first.</span>
<span class="sd">        img_size : int, optional</span>
<span class="sd">            The image size to compile the vision part of the model for. Default is None.</span>
<span class="sd">        compile_dir : str, optional</span>
<span class="sd">            Directory to save the generated QPC package.</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Length of the prefill prompt. Default is None.</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum context length the compiled model can remember. Default is None.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size. Default is 1.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        num_devices : int, optional</span>
<span class="sd">            Number of devices to compile for. Default is 1.</span>
<span class="sd">        num_cores : int, optional</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        mxfp6_matmul : bool, optional</span>
<span class="sd">            Use MXFP6 compression for weights. Default is False.</span>
<span class="sd">        mxint8_kv_cache : bool, optional</span>
<span class="sd">            Use MXINT8 compression for KV cache. Default is False.</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            Not supported for this model; must be None.</span>
<span class="sd">        **compiler_options : dict</span>
<span class="sd">            Additional compiler options for QAIC or QNN compilers.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If `full_batch_size`, `kv_cache_batch_size`, or `num_speculative_tokens` are not None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="p">[</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="n">kv_cache_batch_size</span><span class="p">,</span> <span class="n">num_speculative_tokens</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected &#39;full_batch_size&#39;, &#39;kv_cache_batch_size&#39;, &#39;num_speculative_tokens&#39; to be None but got: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;full_batch_size=</span><span class="si">{</span><span class="n">full_batch_size</span><span class="si">}</span><span class="s2">, kv_cache_batch_size=</span><span class="si">{</span><span class="n">kv_cache_batch_size</span><span class="si">}</span><span class="s2">, num_speculative_tokens=</span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="p">)</span>

        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>

        <span class="c1"># Get specializations from modelling file</span>
        <span class="c1"># TODO: expose this via the auto class as well</span>
        <span class="n">specializations</span><span class="p">,</span> <span class="n">compiler_options</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_specializations</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">prefill_seq_len</span><span class="o">=</span><span class="n">prefill_seq_len</span><span class="p">,</span>
            <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
            <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">custom_io</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="s2">&quot;mxint8&quot;</span> <span class="k">if</span> <span class="n">mxint8_kv_cache</span> <span class="k">else</span> <span class="s2">&quot;float16&quot;</span>
        <span class="c1"># inputs</span>
        <span class="k">for</span> <span class="n">input_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                <span class="n">custom_io</span><span class="p">[</span><span class="n">input_name</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)]]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;float16&quot;</span> <span class="k">if</span> <span class="s2">&quot;pixel_values&quot;</span> <span class="ow">in</span> <span class="n">input_name</span> <span class="k">else</span> <span class="n">kv_cache_dtype</span>
                <span class="p">)</span>

        <span class="c1"># outputs</span>
        <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                <span class="n">custom_io</span><span class="p">[</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span> <span class="k">if</span> <span class="s2">&quot;pixel_values&quot;</span> <span class="ow">in</span> <span class="n">output_name</span> <span class="k">else</span> <span class="n">kv_cache_dtype</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">retained_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="n">mxint8_kv_cache</span><span class="o">=</span><span class="n">mxint8_kv_cache</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_onnx_dynamic_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves the dynamic axes configuration for ONNX export for this model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Dict[str, Dict[int, str]]</span>
<span class="sd">            A dictionary specifying the dynamic axes for inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_onnx_dynamic_axes</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates output by executing the compiled single QPC on Cloud AI 100 Hardware cards.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, Union[torch.Tensor, np.ndarray]]</span>
<span class="sd">            Inputs to run the execution, typically includes `pixel_values`, `input_ids`,</span>
<span class="sd">            `attention_mask`, etc.</span>
<span class="sd">        streamer : TextStreamer, optional</span>
<span class="sd">            A streamer object to display generated tokens in real-time. Default is None.</span>
<span class="sd">        device_ids : List[int], optional</span>
<span class="sd">            IDs of devices for running the QPC. E.g., `[0]` for a single device or</span>
<span class="sd">            `[0, 1, 2, 3]` for tensor slicing. Defaults to `[0]` if not specified.</span>
<span class="sd">        runtime_ai100 : bool, optional</span>
<span class="sd">            If True, uses the AI 100 runtime. PyTorch runtime is not supported for this model.</span>
<span class="sd">            Default is True.</span>
<span class="sd">        generation_len : int, optional</span>
<span class="sd">            The maximum number of tokens to generate. If None, it&#39;s inferred from `ctx_len`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew or np.ndarray</span>
<span class="sd">            Output from the AI 100 runtime, including generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `runtime_ai100` is False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;PyTorch execution is not supported yet for this model!&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cloud_ai_100_generate</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cloud_ai_100_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">enable_debug_logs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs generation for multimodal models using a single QPC on Cloud AI 100 hardware.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, Union[torch.Tensor, np.ndarray]]</span>
<span class="sd">            Input tensors for the multimodal model.</span>
<span class="sd">        device_ids : List[int]</span>
<span class="sd">            IDs of devices for running the QPC.</span>
<span class="sd">        enable_debug_logs : bool, optional</span>
<span class="sd">            If True, enables debug logging for the QAIC inference session. Default is False.</span>
<span class="sd">        generation_len : int, optional</span>
<span class="sd">            The maximum number of tokens to generate. If None, it&#39;s inferred from `ctx_len`.</span>
<span class="sd">        streamer : TextStreamer, optional</span>
<span class="sd">            A streamer object to display generated tokens in real-time. Default is None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew</span>
<span class="sd">            Execution information including generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AssertionError</span>
<span class="sd">            If `generation_len` is not greater than zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_correct_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">qpc_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">enable_debug_logs</span><span class="o">=</span><span class="n">enable_debug_logs</span><span class="p">,</span> <span class="n">activate</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">fbs</span> <span class="o">=</span> <span class="n">get_compilation_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">)</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># Skip inputs/outputs</span>
        <span class="n">qpc_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">input_names</span> <span class="o">+</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">output_names</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Read prompt and ctx len from session</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">prefill_seq_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
        <span class="p">)</span>

        <span class="n">input_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">input_ids_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">num_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">input_ids_length</span> <span class="o">//</span> <span class="o">-</span><span class="n">prefill_seq_len</span><span class="p">)</span>  <span class="c1"># ceil divide without float</span>

        <span class="n">padded_len</span> <span class="o">=</span> <span class="n">num_chunks</span> <span class="o">*</span> <span class="n">prefill_seq_len</span>  <span class="c1"># Convert to a multiple of prompt_len</span>
        <span class="k">if</span> <span class="n">generation_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">generation_len</span> <span class="o">=</span> <span class="n">ctx_len</span> <span class="o">-</span> <span class="n">input_len</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="k">assert</span> <span class="n">generation_len</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;generation length should be greater than zero&quot;</span>
        <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pad_token_id</span><span class="p">)</span>

        <span class="c1"># Prepare inputs for prefill</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">),</span>
            <span class="s2">&quot;constant&quot;</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;cross_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padded_len</span> <span class="o">-</span> <span class="n">input_ids_length</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;pixel_values_RetainedState&quot;</span> <span class="ow">in</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">output_names</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">)</span>

        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">padded_len</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;image_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span>

        <span class="n">qpc_session</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>
        <span class="n">chunk_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">prefill_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="c1"># Run prefill</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks</span><span class="p">):</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prefill_seq_len</span><span class="p">]</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prefill_seq_len</span><span class="p">]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">chunk_inputs</span><span class="p">)</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;image_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;image_idx_output&quot;</span><span class="p">]</span>

        <span class="n">prefill_time</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">prefill_start</span>
        <span class="c1"># Get first token</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_len</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="k">if</span> <span class="s2">&quot;cross_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">bs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">img_tiles</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cross_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">img_tiles</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">generated_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="s2">&quot;pixel_values_RetainedState&quot;</span> <span class="ow">in</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">output_names</span><span class="p">:</span>
            <span class="n">qpc_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">([</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">])</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">)</span>

        <span class="c1"># Decode loop</span>
        <span class="n">decode_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">num_token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="c1"># Prepare inputs for next iteration</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">num_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">decode_end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="n">decode_perf</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_token</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">decode_end</span> <span class="o">-</span> <span class="n">decode_start</span><span class="p">)</span>
        <span class="n">total_time</span> <span class="o">=</span> <span class="n">decode_end</span> <span class="o">-</span> <span class="n">prefill_start</span>
        <span class="n">total_perf</span> <span class="o">=</span> <span class="n">num_token</span> <span class="o">/</span> <span class="n">total_time</span>

        <span class="k">return</span> <span class="n">CloudAI100ExecInfoNew</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">generated_ids</span><span class="o">=</span><span class="n">generated_ids</span><span class="p">,</span>
            <span class="n">perf_metrics</span><span class="o">=</span><span class="n">PerfMetrics</span><span class="p">(</span>
                <span class="n">prefill_time</span><span class="o">=</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="o">=</span><span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="o">=</span><span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="o">=</span><span class="n">total_time</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying multimodal model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the configuration dictionary of the underlying HuggingFace model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>


<div class="viewcode-block" id="QEFFAutoModelForImageTextToText"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">QEFFAutoModelForImageTextToText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient class for multimodal (image-text-to-text) models from the HuggingFace hub.</span>

<span class="sd">    This class supports both single and dual QPC (Quantized Package Compilation) approaches for efficient deployment on Cloud AI 100 hardware.</span>
<span class="sd">    It is recommended to use the ``from_pretrained`` method for initialization.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        import requests</span>
<span class="sd">        from PIL import Image</span>
<span class="sd">        from transformers import AutoProcessor, TextStreamer</span>
<span class="sd">        from QEfficient import QEFFAutoModelForImageTextToText</span>

<span class="sd">        HF_TOKEN = &quot;&quot; # Your HuggingFace token if needed</span>
<span class="sd">        model_name = &quot;meta-llama/Llama-3.2-11B-Vision-Instruct&quot;</span>
<span class="sd">        query = &quot;Describe this image.&quot;</span>
<span class="sd">        image_url = &quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg&quot;</span>

<span class="sd">        # STEP 1: Load processor and model</span>
<span class="sd">        processor = AutoProcessor.from_pretrained(model_name, token=HF_TOKEN)</span>
<span class="sd">        model = QEFFAutoModelForImageTextToText.from_pretrained(</span>
<span class="sd">            model_name, token=HF_TOKEN, attn_implementation=&quot;eager&quot;, kv_offload=False # kv_offload=False for single QPC</span>
<span class="sd">        )</span>

<span class="sd">        # STEP 2: Export &amp; Compile</span>
<span class="sd">        model.compile(</span>
<span class="sd">            prefill_seq_len=32,</span>
<span class="sd">            ctx_len=512,</span>
<span class="sd">            img_size=560,</span>
<span class="sd">            num_cores=16,</span>
<span class="sd">            num_devices=1,</span>
<span class="sd">            mxfp6_matmul=False,</span>
<span class="sd">        )</span>

<span class="sd">        # STEP 3: Prepare inputs</span>
<span class="sd">        image = Image.open(requests.get(image_url, stream=True).raw)</span>
<span class="sd">        messages = [</span>
<span class="sd">            {</span>
<span class="sd">                &quot;role&quot;: &quot;user&quot;,</span>
<span class="sd">                &quot;content&quot;: [</span>
<span class="sd">                    {&quot;type&quot;: &quot;image&quot;},</span>
<span class="sd">                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: query},</span>
<span class="sd">                ],</span>
<span class="sd">            }</span>
<span class="sd">        ]</span>
<span class="sd">        input_text = [processor.apply_chat_template(messages, add_generation_prompt=True)]</span>
<span class="sd">        inputs = processor(</span>
<span class="sd">            text=input_text,</span>
<span class="sd">            images=image,</span>
<span class="sd">            return_tensors=&quot;pt&quot;,</span>
<span class="sd">            add_special_tokens=False,</span>
<span class="sd">            padding=&quot;max_length&quot;, # Consider padding strategy if max_length is crucial</span>
<span class="sd">            max_length=32,</span>
<span class="sd">        )</span>

<span class="sd">        # STEP 4: Run inference</span>
<span class="sd">        streamer = TextStreamer(processor.tokenizer)</span>
<span class="sd">        model.generate(inputs=inputs, streamer=streamer, generation_len=512)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForImageTextToText</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">kv_offload</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate the appropriate internal class for single or dual QPC mode.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The loaded HuggingFace multimodal model.</span>
<span class="sd">        kv_offload : bool, optional</span>
<span class="sd">            If True, uses the dual QPC approach (vision encoder KV offloaded).</span>
<span class="sd">            If False, uses the single QPC approach (entire model in one QPC).</span>
<span class="sd">            Default is True.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the constructor of the selected internal class.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[_QEffAutoModelForImageTextToTextDualQPC, _QEFFAutoModelForImageTextToTextSingleQPC]</span>
<span class="sd">            The wrapped model instance, configured for either dual or single QPC.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kv_offload</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_QEffAutoModelForImageTextToTextDualQPC</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_QEFFAutoModelForImageTextToTextSingleQPC</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="QEFFAutoModelForImageTextToText.from_pretrained"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">kv_offload</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient image-text-to-text model from a pretrained HuggingFace model or local path.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        kv_offload : bool, optional</span>
<span class="sd">            If True, uses the dual QPC approach (vision encoder KV offloaded).</span>
<span class="sd">            If False, uses the single QPC approach (entire model in one QPC).</span>
<span class="sd">            If None, the default behavior of the internal classes is used (typically dual QPC).</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional arguments passed to HuggingFace&#39;s ``from_pretrained``.</span>

<span class="sd">            **Note:** `attn_implementation` and `low_cpu_mem_usage` are automatically set to &quot;eager&quot; and False respectively to ensure compatibility.</span>
<span class="sd">            `continuous_batching` is not supported for image-text-to-text models.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        QEFFAutoModelForImageTextToText</span>
<span class="sd">            An instance initialized with the pretrained weights, wrapped for QEfficient.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `continuous_batching` is provided as True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: add a check to see if kv_offload is allowed for given model by loading the config and checking architecture or type of config here.</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;continuous_batching&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Continuous batching is not supported for image-text-to-text models yet.&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kv_offload</span><span class="o">=</span><span class="n">kv_offload</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div></div>


<span class="n">MISCLASSIFIED_CAUSAL_LM_TO_QEFF_AUTO_CLASS_MAP</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;InternVLChatModel&quot;</span><span class="p">:</span> <span class="n">QEFFAutoModelForImageTextToText</span><span class="p">}</span>


<div class="viewcode-block" id="QEFFAutoModelForCausalLM"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">QEFFAutoModelForCausalLM</span><span class="p">(</span><span class="n">QEFFBaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient class for Causal Language Models from the HuggingFace hub (e.g., GPT-2, Llama).</span>

<span class="sd">    This class provides a unified interface for loading, exporting, compiling, and generating</span>
<span class="sd">    text with causal language models on Cloud AI 100 hardware. It supports features like</span>
<span class="sd">    continuous batching, speculative decoding (TLM), and on-device sampling.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        from QEfficient import QEFFAutoModelForCausalLM</span>
<span class="sd">        from transformers import AutoTokenizer</span>

<span class="sd">        model = QEFFAutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">        model.compile(num_cores=16)</span>
<span class="sd">        tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">        model.generate(prompts=[&quot;Hi there!!&quot;], tokenizer=tokenizer)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span>
        <span class="n">FP8DeQuantLinearToLinearTransform</span><span class="p">,</span>
        <span class="n">CustomOpsTransform</span><span class="p">,</span>
        <span class="n">KVCacheTransform</span><span class="p">,</span>
        <span class="n">SplitGateUpWeightsTransform</span><span class="p">,</span>
        <span class="n">KVCacheExternalModuleMapperTransform</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">continuous_batching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">qaic_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a QEFFAutoModelForCausalLM instance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            The underlying HuggingFace PyTorch Causal Language Model.</span>
<span class="sd">        continuous_batching : bool, optional</span>
<span class="sd">            If True, enables continuous batching mode for future compilation and execution.</span>
<span class="sd">            This setting must be consistent across `from_pretrained` and `compile` calls. Default is False.</span>
<span class="sd">        qaic_config : dict, optional</span>
<span class="sd">            A dictionary for QAIC-specific configurations. Supported keys include:</span>
<span class="sd">            - **speculative_model_type** (str): Specifies the type of Speculative Decoding model (e.g., &quot;target&quot;).</span>
<span class="sd">            - **include_sampler** (bool): If True, enables on-device sampling of next tokens.</span>
<span class="sd">            - **return_pdfs** (bool): If True, returns probability distributions along with sampled tokens.</span>
<span class="sd">              For Speculative Decoding Target Language Models, this is always True.</span>
<span class="sd">            - **max_top_k_ids** (int): Maximum number of top K tokens (&lt;= vocab size) to consider during sampling.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the base class constructor.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If the provided `model` is not a CausalLM or LMHeadModel type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_class_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">model_class_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;ForCausalLM&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">model_class_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;LMHeadModel&quot;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required pytorch module for CausalLM or LMHeadModel, got </span><span class="si">{</span><span class="n">model_class_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># TODO: remove from version 1.20</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">continuous_batching</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;full_batch_size argument is deprecated. Use continuous_batching=True instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="mi">2</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;quantization_config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_config</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">QEFF_AUTO_QUANTIZATION_CONFIG_MAPPING</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Please use `from_pretrained` method to load quantized models, might give unexpected results&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Set use_cache=True to get KV values as output during ONNX export</span>
        <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qaic_config</span><span class="o">=</span><span class="n">qaic_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="o">=</span> <span class="n">continuous_batching</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span> <span class="o">=</span> <span class="n">qaic_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">SpDTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">qaic_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="o">=</span> <span class="n">transformed</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="c1"># ---Sampling---</span>
        <span class="c1"># Note: SamplerTransform should be applied after all other transforms</span>
        <span class="c1"># are done. The role of the sampler is to just add nodes at the output of the</span>
        <span class="c1"># previous transform function.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">SamplerTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">qaic_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># TODO : Update in qaic_config isn&#39;t updated in the hash due to SpDTransforms. Need to move</span>
        <span class="c1"># SpDTransforms to PytorchTransforms.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="p">[</span><span class="s2">&quot;return_pdfs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of the underlying Causal Language Model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The model&#39;s class name, with &quot;QEff&quot; or &quot;QEFF&quot; prefix removed if present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.from_pretrained"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
        <span class="n">continuous_batching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">qaic_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a QEfficient Causal Language Model from a pretrained HuggingFace model or local path.</span>

<span class="sd">        This is the recommended way to initialize a QEfficient Causal Language Model.</span>
<span class="sd">        The interface is similar to ``transformers.AutoModelForCausalLM.from_pretrained``.</span>
<span class="sd">        Once initialized, you can use methods such as ``export``, ``compile``, and ``generate``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pretrained_model_name_or_path : str</span>
<span class="sd">            Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">        continuous_batching : bool, optional</span>
<span class="sd">            Whether this model will be used for continuous batching in the future.</span>
<span class="sd">            If not set to True here, the model cannot be exported/compiled for</span>
<span class="sd">            continuous batching later. Default is False.</span>
<span class="sd">        qaic_config : dict, optional</span>
<span class="sd">            QAIC config dictionary. Supported keys include:</span>

<span class="sd">            - **speculative_model_type** (str): Specify Speculative Decoding Target Language Models.</span>
<span class="sd">            - **include_sampler** (bool): Enable/Disable sampling of next tokens.</span>
<span class="sd">            - **return_pdfs** (bool): Return probability distributions along with sampled next tokens.</span>
<span class="sd">              For Speculative Decoding Target Language Model, ``return_pdfs=True`` always.</span>
<span class="sd">              Otherwise, ``return_pdfs=True`` for Speculative Decoding Draft Language Model</span>
<span class="sd">              and ``return_pdfs=False`` for regular model.</span>
<span class="sd">            - **max_top_k_ids** (int): Maximum number of top K tokens (&lt;= vocab size) to consider during sampling.</span>
<span class="sd">              The values provided in ``top_ks`` tensor must be less than this maximum limit.</span>

<span class="sd">        *args :</span>
<span class="sd">            Positional arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed directly to `cls._hf_auto_class.from_pretrained`.</span>

<span class="sd">            **Note:** `attn_implementation` and `low_cpu_mem_usage` are automatically</span>
<span class="sd">            set to &quot;eager&quot; and False respectively to ensure compatibility.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        QEFFAutoModelForCausalLM</span>
<span class="sd">            An instance initialized with the pretrained weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">continuous_batching</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;full_batch_size argument is deprecated. Use continuous_batching=True instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="mi">2</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kv_offload</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;kv_offload&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">qaic_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">qaic_config</span><span class="p">[</span><span class="s2">&quot;pretrained_model_name_or_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

        <span class="c1"># This is support models that should be classified to in a different auto class but transformers load them via this class</span>

        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">MISCLASSIFIED_CAUSAL_LM_TO_QEFF_AUTO_CLASS_MAP</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">MISCLASSIFIED_CAUSAL_LM_TO_QEFF_AUTO_CLASS_MAP</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">](</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">kv_offload</span><span class="o">=</span><span class="n">kv_offload</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">continuous_batching</span><span class="o">=</span><span class="n">continuous_batching</span><span class="p">,</span>
            <span class="n">qaic_config</span><span class="o">=</span><span class="n">qaic_config</span><span class="p">,</span>
            <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the model configuration as a dictionary.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary of the underlying HuggingFace model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.export"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export the model to ONNX format using ``torch.onnx.export``.</span>

<span class="sd">        This method prepares example inputs and dynamic axes based on the model configuration,</span>
<span class="sd">        then exports the model to an ONNX graph suitable for compilation and deployment</span>
<span class="sd">        on Cloud AI 100 hardware. It handles KV cache inputs/outputs and sampler-related inputs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved.</span>
<span class="sd">            If not provided, the default export directory is used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span>
        <span class="n">fbs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_FBS</span>
        <span class="n">kv_cache_shape</span> <span class="o">=</span> <span class="n">get_padding_shape_from_config</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span>
        <span class="p">)</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)],</span>
        <span class="p">}</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_cache_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># For GPTBigCode arch the pkv is 3d</span>
            <span class="n">pkv_dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;ctx_len&quot;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># pkv is 4d</span>
            <span class="n">pkv_dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
                <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;ctx_len&quot;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;include_sampler&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_pdfs&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;probs&quot;</span><span class="p">)</span>
            <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;next_tokens&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">)</span>

        <span class="c1"># TODO Update the get_padding_shape_from_config method to handle the case when the model config has attention_chunk_size or sliding_window and it should return a list of shapes for each layer</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;model_type&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="n">DYNAMIC_SEQ_LEN_SUPPORTED_MODEL_ARCH</span>
        <span class="p">):</span>
            <span class="n">pkv_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_dummy_pkv_cache</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                    <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pkv_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
                    <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pkv_dynamic_axes</span>
                    <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_RetainedState&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                    <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">kv_cache_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
                    <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pkv_dynamic_axes</span>
                    <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_RetainedState&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">nlk</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_NLK</span>  <span class="c1"># Number of Logits to Keep</span>
            <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nlk</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nlk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;include_sampler&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">example_inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sampling_inputs_and_outputs</span><span class="p">(</span>
                <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">,</span>
                <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                <span class="n">dynamic_axes</span><span class="o">=</span><span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">output_names</span><span class="p">,</span>
            <span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_sampling_inputs_and_outputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">output_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">dynamic_axes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the example inputs, output names, and dynamic axes to include</span>
<span class="sd">        parameters relevant for on-device sampling during ONNX export.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        example_inputs : Dict[str, torch.Tensor]</span>
<span class="sd">            Current dictionary of example inputs.</span>
<span class="sd">        output_names : List[str]</span>
<span class="sd">            Current list of output names.</span>
<span class="sd">        dynamic_axes : Dict[str, Dict[int, str]]</span>
<span class="sd">            Current dictionary of dynamic axes configurations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tuple[Dict[str, torch.Tensor], List[str], Dict[str, Dict[int, str]]]</span>
<span class="sd">            Updated example inputs, output names, and dynamic axes including</span>
<span class="sd">            sampling-related parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">fbs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_FBS</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;last_accepted_output_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;last_accepted_output_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_repetition_penalty_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;past_repetition_penalty_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;past_repetition_penalty_buffer_RetainedState&quot;</span><span class="p">)</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;repetition_penalties&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_REPETITION_PENALTIES</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;repetition_penalties&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_presence_penalty_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;past_presence_penalty_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;past_presence_penalty_buffer_RetainedState&quot;</span><span class="p">)</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;presence_penalties&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">+</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_PRESENCE_PENALTIES</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;presence_penalties&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;temperatures&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_TEMPERATURES</span>
        <span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;temperatures&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">max_top_k_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_top_k_ids&quot;</span><span class="p">,</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_MAX_TOP_K_IDS</span><span class="p">)</span>
        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;top_ks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_top_k_ids</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;top_ks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;top_ps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_TOP_PS</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;top_ps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;min_ps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_MIN_PS</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;min_ps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;random_numbers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;random_numbers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="k">return</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build_prefill_specialization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Builds a dictionary representing a compilation specialization for the prefill phase.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Length of the prefill prompt. Default is 32.</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum context length the compiled model can remember. Default is 128.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size for the prefill. Default is 1.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Batch size for KV cache. If not provided, it defaults based on `full_batch_size` or `batch_size`.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Continuous batching batch size. Used if `continuous_batching` is enabled. Default is None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Dict[str, Union[int, str]]</span>
<span class="sd">            A dictionary defining the prefill specialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">prefill_seq_len</span><span class="p">,</span>
            <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="n">ctx_len</span><span class="p">,</span>
            <span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">spec</span><span class="p">[</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">spec</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_batch_size</span>
        <span class="k">if</span> <span class="n">full_batch_size</span><span class="p">:</span>
            <span class="n">spec</span><span class="p">[</span><span class="s2">&quot;full_batch_exec_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_batch_size</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build_decode_specialization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Builds a dictionary representing a compilation specialization for the decode phase.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Length of the prefill prompt. Used to avoid duplicate specializations. Default is 32.</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum context length the compiled model can remember. Default is 128.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size for the decode phase. Default is 1.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Batch size for KV cache. If not provided, it defaults based on `full_batch_size` or `batch_size`.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Continuous batching batch size. Used if `continuous_batching` is enabled. Default is None.</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            Number of speculative tokens for Speculative Decoding Target Language Model. Default is None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Optional[Dict[str, Union[int, str]]]</span>
<span class="sd">            A dictionary defining the decode specialization, or None if it would be a duplicate</span>
<span class="sd">            of the prefill specialization (e.g., if prefill_seq_len is 1 and not continuous batching).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prefill_seq_len</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Avoid duplication with prefill</span>
        <span class="n">spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">full_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="n">ctx_len</span><span class="p">,</span>
            <span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">spec</span><span class="p">[</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">spec</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_batch_size</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.compile"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mxint8_kv_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefill_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</span>

<span class="sd">        This method generates a ``qpc`` package. If the model has not been exported yet,</span>
<span class="sd">        this method will handle the export process. Additional arguments for the `qaic-exec`</span>
<span class="sd">        compiler can be passed as keyword arguments.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX model. If not provided, the model will be exported first.</span>
<span class="sd">        compile_dir : str, optional</span>
<span class="sd">            Directory to save the generated QPC package. If not provided, a default directory is used.</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Length of the prefill prompt. Default is 32.</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum context length the compiled model can remember. Default is 128.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size. Default is 1.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Continuous batching batch size. Required if `continuous_batching=True` was</span>
<span class="sd">            set during `from_pretrained`.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Batch size for KV cache. If not provided, it defaults to `full_batch_size` (if</span>
<span class="sd">            continuous batching) or `batch_size`.</span>
<span class="sd">        num_devices : int, optional</span>
<span class="sd">            Number of devices to compile for. Default is 1.</span>
<span class="sd">        num_cores : int, optional</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        mxfp6_matmul : bool, optional</span>
<span class="sd">            Use MXFP6 compression for weights. Default is False.</span>
<span class="sd">        mxint8_kv_cache : bool, optional</span>
<span class="sd">            Use MXINT8 compression for KV cache. Default is False.</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            Number of speculative tokens for Speculative Decoding Target Language Model.</span>
<span class="sd">            Required if the model is configured as a Target Language Model (`is_tlm=True`).</span>
<span class="sd">        prefill_only : bool, optional</span>
<span class="sd">            If True, compiles only for the prefill stage. If False, compiles only for</span>
<span class="sd">            the decode stage. If None, compiles for both stages. Default is None.</span>
<span class="sd">        **compiler_options : dict</span>
<span class="sd">            Additional compiler options for QAIC or QNN compilers.</span>

<span class="sd">            **For QAIC Compiler:** Extra arguments for qaic-exec can be passed. Some common options include:</span>

<span class="sd">            - mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</span>
<span class="sd">            - aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</span>
<span class="sd">            - allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</span>

<span class="sd">            Params are converted to flags as below:</span>

<span class="sd">            - ``aic_num_cores=16`` -&gt; ``-aic-num-cores=16``</span>
<span class="sd">            - ``convert_to_fp16=True`` -&gt; ``-convert-to-fp16``</span>

<span class="sd">            **For QNN Compiler:** Following arguments can be passed as:</span>

<span class="sd">            - enable_qnn (bool): Enables QNN Compilation.</span>
<span class="sd">            - qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If `prefill_only` is not a boolean.</span>
<span class="sd">            If `full_batch_size` is None when `continuous_batching` is True.</span>
<span class="sd">            If `num_speculative_tokens` is None when the model is a TLM.</span>
<span class="sd">        ValueError</span>
<span class="sd">            If KV caching is requested without continuous batching (`full_batch_size`).</span>
<span class="sd">            If `include_sampler` is True and `num_speculative_tokens` is greater than 0.</span>
<span class="sd">            If `num_speculative_tokens` is not an integer greater than 1.</span>
<span class="sd">            If `prefill_seq_len` is less than `num_speculative_tokens + 1` for TLM models.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># --- Validation ---</span>
        <span class="k">if</span> <span class="n">prefill_only</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prefill_only</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;`prefill_only` must be a boolean.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_and_get_num_speculative_tokens</span><span class="p">(</span><span class="n">num_speculative_tokens</span><span class="p">,</span> <span class="n">prefill_seq_len</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="ow">and</span> <span class="n">full_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;`full_batch_size` is required when `continuous_batching=True`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kv_cache_batch_size</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">full_batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;KV caching requires continuous batching. Please set `full_batch_size` and &quot;</span>
                <span class="s2">&quot;enable `continuous_batching=True` in `from_pretrained`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">qaic_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;include_sampler&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">num_speculative_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">num_speculative_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Currently, sampler does not support `num_speculative_tokens` &gt; 0.&quot;</span><span class="p">)</span>

        <span class="c1"># Infer kv_cache_batch_size if not provided</span>
        <span class="n">kv_cache_batch_size</span> <span class="o">=</span> <span class="n">kv_cache_batch_size</span> <span class="ow">or</span> <span class="n">full_batch_size</span> <span class="ow">or</span> <span class="n">batch_size</span>

        <span class="c1"># --- Specializations ---</span>
        <span class="n">specializations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">prefill_only</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">prefill_only</span> <span class="ow">or</span> <span class="n">prefill_seq_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">specializations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build_prefill_specialization</span><span class="p">(</span>
                    <span class="n">prefill_seq_len</span><span class="o">=</span><span class="n">prefill_seq_len</span><span class="p">,</span>
                    <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">kv_cache_batch_size</span><span class="o">=</span><span class="n">kv_cache_batch_size</span><span class="p">,</span>
                    <span class="n">full_batch_size</span><span class="o">=</span><span class="n">full_batch_size</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">prefill_only</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">prefill_only</span><span class="p">:</span>
            <span class="n">decode_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_decode_specialization</span><span class="p">(</span>
                <span class="n">prefill_seq_len</span><span class="o">=</span><span class="n">prefill_seq_len</span><span class="p">,</span>
                <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">kv_cache_batch_size</span><span class="o">=</span><span class="n">kv_cache_batch_size</span><span class="p">,</span>
                <span class="n">full_batch_size</span><span class="o">=</span><span class="n">full_batch_size</span><span class="p">,</span>
                <span class="n">num_speculative_tokens</span><span class="o">=</span><span class="n">num_speculative_tokens</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">decode_spec</span><span class="p">:</span>
                <span class="n">specializations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decode_spec</span><span class="p">)</span>

        <span class="c1"># --- Compilation ---</span>
        <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="s2">&quot;mxint8&quot;</span> <span class="k">if</span> <span class="n">mxint8_kv_cache</span> <span class="k">else</span> <span class="s2">&quot;float16&quot;</span>
        <span class="n">custom_io</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;_RetainedState&quot;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                    <span class="n">custom_io</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}{</span><span class="n">suffix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>

        <span class="n">qpc_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">retained_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">num_speculative_tokens</span><span class="o">=</span><span class="n">num_speculative_tokens</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="n">mxint8_kv_cache</span><span class="o">=</span><span class="n">mxint8_kv_cache</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">qpc_path</span></div>

    <span class="c1"># FIXME: Update this method to match with transformers AutoModelForCausalLM.generate</span>
<div class="viewcode-block" id="QEFFAutoModelForCausalLM.generate"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizerFast</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span><span class="p">],</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate output by executing the compiled QPC on Cloud AI 100 hardware.</span>

<span class="sd">        This method runs sequential execution based on the compiled model&#39;s batch size and the number of prompts.</span>
<span class="sd">        If the number of prompts is not divisible by the batch size, the last batch will be dropped.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tokenizer : PreTrainedTokenizer or PreTrainedTokenizerFast</span>
<span class="sd">            Tokenizer for the model.</span>
<span class="sd">        prompts : list of str</span>
<span class="sd">            List of prompts to generate output for.</span>
<span class="sd">        device_id : list of int, optional</span>
<span class="sd">            Device IDs for running the QPC. Defaults to `[0]` if not specified.</span>
<span class="sd">        runtime_ai100 : bool, optional</span>
<span class="sd">            Whether to use AI 100 runtime. Default is True.</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments. Currently supports:</span>
<span class="sd">            - `generation_len (int, optional)`: The maximum number of tokens to generate.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew</span>
<span class="sd">            Output from the AI 100 runtime, containing generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If the QPC path is not set (i.e., `compile` was not run).</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If `runtime_ai100` is False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>
            <span class="n">generation_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;generation_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">cloud_ai_100_exec_kv</span><span class="p">(</span>
                <span class="n">tokenizer</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
                <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
                <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span><span class="p">,</span>
                <span class="n">is_tlm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only AI_100 runtime is supported right now via generate API&quot;</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_and_get_num_speculative_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validates and retrieves the number of speculative tokens for TLM models.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            The number of speculative tokens provided by the user.</span>
<span class="sd">        prefill_seq_len : int</span>
<span class="sd">            The prefill sequence length.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The determined number of speculative tokens.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If `num_speculative_tokens` is None when `is_tlm` is True.</span>
<span class="sd">        ValueError</span>
<span class="sd">            If `num_speculative_tokens` is not an integer greater than 1.</span>
<span class="sd">            If `prefill_seq_len` is less than `num_speculative_tokens + 1`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;speculative_config&quot;</span><span class="p">):</span>
            <span class="n">num_speculative_tokens_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speculative_config</span><span class="p">[</span><span class="s2">&quot;num_speculative_tokens&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">num_speculative_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;arg `num_speculative_tokens` is a fixed value of </span><span class="si">{</span><span class="n">num_speculative_tokens_</span><span class="si">}</span><span class="s2"> for this model.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; Passed value of </span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2"> will be ignored.&quot;</span>
                <span class="p">)</span>
            <span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="n">num_speculative_tokens_</span>
        <span class="k">elif</span> <span class="n">num_speculative_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;missing required argument `num_speculative_tokens` as `is_tlm` instance variable is True.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_speculative_tokens</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`num_speculative_tokens` arg should be an integer greater than 1, got </span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">num_logits_to_keep</span> <span class="o">=</span> <span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">prefill_seq_len</span> <span class="o">&lt;</span> <span class="n">num_logits_to_keep</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;sequence length (</span><span class="si">{</span><span class="n">prefill_seq_len</span><span class="si">}</span><span class="s2">) must be at least `num_speculative_tokens+1` (</span><span class="si">{</span><span class="n">num_logits_to_keep</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">num_speculative_tokens</span></div>


<div class="viewcode-block" id="QEFFAutoModelForSpeechSeq2Seq"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">QEFFAutoModelForSpeechSeq2Seq</span><span class="p">(</span><span class="n">QEFFTransformersBase</span><span class="p">,</span> <span class="n">MultimodalUtilityMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEfficient class for sequence-to-sequence speech-to-text models (e.g., Whisper, Encoder-Decoder speech models).</span>

<span class="sd">    This class enables efficient export, compilation, and inference of speech models on Cloud AI 100 hardware.</span>
<span class="sd">    It is recommended to use the ``from_pretrained`` method for initialization.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    .. code-block:: python</span>

<span class="sd">        from datasets import load_dataset</span>
<span class="sd">        from transformers import AutoProcessor</span>
<span class="sd">        from QEfficient import QEFFAutoModelForSpeechSeq2Seq</span>

<span class="sd">        base_model_name = &quot;openai/whisper-tiny&quot;</span>
<span class="sd">        ## STEP 1 -- load audio sample, using a standard english dataset, can load specific files if longer audio needs to be tested; also load initial processor</span>
<span class="sd">        ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_dummy&quot;, &quot;clean&quot;, split=&quot;validation&quot;)</span>
<span class="sd">        data = ds[0][&quot;audio&quot;][&quot;array&quot;]</span>
<span class="sd">        # reshape to so shape corresponds to data with batch size 1</span>
<span class="sd">        data = data.reshape(-1)</span>
<span class="sd">        sample_rate = ds[0][&quot;audio&quot;][&quot;sampling_rate&quot;]</span>
<span class="sd">        processor = AutoProcessor.from_pretrained(base_model_name)</span>

<span class="sd">        ## STEP 2 -- init base model</span>
<span class="sd">        qeff_model = QEFFAutoModelForSpeechSeq2Seq.from_pretrained(base_model_name)</span>

<span class="sd">        ## STEP 3 -- export and compile model</span>
<span class="sd">        qeff_model.compile()</span>

<span class="sd">        ## STEP 4 -- generate output for loaded input and processor</span>
<span class="sd">        exec_info = qeff_model.generate(inputs=processor(data, sampling_rate=sample_rate, return_tensors=&quot;pt&quot;), generation_len=25)</span>

<span class="sd">        ## STEP 5 (optional) -- use processor to decode output</span>
<span class="sd">        print(processor.batch_decode(exec_info.generated_ids)[0])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForSpeechSeq2Seq</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">CustomOpsTransform</span><span class="p">,</span> <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">KVCacheTransform</span><span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a QEFFAutoModelForSpeechSeq2Seq instance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : nn.Module</span>
<span class="sd">            A PyTorch model with a sequence-to-sequence speech-to-text head (e.g., Whisper).</span>
<span class="sd">        **kwargs :</span>
<span class="sd">            Additional keyword arguments passed to the base class constructor.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If the model is not a supported speech-to-text model (i.e., not a `ForConditionalGeneration` model).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_class_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">model_class_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;ForConditionalGeneration&quot;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required pytorch module with ForConditionalGeneration, got </span><span class="si">{</span><span class="n">model_class_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_params</span><span class="p">[</span><span class="s2">&quot;qeff_auto_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the configuration dictionary of the underlying HuggingFace model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict</span>
<span class="sd">            The configuration dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="vm">__dict__</span>

<div class="viewcode-block" id="QEFFAutoModelForSpeechSeq2Seq.export"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export the model to ONNX format using ``torch.onnx.export``.</span>

<span class="sd">        This method prepares example inputs and dynamic axes based on the model configuration,</span>
<span class="sd">        then exports the model to an ONNX graph suitable for compilation and deployment on Cloud AI 100 hardware.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        export_dir : str, optional</span>
<span class="sd">            Directory path where the exported ONNX graph will be saved.</span>
<span class="sd">            If not provided, the default export directory is used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the generated ONNX graph file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_dummy_inputs</span><span class="p">()</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_onnx_dynamic_axes</span><span class="p">()</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">dynamic_axes</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModelForSpeechSeq2Seq.compile"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">encoder_ctx_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mxint8_kv_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</span>

<span class="sd">        This method generates a ``qpc`` package. If the model has not been exported yet,</span>
<span class="sd">        this method will handle the export process. Additional arguments for the `qaic-exec`</span>
<span class="sd">        compiler can be passed as keyword arguments.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        onnx_path : str, optional</span>
<span class="sd">            Path to a pre-exported ONNX model. If not provided, the model will be exported first.</span>
<span class="sd">        compile_dir : str, optional</span>
<span class="sd">            Directory to save the generated QPC package.</span>
<span class="sd">        prefill_seq_len : int, optional</span>
<span class="sd">            Prefill sequence length. This parameter is typically not critically used for</span>
<span class="sd">            SpeechSeq2Seq models&#39; decoder compilation as the first decoder input is `seq_len=1`.</span>
<span class="sd">            Default is 1.</span>
<span class="sd">        encoder_ctx_len : int, optional</span>
<span class="sd">            Maximum context length for the encoder part of the model. If None, it&#39;s inferred</span>
<span class="sd">            from the model configuration or defaults (e.g., 1500 for Whisper).</span>
<span class="sd">        ctx_len : int, optional</span>
<span class="sd">            Maximum decoder context length. This defines the maximum output sequence length</span>
<span class="sd">            the compiled model can handle. Default is 150.</span>
<span class="sd">        batch_size : int, optional</span>
<span class="sd">            Batch size. Default is 1.</span>
<span class="sd">        num_devices : int, optional</span>
<span class="sd">            Number of devices to compile for. Default is 1.</span>
<span class="sd">        num_cores : int, optional</span>
<span class="sd">            Number of cores to use for compilation.</span>
<span class="sd">        mxfp6_matmul : bool, optional</span>
<span class="sd">            Use MXFP6 compression for weights. Default is False.</span>
<span class="sd">        mxint8_kv_cache : bool, optional</span>
<span class="sd">            Use MXINT8 compression for KV cache. Default is False.</span>
<span class="sd">        full_batch_size : int, optional</span>
<span class="sd">            Not yet supported for this model.</span>
<span class="sd">        kv_cache_batch_size : int, optional</span>
<span class="sd">            Not yet supported for this model.</span>
<span class="sd">        num_speculative_tokens : int, optional</span>
<span class="sd">            Not yet supported for this model.</span>
<span class="sd">        **compiler_options : dict</span>
<span class="sd">            Additional compiler options for QAIC.</span>

<span class="sd">            **For QAIC Compiler:** Extra arguments for qaic-exec can be passed. Some common options include:</span>

<span class="sd">            - mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</span>
<span class="sd">            - aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</span>
<span class="sd">            - allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</span>

<span class="sd">            Params are converted to flags as below:</span>

<span class="sd">            - ``aic_num_cores=16`` -&gt; ``-aic-num-cores=16``</span>
<span class="sd">            - ``convert_to_fp16=True`` -&gt; ``-convert-to-fp16``</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Path to the compiled QPC package.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">specializations</span><span class="p">,</span> <span class="n">compiler_options</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_specializations</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">encoder_ctx_len</span><span class="p">,</span>
            <span class="n">ctx_len</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">full_batch_size</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Continuous batching is not yet enabled for AutoModelForSpeechSeq2Seq&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kv_cache_batch_size</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Prefix caching is not yet enabled for AutoModelForSpeechSeq2Seq&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mxint8_kv_cache</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;mxint8 cache is not yet enabled for AutoModelForSpeechSeq2Seq&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Speculative decoding is not yet enabled for AutoModelForSpeechSeq2Seq&quot;</span><span class="p">)</span>

        <span class="n">output_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>

        <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>
        <span class="n">custom_io</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">custom_io</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>

        <span class="c1"># Slice output_names to get input names</span>
        <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                <span class="n">custom_io</span><span class="p">[</span><span class="n">output_name</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">)]]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>

        <span class="c1"># Get output names</span>
        <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
                <span class="n">custom_io</span><span class="p">[</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
            <span class="n">compile_dir</span><span class="o">=</span><span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">retained_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModelForSpeechSeq2Seq.generate"><a class="viewcode-back" href="../../../../source/qeff_autoclasses.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate output until ``&lt;|endoftext|&gt;`` token or `generation_len` is reached,</span>
<span class="sd">        by executing the compiled QPC on Cloud AI 100 hardware.</span>

<span class="sd">        This method performs sequential execution based on the compiled model&#39;s batch size</span>
<span class="sd">        and the provided audio tensors. It manages the iterative decoding process and KV cache.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inputs : Dict[str, np.ndarray]</span>
<span class="sd">            Model inputs for inference, typically a dictionary containing:</span>
<span class="sd">            - `input_features` (np.ndarray): Preprocessed audio features.</span>
<span class="sd">            - `decoder_input_ids` (np.ndarray): Initial decoder input IDs (e.g., start token).</span>
<span class="sd">            - `decoder_position_ids` (np.ndarray): Initial decoder position IDs.</span>
<span class="sd">            These should be prepared to match the compiled model&#39;s expectations.</span>
<span class="sd">        generation_len : int</span>
<span class="sd">            Maximum number of tokens to generate. The generation stops if this limit is reached</span>
<span class="sd">            or the model generates an end-of-sequence token.</span>
<span class="sd">        streamer : TextStreamer, optional</span>
<span class="sd">            Streamer to receive generated tokens in real-time. Default is None.</span>
<span class="sd">        device_ids : List[int], optional</span>
<span class="sd">            Device IDs for running the QPC. Defaults to `[0]` if not specified.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        CloudAI100ExecInfoNew</span>
<span class="sd">            Output from the AI 100 runtime, including generated IDs and performance metrics.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        TypeError</span>
<span class="sd">            If the QPC path is not set (i.e., `compile` was not run).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_correct_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

        <span class="c1"># add start token id and initial position ids to inputs</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">input_names</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">output_names</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="c1"># encoder run</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># array to hold generated tokens</span>
        <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="n">generated_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">generated_ids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>

        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_mel_bins</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

        <span class="n">loop_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">num_tokens</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">generation_len</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">num_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">next_token</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span> <span class="o">=</span> <span class="n">calculate_latency</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">loop_start</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">CloudAI100ExecInfoNew</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">generated_ids</span><span class="o">=</span><span class="n">generated_ids</span><span class="p">,</span>
            <span class="n">perf_metrics</span><span class="o">=</span><span class="n">PerfMetrics</span><span class="p">(</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="p">),</span>
        <span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>