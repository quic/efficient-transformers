<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QEfficient Auto Classes &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/my_theme.css?v=f6ee2d30" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CLI API Reference" href="cli_api.html" />
    <link rel="prev" title="Fetaures Enablement Guide" href="features_enablement.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Release Documents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="release_docs.html">Efficient Transformer Library - 1.20.0 Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="supported_features.html">Supported Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Cloud AI 100</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="features_enablement.html">Fetaures Enablement Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">QEfficient Auto Classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#qeffautomodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#high-level-api">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautomodel"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautopeftmodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautoloramodelforcausallm"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.compile"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate"><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautomodelforimagetexttotext"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText.from_pretrained()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qeffautomodelforspeechseq2seq"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">High-Level API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.from_pretrained"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.export()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.generate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cli_api.html">CLI API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">QEfficient Auto Classes</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/qeff_autoclasses.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="qefficient-auto-classes">
<h1>QEfficient Auto Classes<a class="headerlink" href="#qefficient-auto-classes" title="Permalink to this heading"></a></h1>
<section id="qeffautomodelforcausallm">
<span id="id1"></span><h2><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code><a class="headerlink" href="#qeffautomodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qaic_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for Causal Language Models from the HuggingFace hub (e.g., GPT-2, Llama).</p>
<p>This class provides a unified interface for loading, exporting, compiling, and generating
text with causal language models on Cloud AI 100 hardware. It supports features like
continuous batching, speculative decoding (TLM), and on-device sampling.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Hi there!!&quot;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<section id="high-level-api">
<h3>High-Level API<a class="headerlink" href="#high-level-api" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qaic_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEfficient Causal Language Model from a pretrained HuggingFace model or local path.</p>
<p>This is the recommended way to initialize a QEfficient Causal Language Model.
The interface is similar to <code class="docutils literal notranslate"><span class="pre">transformers.AutoModelForCausalLM.from_pretrained</span></code>.
Once initialized, you can use methods such as <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">compile</span></code>, and <code class="docutils literal notranslate"><span class="pre">generate</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>continuous_batching</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether this model will be used for continuous batching in the future.
If not set to True here, the model cannot be exported/compiled for
continuous batching later. Default is False.</p></li>
<li><p><strong>qaic_config</strong> (<em>dict</em><em>, </em><em>optional</em>) – <p>QAIC config dictionary. Supported keys include:</p>
<ul>
<li><p><strong>speculative_model_type</strong> (str): Specify Speculative Decoding Target Language Models.</p></li>
<li><p><strong>include_sampler</strong> (bool): Enable/Disable sampling of next tokens.</p></li>
<li><p><strong>return_pdfs</strong> (bool): Return probability distributions along with sampled next tokens.
For Speculative Decoding Target Language Model, <code class="docutils literal notranslate"><span class="pre">return_pdfs=True</span></code> always.
Otherwise, <code class="docutils literal notranslate"><span class="pre">return_pdfs=True</span></code> for Speculative Decoding Draft Language Model
and <code class="docutils literal notranslate"><span class="pre">return_pdfs=False</span></code> for regular model.</p></li>
<li><p><strong>max_top_k_ids</strong> (int): Maximum number of top K tokens (&lt;= vocab size) to consider during sampling.
The values provided in <code class="docutils literal notranslate"><span class="pre">top_ks</span></code> tensor must be less than this maximum limit.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Positional arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p>
<p><strong>Note:</strong> <cite>attn_implementation</cite> and <cite>low_cpu_mem_usage</cite> are automatically
set to “eager” and False respectively to ensure compatibility.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance initialized with the pretrained weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEFFAutoModelForCausalLM</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Export the model to ONNX format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<p>This method prepares example inputs and dynamic axes based on the model configuration,
then exports the model to an ONNX graph suitable for compilation and deployment
on Cloud AI 100 hardware. It handles KV cache inputs/outputs and sampler-related inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>export_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory path where the exported ONNX graph will be saved.
If not provided, the default export directory is used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the generated ONNX graph file.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_cache_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_speculative_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</p>
<p>This method generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package. If the model has not been exported yet,
this method will handle the export process. Additional arguments for the <cite>qaic-exec</cite>
compiler can be passed as keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Path to a pre-exported ONNX model. If not provided, the model will be exported first.</p></li>
<li><p><strong>compile_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the generated QPC package. If not provided, a default directory is used.</p></li>
<li><p><strong>prefill_seq_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Length of the prefill prompt. Default is 32.</p></li>
<li><p><strong>ctx_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum context length the compiled model can remember. Default is 128.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size. Default is 1.</p></li>
<li><p><strong>full_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Continuous batching batch size. Required if <cite>continuous_batching=True</cite> was
set during <cite>from_pretrained</cite>.</p></li>
<li><p><strong>kv_cache_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size for KV cache. If not provided, it defaults to <cite>full_batch_size</cite> (if
continuous batching) or <cite>batch_size</cite>.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to compile for. Default is 1.</p></li>
<li><p><strong>num_cores</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of cores to use for compilation.</p></li>
<li><p><strong>mxfp6_matmul</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXFP6 compression for weights. Default is False.</p></li>
<li><p><strong>mxint8_kv_cache</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXINT8 compression for KV cache. Default is False.</p></li>
<li><p><strong>num_speculative_tokens</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of speculative tokens for Speculative Decoding Target Language Model.
Required if the model is configured as a Target Language Model (<cite>is_tlm=True</cite>).</p></li>
<li><p><strong>prefill_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, compiles only for the prefill stage. If False, compiles only for
the decode stage. If None, compiles for both stages. Default is None.</p></li>
<li><p><strong>**compiler_options</strong> (<em>dict</em>) – <p>Additional compiler options for QAIC or QNN compilers.</p>
<p><strong>For QAIC Compiler:</strong> Extra arguments for qaic-exec can be passed. Some common options include:</p>
<ul>
<li><p>mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</p></li>
<li><p>aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</p></li>
<li><p>allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</p></li>
</ul>
<p>Params are converted to flags as below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aic_num_cores=16</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-aic-num-cores=16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_to_fp16=True</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-convert-to-fp16</span></code></p></li>
</ul>
<p><strong>For QNN Compiler:</strong> Following arguments can be passed as:</p>
<ul>
<li><p>enable_qnn (bool): Enables QNN Compilation.</p></li>
<li><p>qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the compiled QPC package.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – If <cite>prefill_only</cite> is not a boolean.
    If <cite>full_batch_size</cite> is None when <cite>continuous_batching</cite> is True.
    If <cite>num_speculative_tokens</cite> is None when the model is a TLM.</p></li>
<li><p><strong>ValueError</strong> – If KV caching is requested without continuous batching (<cite>full_batch_size</cite>).
    If <cite>include_sampler</cite> is True and <cite>num_speculative_tokens</cite> is greater than 0.
    If <cite>num_speculative_tokens</cite> is not an integer greater than 1.
    If <cite>prefill_seq_len</cite> is less than <cite>num_speculative_tokens + 1</cite> for TLM models.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime_ai100</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate output by executing the compiled QPC on Cloud AI 100 hardware.</p>
<p>This method runs sequential execution based on the compiled model’s batch size and the number of prompts.
If the number of prompts is not divisible by the batch size, the last batch will be dropped.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizer</em><em> or </em><em>PreTrainedTokenizerFast</em>) – Tokenizer for the model.</p></li>
<li><p><strong>prompts</strong> (<em>list</em><em> of </em><em>str</em>) – List of prompts to generate output for.</p></li>
<li><p><strong>device_id</strong> (<em>list</em><em> of </em><em>int</em><em>, </em><em>optional</em>) – Device IDs for running the QPC. Defaults to <cite>[0]</cite> if not specified.</p></li>
<li><p><strong>runtime_ai100</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to use AI 100 runtime. Default is True.</p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments. Currently supports:
- <cite>generation_len (int, optional)</cite>: The maximum number of tokens to generate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output from the AI 100 runtime, containing generated IDs and performance metrics.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>CloudAI100ExecInfoNew</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – If the QPC path is not set (i.e., <cite>compile</cite> was not run).</p></li>
<li><p><strong>NotImplementedError</strong> – If <cite>runtime_ai100</cite> is False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
</section>
<section id="qeffautomodel">
<span id="id2"></span><h2><code class="docutils literal notranslate"><span class="pre">QEFFAutoModel</span></code><a class="headerlink" href="#qeffautomodel" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for general transformer models from the HuggingFace hub (e.g., BERT, Sentence Transformers).</p>
<p>This class provides a unified interface for loading, exporting, compiling, and running
various encoder-only transformer models on Cloud AI 100 hardware. It supports pooling
for embedding extraction.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;My name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="c1"># Output will be a dictionary containing extracted features.</span>
</pre></div>
</div>
</dd></dl>

<section id="id3">
<h3>High-Level API<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEFFAutoModel.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEfficient transformer model from a pretrained HuggingFace model or local path.</p>
<p>This is the recommended way to initialize a QEfficient transformer model. The interface is similar to
<code class="docutils literal notranslate"><span class="pre">transformers.AutoModel.from_pretrained</span></code>. Once initialized, you can use methods such as <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">compile</span></code>, and <code class="docutils literal notranslate"><span class="pre">generate</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em> or </em><em>Callable</em><em>, </em><em>optional</em>) – The pooling method to use. Options include:
- “mean”: Mean pooling
- “max”: Max pooling
- “cls”: CLS token pooling
- “avg”: Average pooling
- Callable: A custom pooling function
- None: No pooling applied. Default is None.</p></li>
<li><p><strong>*args</strong> – Positional arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p>
<p><strong>Note:</strong> <cite>attn_implementation</cite> and <cite>low_cpu_mem_usage</cite> are automatically
set to “eager” and False respectively to ensure compatibility.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance initialized with the pretrained weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEFFAutoModel</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModel.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export" title="Permalink to this definition"></a></dt>
<dd><p>Export the model to ONNX format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<p>This method prepares example inputs and dynamic axes based on the model configuration,
then exports the model to an ONNX graph suitable for compilation and deployment on Cloud AI 100 hardware.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>export_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory path where the exported ONNX graph will be saved. If not provided,
the default export directory is used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the generated ONNX graph file.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModel.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</p>
<p>This method generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package. If the model has not been exported yet,
this method will handle the export process. Additional arguments for the <cite>qaic-exec</cite>
compiler can be passed as keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Path to a pre-exported ONNX model. If not provided, the model will be exported first.</p></li>
<li><p><strong>compile_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the generated QPC package. If not provided, a default directory is used.</p></li>
<li><p><strong>seq_len</strong> (<em>int</em><em> or </em><em>list</em><em> of </em><em>int</em><em>, </em><em>optional</em>) – The length(s) of the prompt(s) to compile for. Can be a single integer or a list of integers
to create multiple specializations. Default is 32.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size. Default is 1.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to compile for. Default is 1.</p></li>
<li><p><strong>num_cores</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of cores to use for compilation.</p></li>
<li><p><strong>mxfp6_matmul</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXFP6 compression for weights. Default is False.</p></li>
<li><p><strong>**compiler_options</strong> (<em>dict</em>) – <p>Additional compiler options for QAIC or QNN compilers. These are passed directly
to the underlying compilation command.</p>
<p><strong>For QAIC Compiler:</strong> Extra arguments for qaic-exec can be passed. Some common options include:</p>
<ul>
<li><p>mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</p></li>
<li><p>aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</p></li>
<li><p>allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</p></li>
</ul>
<p>Params are converted to flags as below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aic_num_cores=16</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-aic-num-cores=16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_to_fp16=True</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-convert-to-fp16</span></code></p></li>
</ul>
<p><strong>For QNN Compiler:</strong> Following arguments can be passed as:</p>
<ul>
<li><p>enable_qnn (bool): Enables QNN Compilation.</p></li>
<li><p>qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the compiled QPC package.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModel.</span></span><span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime_ai100</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModel.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate output by executing the compiled QPC on Cloud AI 100 hardware or using PyTorch runtime.</p>
<p>This method runs sequential execution based on the compiled model’s batch size and the number of prompts.
If the number of prompts is not divisible by the batch size, the last batch will be dropped.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em><em> or </em><em>np.ndarray</em>) – Input data for the model. For AI 100 runtime, this typically includes
<cite>input_ids</cite> and <cite>attention_mask</cite>.</p></li>
<li><p><strong>device_ids</strong> (<em>list</em><em> of </em><em>int</em><em>, </em><em>optional</em>) – Device IDs for running the QPC. Defaults to <cite>[0]</cite> if not specified and <cite>runtime_ai100</cite> is True.</p></li>
<li><p><strong>runtime_ai100</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to use the AI 100 runtime for inference. If False, the PyTorch
runtime will be used. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output from the AI 100 or PyTorch runtime. The type depends on the runtime and model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor or np.ndarray</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
</section>
<section id="qeffautopeftmodelforcausallm">
<span id="id4"></span><h2><code class="docutils literal notranslate"><span class="pre">QEffAutoPeftModelForCausalLM</span></code><a class="headerlink" href="#qeffautopeftmodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoPeftModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for loading and running Causal Language Models with PEFT adapters (currently only LoRA is supported).</p>
<p>This class enables efficient inference and deployment of PEFT-adapted models on Cloud AI 100 hardware.
Once exported and compiled for an adapter, the same base model can be reused with other compatible adapters.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEffAutoPeftModelForCausalLM</span>

<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ctx_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="c1"># Magicoder adapter</span>
<span class="n">m</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;def fibonacci&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="c1"># Math problems</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?&quot;</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<section id="id5">
<h3>High-Level API<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEffAutoPeftModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEffAutoPeftModelForCausalLM from a pretrained model and adapter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>finite_adapters</strong> (<em>bool</em><em>, </em><em>optional</em>) – Set True to enable finite adapter mode with QEffAutoLoraModelForCausalLM class.</p></li>
<li><p><strong>adapter_name</strong> (<em>str</em><em>, </em><em>optional</em>) – Name used to identify the loaded adapter.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments for peft.AutoPeftModelForCausalLM.</p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments for peft.AutoPeftModelForCausalLM.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance initialized with the pretrained weights and adapter.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEffAutoPeftModelForCausalLM</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>NotImplementedError</strong> – If continuous batching is requested (not supported).</p></li>
<li><p><strong>TypeError</strong> – If adapter name is missing in finite adapter mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export">
<span class="sig-prename descclassname"><span class="pre">QEffAutoPeftModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Export the model with the active adapter to ONNX format.</p>
<p>This method prepares example inputs and dynamic axes based on the model and adapter configuration,
then exports the model to an ONNX graph suitable for compilation and deployment on Cloud AI 100 hardware.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>export_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory path where the exported ONNX graph will be saved.
If not provided, the default export directory is used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the generated ONNX graph file.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile">
<span class="sig-prename descclassname"><span class="pre">QEffAutoPeftModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported ONNX model for Cloud AI 100 hardware.</p>
<p>This method generates a QPC package. If the model has not been exported yet, this method will handle the export process.
Additional arguments for the QAIC compiler can be passed as keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Path to a pre-exported ONNX model.</p></li>
<li><p><strong>compile_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the generated QPC package.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size for compilation. Default is 1.</p></li>
<li><p><strong>prefill_seq_len</strong> (<em>int</em>) – Length of the prefill prompt.</p></li>
<li><p><strong>ctx_len</strong> (<em>int</em>) – Maximum context length the compiled model can remember.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to compile for. Default is 1.</p></li>
<li><p><strong>num_cores</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of cores to use for compilation. Default is 16.</p></li>
<li><p><strong>mxfp6_matmul</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXFP6 compression for weights. Default is False.</p></li>
<li><p><strong>mxint8_kv_cache</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXINT8 compression for KV cache. Default is False.</p></li>
<li><p><strong>**compiler_options</strong> – <p>Additional compiler options for QAIC.</p>
<p><strong>For QAIC Compiler:</strong> Extra arguments for qaic-exec can be passed. Some common options include:</p>
<ul>
<li><p>mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</p></li>
<li><p>aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</p></li>
<li><p>allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</p></li>
</ul>
<p>Params are converted to flags as below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aic_num_cores=16</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-aic-num-cores=16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_to_fp16=True</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-convert-to-fp16</span></code></p></li>
</ul>
<p><strong>For QNN Compiler:</strong> Following arguments can be passed as:</p>
<ul>
<li><p>enable_qnn (bool): Enables QNN Compilation.</p></li>
<li><p>qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the compiled QPC package.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate">
<span class="sig-prename descclassname"><span class="pre">QEffAutoPeftModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">GenerationConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">StoppingCriteria</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streamer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BaseStreamer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/auto.html#QEffAutoPeftModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.auto.QEffAutoPeftModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate tokens from the compiled binary using the active adapter.</p>
<p>This method takes similar parameters as HuggingFace’s <code class="docutils literal notranslate"><span class="pre">model.generate()</span></code> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em><em> or </em><em>np.ndarray</em><em>, </em><em>optional</em>) – Input IDs for generation.</p></li>
<li><p><strong>device_ids</strong> (<em>List</em><em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – Device IDs for running inference.</p></li>
<li><p><strong>generation_config</strong> (<em>GenerationConfig</em><em>, </em><em>optional</em>) – Generation configuration to merge with model-specific config.</p></li>
<li><p><strong>stopping_criteria</strong> (<em>StoppingCriteria</em><em>, </em><em>optional</em>) – Custom stopping criteria for generation.</p></li>
<li><p><strong>streamer</strong> (<em>BaseStreamer</em><em>, </em><em>optional</em>) – Streamer to receive generated tokens.</p></li>
<li><p><strong>**kwargs</strong> – Additional parameters for generation_config or to be passed to the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Generated token IDs.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
</section>
<section id="qeffautoloramodelforcausallm">
<span id="id6"></span><h2><code class="docutils literal notranslate"><span class="pre">QEffAutoLoraModelForCausalLM</span></code><a class="headerlink" href="#qeffautoloramodelforcausallm" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.peft.lora.auto.</span></span><span class="sig-name descname"><span class="pre">QEffAutoLoraModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for loading models with multiple LoRA adapters for causal language modeling.</p>
<p>This class enables mixed batch inference with different adapters on Cloud AI 100 hardware.
Currently, only Mistral and Llama models are supported. Once exported and compiled, the QPC can perform
mixed batch inference using the <cite>prompt_to_adapter_mapping</cite> argument.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.peft.lora</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEffAutoLoraModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">QEffAutoLoraModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;predibase/magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;magicoder&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;code prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;math prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;generic&quot;</span><span class="p">]</span>
<span class="n">m</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span><span class="n">prompt_to_adapter_mapping</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;magicoder&quot;</span><span class="p">,</span> <span class="s2">&quot;gsm8k&quot;</span><span class="p">,</span> <span class="s2">&quot;base&quot;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<section id="id7">
<h3>High-Level API<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEffAutoLoraModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">continuous_batching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qaic_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEfficient Causal Language Model from a pretrained HuggingFace model or local path.</p>
<p>This is the recommended way to initialize a QEfficient Causal Language Model.
The interface is similar to <code class="docutils literal notranslate"><span class="pre">transformers.AutoModelForCausalLM.from_pretrained</span></code>.
Once initialized, you can use methods such as <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">compile</span></code>, and <code class="docutils literal notranslate"><span class="pre">generate</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>continuous_batching</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether this model will be used for continuous batching in the future.
If not set to True here, the model cannot be exported/compiled for
continuous batching later. Default is False.</p></li>
<li><p><strong>qaic_config</strong> (<em>dict</em><em>, </em><em>optional</em>) – <p>QAIC config dictionary. Supported keys include:</p>
<ul>
<li><p><strong>speculative_model_type</strong> (str): Specify Speculative Decoding Target Language Models.</p></li>
<li><p><strong>include_sampler</strong> (bool): Enable/Disable sampling of next tokens.</p></li>
<li><p><strong>return_pdfs</strong> (bool): Return probability distributions along with sampled next tokens.
For Speculative Decoding Target Language Model, <code class="docutils literal notranslate"><span class="pre">return_pdfs=True</span></code> always.
Otherwise, <code class="docutils literal notranslate"><span class="pre">return_pdfs=True</span></code> for Speculative Decoding Draft Language Model
and <code class="docutils literal notranslate"><span class="pre">return_pdfs=False</span></code> for regular model.</p></li>
<li><p><strong>max_top_k_ids</strong> (int): Maximum number of top K tokens (&lt;= vocab size) to consider during sampling.
The values provided in <code class="docutils literal notranslate"><span class="pre">top_ks</span></code> tensor must be less than this maximum limit.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Positional arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p>
<p><strong>Note:</strong> <cite>attn_implementation</cite> and <cite>low_cpu_mem_usage</cite> are automatically
set to “eager” and False respectively to ensure compatibility.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance initialized with the pretrained weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEFFAutoModelForCausalLM</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export">
<span class="sig-prename descclassname"><span class="pre">QEffAutoLoraModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export" title="Permalink to this definition"></a></dt>
<dd><p>Export the model with all loaded adapters to ONNX format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<p>The exported ONNX graph will support mixed batch inference with multiple adapters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>export_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the exported ONNX graph. If not provided, the default export directory is used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the generated ONNX graph.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If no adapters are loaded.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.compile">
<span class="sig-prename descclassname"><span class="pre">QEffAutoLoraModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_cache_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_speculative_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</p>
<p>This method generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package. If the model has not been exported yet,
this method will handle the export process. Additional arguments for the <cite>qaic-exec</cite>
compiler can be passed as keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Path to a pre-exported ONNX model. If not provided, the model will be exported first.</p></li>
<li><p><strong>compile_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the generated QPC package. If not provided, a default directory is used.</p></li>
<li><p><strong>prefill_seq_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Length of the prefill prompt. Default is 32.</p></li>
<li><p><strong>ctx_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum context length the compiled model can remember. Default is 128.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size. Default is 1.</p></li>
<li><p><strong>full_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Continuous batching batch size. Required if <cite>continuous_batching=True</cite> was
set during <cite>from_pretrained</cite>.</p></li>
<li><p><strong>kv_cache_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size for KV cache. If not provided, it defaults to <cite>full_batch_size</cite> (if
continuous batching) or <cite>batch_size</cite>.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to compile for. Default is 1.</p></li>
<li><p><strong>num_cores</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of cores to use for compilation.</p></li>
<li><p><strong>mxfp6_matmul</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXFP6 compression for weights. Default is False.</p></li>
<li><p><strong>mxint8_kv_cache</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXINT8 compression for KV cache. Default is False.</p></li>
<li><p><strong>num_speculative_tokens</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of speculative tokens for Speculative Decoding Target Language Model.
Required if the model is configured as a Target Language Model (<cite>is_tlm=True</cite>).</p></li>
<li><p><strong>prefill_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, compiles only for the prefill stage. If False, compiles only for
the decode stage. If None, compiles for both stages. Default is None.</p></li>
<li><p><strong>**compiler_options</strong> (<em>dict</em>) – <p>Additional compiler options for QAIC or QNN compilers.</p>
<p><strong>For QAIC Compiler:</strong> Extra arguments for qaic-exec can be passed. Some common options include:</p>
<ul>
<li><p>mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</p></li>
<li><p>aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</p></li>
<li><p>allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</p></li>
</ul>
<p>Params are converted to flags as below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aic_num_cores=16</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-aic-num-cores=16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_to_fp16=True</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-convert-to-fp16</span></code></p></li>
</ul>
<p><strong>For QNN Compiler:</strong> Following arguments can be passed as:</p>
<ul>
<li><p>enable_qnn (bool): Enables QNN Compilation.</p></li>
<li><p>qnn_config (str): Path of QNN Config parameters file. Any extra parameters for QNN compilation can be passed via this file.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the compiled QPC package.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – If <cite>prefill_only</cite> is not a boolean.
    If <cite>full_batch_size</cite> is None when <cite>continuous_batching</cite> is True.
    If <cite>num_speculative_tokens</cite> is None when the model is a TLM.</p></li>
<li><p><strong>ValueError</strong> – If KV caching is requested without continuous batching (<cite>full_batch_size</cite>).
    If <cite>include_sampler</cite> is True and <cite>num_speculative_tokens</cite> is greater than 0.
    If <cite>num_speculative_tokens</cite> is not an integer greater than 1.
    If <cite>prefill_seq_len</cite> is less than <cite>num_speculative_tokens + 1</cite> for TLM models.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate">
<span class="sig-prename descclassname"><span class="pre">QEffAutoLoraModelForCausalLM.</span></span><span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_to_adapter_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">runtime</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'AI_100'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/peft/lora/auto.html#QEffAutoLoraModelForCausalLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate output for a batch of prompts using the compiled QPC on Cloud AI 100 hardware.</p>
<p>This method supports mixed batch inference, where each prompt can use a different adapter as specified
by <cite>prompt_to_adapter_mapping</cite>. If the number of prompts is not divisible by the compiled batch size,
the last incomplete batch will be dropped.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizerFast</em><em> or </em><em>PreTrainedTokenizer</em>) – Tokenizer used for inference.</p></li>
<li><p><strong>prompts</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of prompts to generate outputs for.</p></li>
<li><p><strong>prompt_to_adapter_mapping</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of adapter names to use for each prompt. Use “base” for the base model (no adapter).</p></li>
<li><p><strong>device_id</strong> (<em>List</em><em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – Device IDs to use for execution. If <cite>None</cite>, auto-device-picker is used.</p></li>
<li><p><strong>runtime</strong> (<em>str</em><em>, </em><em>optional</em>) – Runtime to use. Only “AI_100” is currently supported. Default is “AI_100”.</p></li>
<li><p><strong>**kwargs</strong> – Additional generation parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Model outputs for each prompt.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If runtime is not “AI_100”.</p></li>
<li><p><strong>TypeError</strong> – If the model has not been compiled.</p></li>
<li><p><strong>RuntimeError</strong> – If the number of prompts does not match the number of adapter mappings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
</section>
<section id="qeffautomodelforimagetexttotext">
<span id="id8"></span><h2><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForImageTextToText</span></code><a class="headerlink" href="#qeffautomodelforimagetexttotext" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForImageTextToText</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_offload</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForImageTextToText"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for multimodal (image-text-to-text) models from the HuggingFace hub.</p>
<p>This class supports both single and dual QPC (Quantized Package Compilation) approaches for efficient deployment on Cloud AI 100 hardware.
It is recommended to use the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">TextStreamer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForImageTextToText</span>

<span class="n">HF_TOKEN</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1"># Your HuggingFace token if needed</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-11B-Vision-Instruct&quot;</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Describe this image.&quot;</span>
<span class="n">image_url</span> <span class="o">=</span> <span class="s2">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg&quot;</span>

<span class="c1"># STEP 1: Load processor and model</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">HF_TOKEN</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForImageTextToText</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">HF_TOKEN</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="n">kv_offload</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># kv_offload=False for single QPC</span>
<span class="p">)</span>

<span class="c1"># STEP 2: Export &amp; Compile</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">prefill_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">ctx_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">img_size</span><span class="o">=</span><span class="mi">560</span><span class="p">,</span>
    <span class="n">num_cores</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># STEP 3: Prepare inputs</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">]</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
    <span class="n">text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span>
    <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="c1"># Consider padding strategy if max_length is crucial</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># STEP 4: Run inference</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<section id="id9">
<h3>High-Level API<a class="headerlink" href="#id9" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForImageTextToText.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_offload</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForImageTextToText.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForImageTextToText.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEfficient image-text-to-text model from a pretrained HuggingFace model or local path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>kv_offload</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses the dual QPC approach (vision encoder KV offloaded).
If False, uses the single QPC approach (entire model in one QPC).
If None, the default behavior of the internal classes is used (typically dual QPC).</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional arguments passed to HuggingFace’s <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>.</p>
<p><strong>Note:</strong> <cite>attn_implementation</cite> and <cite>low_cpu_mem_usage</cite> are automatically set to “eager” and False respectively to ensure compatibility.
<cite>continuous_batching</cite> is not supported for image-text-to-text models.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance initialized with the pretrained weights, wrapped for QEfficient.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEFFAutoModelForImageTextToText</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – If <cite>continuous_batching</cite> is provided as True.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
</section>
<section id="qeffautomodelforspeechseq2seq">
<span id="id10"></span><h2><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></code><a class="headerlink" href="#qeffautomodelforspeechseq2seq" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.transformers.models.modeling_auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>QEfficient class for sequence-to-sequence speech-to-text models (e.g., Whisper, Encoder-Decoder speech models).</p>
<p>This class enables efficient export, compilation, and inference of speech models on Cloud AI 100 hardware.
It is recommended to use the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method for initialization.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForSpeechSeq2Seq</span>

<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;openai/whisper-tiny&quot;</span>
<span class="c1">## STEP 1 -- load audio sample, using a standard english dataset, can load specific files if longer audio needs to be tested; also load initial processor</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span><span class="p">,</span> <span class="s2">&quot;clean&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span>
<span class="c1"># reshape to so shape corresponds to data with batch size 1</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sample_rate</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;sampling_rate&quot;</span><span class="p">]</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>

<span class="c1">## STEP 2 -- init base model</span>
<span class="n">qeff_model</span> <span class="o">=</span> <span class="n">QEFFAutoModelForSpeechSeq2Seq</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>

<span class="c1">## STEP 3 -- export and compile model</span>
<span class="n">qeff_model</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="c1">## STEP 4 -- generate output for loaded input and processor</span>
<span class="n">exec_info</span> <span class="o">=</span> <span class="n">qeff_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">processor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">),</span> <span class="n">generation_len</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1">## STEP 5 (optional) -- use processor to decode output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">exec_info</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<section id="id11">
<h3>High-Level API<a class="headerlink" href="#id11" title="Permalink to this heading"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.</span></span><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Load a QEfficient transformer model from a pretrained HuggingFace model or local path.</p>
<p>This is the recommended way to initialize any QEfficient transformer model.
The interface is similar to <code class="docutils literal notranslate"><span class="pre">transformers.AutoModel.from_pretrained</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<em>str</em>) – Model card name from HuggingFace or local path to model directory.</p></li>
<li><p><strong>*args</strong> – Positional arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p></li>
<li><p><strong>**kwargs</strong> – <p>Keyword arguments passed directly to <cite>cls._hf_auto_class.from_pretrained</cite>.</p>
<p><strong>Note:</strong> <cite>attn_implementation</cite> and <cite>low_cpu_mem_usage</cite> are automatically set to “eager” and False respectively to ensure compatibility.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance of the specific QEFFAutoModel subclass, initialized with the pretrained weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>QEFFTransformersBase</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.export" title="Permalink to this definition"></a></dt>
<dd><p>Export the model to ONNX format using <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code>.</p>
<p>This method prepares example inputs and dynamic axes based on the model configuration,
then exports the model to an ONNX graph suitable for compilation and deployment on Cloud AI 100 hardware.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>export_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory path where the exported ONNX graph will be saved.
If not provided, the default export directory is used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the generated ONNX graph file.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">150</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_cache_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6_matmul</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_speculative_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">compiler_options</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compile the exported ONNX model using the Cloud AI 100 Platform SDK compiler.</p>
<p>This method generates a <code class="docutils literal notranslate"><span class="pre">qpc</span></code> package. If the model has not been exported yet,
this method will handle the export process. Additional arguments for the <cite>qaic-exec</cite>
compiler can be passed as keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_path</strong> (<em>str</em><em>, </em><em>optional</em>) – Path to a pre-exported ONNX model. If not provided, the model will be exported first.</p></li>
<li><p><strong>compile_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – Directory to save the generated QPC package.</p></li>
<li><p><strong>prefill_seq_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Prefill sequence length. This parameter is typically not critically used for
SpeechSeq2Seq models’ decoder compilation as the first decoder input is <cite>seq_len=1</cite>.
Default is 1.</p></li>
<li><p><strong>encoder_ctx_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum context length for the encoder part of the model. If None, it’s inferred
from the model configuration or defaults (e.g., 1500 for Whisper).</p></li>
<li><p><strong>ctx_len</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum decoder context length. This defines the maximum output sequence length
the compiled model can handle. Default is 150.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch size. Default is 1.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to compile for. Default is 1.</p></li>
<li><p><strong>num_cores</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of cores to use for compilation.</p></li>
<li><p><strong>mxfp6_matmul</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXFP6 compression for weights. Default is False.</p></li>
<li><p><strong>mxint8_kv_cache</strong> (<em>bool</em><em>, </em><em>optional</em>) – Use MXINT8 compression for KV cache. Default is False.</p></li>
<li><p><strong>full_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Not yet supported for this model.</p></li>
<li><p><strong>kv_cache_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Not yet supported for this model.</p></li>
<li><p><strong>num_speculative_tokens</strong> (<em>int</em><em>, </em><em>optional</em>) – Not yet supported for this model.</p></li>
<li><p><strong>**compiler_options</strong> (<em>dict</em>) – <p>Additional compiler options for QAIC.</p>
<p><strong>For QAIC Compiler:</strong> Extra arguments for qaic-exec can be passed. Some common options include:</p>
<ul>
<li><p>mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. Defaults to -1.</p></li>
<li><p>aic_enable_depth_first (bool, optional): Enables DFS with default memory size. Defaults to False.</p></li>
<li><p>allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. Defaults to False.</p></li>
</ul>
<p>Params are converted to flags as below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aic_num_cores=16</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-aic-num-cores=16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">convert_to_fp16=True</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">-convert-to-fp16</span></code></p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to the compiled QPC package.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate">
<span class="sig-prename descclassname"><span class="pre">QEFFAutoModelForSpeechSeq2Seq.</span></span><span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streamer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TextStreamer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/QEfficient/transformers/models/modeling_auto.html#QEFFAutoModelForSpeechSeq2Seq.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForSpeechSeq2Seq.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate output until <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> token or <cite>generation_len</cite> is reached,
by executing the compiled QPC on Cloud AI 100 hardware.</p>
<p>This method performs sequential execution based on the compiled model’s batch size
and the provided audio tensors. It manages the iterative decoding process and KV cache.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>np.ndarray</em><em>]</em>) – Model inputs for inference, typically a dictionary containing:
- <cite>input_features</cite> (np.ndarray): Preprocessed audio features.
- <cite>decoder_input_ids</cite> (np.ndarray): Initial decoder input IDs (e.g., start token).
- <cite>decoder_position_ids</cite> (np.ndarray): Initial decoder position IDs.
These should be prepared to match the compiled model’s expectations.</p></li>
<li><p><strong>generation_len</strong> (<em>int</em>) – Maximum number of tokens to generate. The generation stops if this limit is reached
or the model generates an end-of-sequence token.</p></li>
<li><p><strong>streamer</strong> (<em>TextStreamer</em><em>, </em><em>optional</em>) – Streamer to receive generated tokens in real-time. Default is None.</p></li>
<li><p><strong>device_ids</strong> (<em>List</em><em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – Device IDs for running the QPC. Defaults to <cite>[0]</cite> if not specified.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output from the AI 100 runtime, including generated IDs and performance metrics.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>CloudAI100ExecInfoNew</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>TypeError</strong> – If the QPC path is not set (i.e., <cite>compile</cite> was not run).</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="features_enablement.html" class="btn btn-neutral float-left" title="Fetaures Enablement Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="cli_api.html" class="btn btn-neutral float-right" title="CLI API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: release/v1.20
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../../../../index.html">main</a></dd>
        <dd><a href="../../v1.18/index.html">release/v1.18</a></dd>
        <dd><a href="../../v1.19/index.html">release/v1.19</a></dd>
        <dd><a href="../index.html">release/v1.20</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>