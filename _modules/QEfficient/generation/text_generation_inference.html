<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QEfficient.generation.text_generation_inference &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/my_theme.css?v=547657ed" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Upgrade Efficient-Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/upgrade.html">Using GitHub Repository</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/quick_start.html">Transformed models and QPC storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/quick_start.html#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/quick_start.html#python-api">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Command Line Interface Use (CLI)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/cli_api.html"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.infer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/cli_api.html#module-QEfficient.cloud.execute.main"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.execute</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/cli_api.html#module-QEfficient.compile.compile_helper.compile"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/cli_api.html#module-QEfficient.cloud.export.main"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.export</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/cli_api.html#module-QEfficient.cloud.finetune.main"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.finetune</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/hl_api.html">High Level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/ll_api.html">Low Level API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">QEfficient.generation.text_generation_inference</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for QEfficient.generation.text_generation_inference</h1><div class="highlight"><pre>
<span></span><span class="c1"># -----------------------------------------------------------------------------</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="c1">#</span>
<span class="c1"># -----------------------------------------------------------------------------</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">time</span><span class="w"> </span><span class="kn">import</span> <span class="n">perf_counter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.generation.cloud_infer</span><span class="w"> </span><span class="kn">import</span> <span class="n">QAICInferenceSession</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">padding_check_and_fix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils.logging_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>


<div class="viewcode-block" id="PerfMetrics"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.PerfMetrics">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PerfMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Holds all performance metrics</span>

<span class="sd">    Args:</span>
<span class="sd">        :prefill_time (float): Time for prefilling.</span>
<span class="sd">        :decode_perf (float): Decoding performance.</span>
<span class="sd">        :total_perf (float): Total performance.</span>
<span class="sd">        :total_time (float): Total time.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">prefill_time</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">decode_perf</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">total_perf</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">total_time</span><span class="p">:</span> <span class="nb">float</span></div>


<div class="viewcode-block" id="CloudAI100ExecInfo"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.CloudAI100ExecInfo">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CloudAI100ExecInfo</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Holds all the information about Cloud AI 100 execution</span>

<span class="sd">    Args:</span>
<span class="sd">        :batch_size (int): Batch size of the QPC compilation.</span>
<span class="sd">        :generated_texts (Union[List[List[str]], List[str]]): Generated text(s).</span>
<span class="sd">        :generated_ids (Union[List[np.ndarray], np.ndarray]): Generated IDs.</span>
<span class="sd">        :perf_metrics (PerfMetrics): Performance metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">generated_texts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span>
    <span class="n">generated_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="n">perf_metrics</span><span class="p">:</span> <span class="n">PerfMetrics</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Average Prefill time a.k.a TTFT is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">prefill_time</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Decode is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">decode_perf</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens/sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Total is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_perf</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens/sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Total (E2E) inference time is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_time</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens/sec&quot;</span></div>


<div class="viewcode-block" id="CloudAI100ExecInfoNew"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.CloudAI100ExecInfoNew">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CloudAI100ExecInfoNew</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">generated_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="n">perf_metrics</span><span class="p">:</span> <span class="n">PerfMetrics</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Average Prefill time a.k.a TTFT is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">prefill_time</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Decode is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">decode_perf</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> token/sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Total is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_perf</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> token/sec</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Total (E2E) inference time is= </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_time</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> sec&quot;</span></div>


<span class="n">io_files</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">write_io_files</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">write_io_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">write_io_subdir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">write_io_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">include_dims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reset</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">io_files</span>
    <span class="k">if</span> <span class="n">reset</span><span class="p">:</span>
        <span class="n">io_files</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">io</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">write_io_subdir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iname</span><span class="p">,</span> <span class="n">i_array</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">i_array</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">write_io_subdir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">iname</span><span class="si">}</span><span class="s2">.raw&quot;</span><span class="p">)</span>
        <span class="n">i_spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_subdir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">iname</span><span class="si">}</span><span class="s2">.raw&quot;</span><span class="p">,</span>
            <span class="s2">&quot;io-direction&quot;</span><span class="p">:</span> <span class="s2">&quot;in&quot;</span><span class="p">,</span>
            <span class="s2">&quot;elem-size&quot;</span><span class="p">:</span> <span class="n">i_array</span><span class="o">.</span><span class="n">itemsize</span><span class="p">,</span>
            <span class="s2">&quot;map-to&quot;</span><span class="p">:</span> <span class="n">iname</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">include_dims</span><span class="p">:</span>
            <span class="n">i_spec</span><span class="p">[</span><span class="s2">&quot;dims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">i_array</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">io</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_spec</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">o_name</span><span class="p">,</span> <span class="n">o_array</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">o_array</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">write_io_subdir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">o_name</span><span class="si">}</span><span class="s2">.raw&quot;</span><span class="p">)</span>
        <span class="n">o_spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_subdir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">o_name</span><span class="si">}</span><span class="s2">.raw&quot;</span><span class="p">,</span>
            <span class="s2">&quot;io-direction&quot;</span><span class="p">:</span> <span class="s2">&quot;out&quot;</span><span class="p">,</span>
            <span class="s2">&quot;elem-size&quot;</span><span class="p">:</span> <span class="n">o_array</span><span class="o">.</span><span class="n">itemsize</span><span class="p">,</span>
            <span class="s2">&quot;map-to&quot;</span><span class="p">:</span> <span class="n">o_name</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">include_dims</span> <span class="ow">or</span> <span class="n">o_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_RetainedState&quot;</span><span class="p">):</span>
            <span class="n">o_spec</span><span class="p">[</span><span class="s2">&quot;dims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">o_array</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">io</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o_spec</span><span class="p">)</span>
    <span class="n">io_files</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">io</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">write_io_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">write_io_name</span><span class="si">}</span><span class="s2">.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s2">&quot;IO-files&quot;</span><span class="p">:</span> <span class="n">io_files</span><span class="p">},</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">latency_stats_bertstyle</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">qpc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to execute Bertstyle ONNX model on Cloud AI 100.</span>

<span class="sd">    Args:</span>
<span class="sd">        :model_name (str): Hugging Face Model Card name, Example: gpt2.</span>
<span class="sd">        :qpc_path (str): Path to save generated binary file after compilation.</span>
<span class="sd">        :seq_len (int): Sequence length.</span>
<span class="sd">        :prompt (str): Sample prompt for the model text generation.</span>
<span class="sd">        :device_id (List[int]): Device Ids to be used for compilation. If devices &gt; 1, it enables multiple card setup.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
    <span class="n">padding_check_and_fix</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>  <span class="c1"># Check and fix tokenizer viability</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">)</span>
    <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cur_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">init_len</span> <span class="o">=</span> <span class="n">cur_len</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
    <span class="k">while</span> <span class="n">next_token_id</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;=</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">:],</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="n">next_token_id</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">cur_len</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="n">cur_len</span> <span class="o">-</span> <span class="n">init_len</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;tok/s&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="get_compilation_dims"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.get_compilation_dims">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_compilation_dims</span><span class="p">(</span><span class="n">qpc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to fetch compilation dimensions from specializations.json.</span>
<span class="sd">    Uses qpc path to compute path to specializations.json.</span>

<span class="sd">    Args:</span>
<span class="sd">        qpc_path (str): Path to directory comprising generated binary file after compilation.</span>

<span class="sd">    Returns:</span>
<span class="sd">    :tuple: compilation batch size, compilation context length, compilation full batch size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">qpc_base_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">normpath</span><span class="p">(</span><span class="n">qpc_path</span><span class="p">))</span>
    <span class="n">specialization_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">qpc_base_path</span><span class="p">,</span> <span class="s2">&quot;specializations.json&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;specialization_file_path : </span><span class="si">{</span><span class="n">specialization_file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">specialization_file_path</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">specialization_file_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;expected specializations.json file at path, </span><span class="si">{</span><span class="n">qpc_base_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">compilation_batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;specializations&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
    <span class="n">compilation_ctx_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;specializations&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">compilation_fbs</span> <span class="o">:=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;specializations&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">compilation_fbs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">compilation_fbs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">compilation_batch_size</span><span class="p">,</span> <span class="n">compilation_ctx_len</span><span class="p">,</span> <span class="n">compilation_fbs</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">get_input_prompts</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompts_txt_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prompts_txt_file_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please pass at least one argument either using --prompt or --prompts_txt_file_path&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prompts_txt_file_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Found inputs passed using txt file as well as CLI, taking inputs from given txt file&quot;</span><span class="p">)</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">read_prompts_txt_file</span><span class="p">(</span><span class="n">prompts_txt_file_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">prompt</span>


<div class="viewcode-block" id="fix_prompts"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.fix_prompts">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">fix_prompts</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">full_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjusts the list of prompts to match the required batch size.</span>

<span class="sd">    ``Mandatory`` Args:</span>
<span class="sd">        prompt (List[str]): List of input prompts.</span>
<span class="sd">        batch_size (int): The batch size to process at a time.</span>

<span class="sd">    ``Optional`` Args:</span>
<span class="sd">        full_batch_size (Optional[int]): The full batch size if different from batch_size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: Adjusted list of prompts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">exec_batch_size</span> <span class="o">=</span> <span class="n">full_batch_size</span> <span class="k">if</span> <span class="n">full_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">batch_size</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">exec_batch_size</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Number of prompts are less than batch size/full batch size, repeating to required batch size&quot;</span><span class="p">)</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span><span class="n">prompt</span> <span class="o">*</span> <span class="p">(</span><span class="n">exec_batch_size</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))[:</span><span class="n">exec_batch_size</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">full_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Number of prompts are not multiple of batch size, dropping last incomplete batch from given input prompts&quot;</span>
        <span class="p">)</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[:</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">prompt</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">read_prompts_txt_file</span><span class="p">(</span><span class="n">prompts_txt_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prompts_txt_file_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">prompt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">prompt</span>


<span class="k">def</span><span class="w"> </span><span class="nf">print_latency_stats_kv</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">exec_info</span><span class="p">,</span> <span class="n">automation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">automation</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input=&quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output=&quot;</span><span class="p">,</span> <span class="n">exec_info</span><span class="o">.</span><span class="n">generated_texts</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">exec_info</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">========================= Performance Stats =========================&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">exec_info</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch Performance : </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">exec_info</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=====================================================================&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="calculate_latency"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.calculate_latency">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">calculate_latency</span><span class="p">(</span><span class="n">total_decoded_tokens</span><span class="p">,</span> <span class="n">loop_start</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">decode_pause_time</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method will calculate the latency metrics using the time loops and based on the total decoded token count.</span>

<span class="sd">    Args:</span>
<span class="sd">        :total_decoded_tokens (int): Number of tokens generated in decode stage.</span>
<span class="sd">        :loop_start (float): Start time of decode loop.</span>
<span class="sd">        :start (float): Start time.</span>
<span class="sd">        :end (float): End time.</span>
<span class="sd">        :decode_pause_time (float): Total decode pause time in continuous batching decode stage.</span>

<span class="sd">    Returns:</span>
<span class="sd">    :tuple: prefill time, decode performance, total performance, total time</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prefill_time</span> <span class="o">=</span> <span class="n">loop_start</span> <span class="o">-</span> <span class="n">start</span> <span class="o">+</span> <span class="n">decode_pause_time</span>
    <span class="n">decode_perf</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_decoded_tokens</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">loop_start</span> <span class="o">-</span> <span class="n">decode_pause_time</span><span class="p">)</span>
    <span class="n">total_perf</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_decoded_tokens</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
    <span class="k">return</span> <span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span></div>


<div class="viewcode-block" id="cloud_ai_100_exec_kv"><a class="viewcode-back" href="../../../source/hl_api.html#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">cloud_ai_100_exec_kv</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">],</span>
    <span class="n">qpc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompts_txt_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_debug_logs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">write_io_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">automation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_tlm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method generates output until ``eos`` or ``generation_len`` by executing the compiled ``qpc`` on ``Cloud AI 100`` Hardware cards.</span>
<span class="sd">    This is a sequential execution based on the ``batch_size`` of the compiled model and the number of prompts passed.</span>
<span class="sd">    If the number of prompts cannot be divided by the ``batch_size``, the last unfulfilled batch will be dropped.</span>

<span class="sd">    ``Mandatory`` Args:</span>
<span class="sd">        :tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast]): Model tokenizer.</span>
<span class="sd">        :qpc_path (str): Path to the saved generated binary file after compilation.</span>

<span class="sd">    ``Optional`` Args:</span>
<span class="sd">        :prompt (str): Sample prompt for the model text generation. ``Defaults to None``.</span>
<span class="sd">        :prompts_txt_file_path (str): Path of the prompt text file. ``Defaults to None``.</span>
<span class="sd">        :generation_len (int): Maximum context length for the model during compilation. ``Defaults to None``.</span>
<span class="sd">        :device_id (List[int]): Device IDs to be used for execution. If ``len(device_id) &gt; 1``, it enables multiple card setup. If ``None``, auto-device-picker will be used. ``Defaults to None``.</span>
<span class="sd">        :enable_debug_logs (bool): If True, it enables debugging logs. ``Defaults to False``.</span>
<span class="sd">        :stream (bool): If True, enable streamer, which returns tokens one by one as the model generates them. ``Defaults to True``.</span>
<span class="sd">        :Write_io_dir (str): Path to write the input and output files. ``Defaults to None``.</span>
<span class="sd">        :automation (bool): If true, it prints input, output, and performance stats. ``Defaults to False``.</span>
<span class="sd">        :prompt_to_lora_id_mapping (List[int]): Mapping to associate prompts with their respective LoRA adapter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :CloudAI100ExecInfo: Object holding execution output and performance details.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import transformers</span>
<span class="sd">        import QEfficient</span>
<span class="sd">        base_path, onnx_model_path = QEfficient.export(model_name=&quot;gpt2&quot;)</span>
<span class="sd">        qpc_path = QEfficient.compile(onnx_path=onnx_model_path, qpc_path=os.path.join(base_path, &quot;qpc&quot;), num_cores=14, device_group=[0])</span>
<span class="sd">        tokenizer = transformers.AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">        exec_info = QEfficient.cloud_ai_100_exec_kv(tokenizer=tokenizer, qpc_path=qpc_path, prompt=&quot;Hi there!!&quot;, device_id=[0])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">full_batch_size</span> <span class="o">=</span> <span class="n">get_compilation_dims</span><span class="p">(</span><span class="n">qpc_path</span><span class="p">)</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_input_prompts</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">prompts_txt_file_path</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">fix_prompts</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">full_batch_size</span><span class="p">)</span>
    <span class="n">generate_text</span> <span class="o">=</span> <span class="n">TextGeneration</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">qpc_path</span><span class="o">=</span><span class="n">qpc_path</span><span class="p">,</span>
        <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
        <span class="n">enable_debug_logs</span><span class="o">=</span><span class="n">enable_debug_logs</span><span class="p">,</span>
        <span class="n">write_io_dir</span><span class="o">=</span><span class="n">write_io_dir</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="o">=</span><span class="n">full_batch_size</span><span class="p">,</span>
        <span class="n">is_tlm</span><span class="o">=</span><span class="n">is_tlm</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">full_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">exec_info</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">generate_text</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">],</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">stream</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">prefill_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">([</span><span class="n">info</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">prefill_time</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">])</span>
        <span class="n">decode_perf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">([</span><span class="n">info</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">decode_perf</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">])</span>
        <span class="n">total_perf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">([</span><span class="n">info</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_perf</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">])</span>
        <span class="n">total_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">([</span><span class="n">info</span><span class="o">.</span><span class="n">perf_metrics</span><span class="o">.</span><span class="n">total_time</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">])</span>
        <span class="n">generated_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">generated_texts</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">]</span>
        <span class="n">generated_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">generated_ids</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">exec_info</span><span class="p">]</span>

        <span class="n">exec_info</span> <span class="o">=</span> <span class="n">CloudAI100ExecInfo</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">generated_texts</span><span class="o">=</span><span class="n">generated_texts</span><span class="p">,</span>
            <span class="n">generated_ids</span><span class="o">=</span><span class="n">generated_ids</span><span class="p">,</span>
            <span class="n">perf_metrics</span><span class="o">=</span><span class="n">PerfMetrics</span><span class="p">(</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">exec_info</span> <span class="o">=</span> <span class="n">generate_text</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="o">=</span><span class="n">prompt_to_lora_id_mapping</span>
        <span class="p">)</span>

    <span class="n">print_latency_stats_kv</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">exec_info</span><span class="o">=</span><span class="n">exec_info</span><span class="p">,</span> <span class="n">automation</span><span class="o">=</span><span class="n">automation</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exec_info</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">QEffTextGenerationBase</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">],</span>
        <span class="n">qpc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_debug_logs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">write_io_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_tlm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span> <span class="o">=</span> <span class="n">ctx_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="o">=</span> <span class="n">write_io_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="o">=</span> <span class="n">is_tlm</span>

        <span class="c1"># Load QPC</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">enable_debug_logs</span><span class="o">=</span><span class="n">enable_debug_logs</span><span class="p">)</span>

        <span class="c1"># Fetch the variables from the QPC</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_vocab_size</span><span class="p">()</span>  <span class="c1"># Fetch Vocab size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_batch_size_prefill_seq_len</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_decode_seq_len</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">full_batch_size</span> <span class="k">if</span> <span class="n">full_batch_size</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_full_batch_size</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># Check and fetch full batch size if CB is enabled</span>

        <span class="c1"># Initialize the storage variables.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_index</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Variables to be re-initialized for every run</span>
        <span class="c1"># These parameters will be initialized in initialize_lora_id_mapping method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_prefill</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># These parameters will be initialized to np arrays in initialize_decode_inputs method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_input_ids</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_pos_ids</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generation_len</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_tokenizer_params</span><span class="p">()</span>  <span class="c1"># set tokenizer params</span>
        <span class="c1"># Skip inputs/outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">skip_buffers</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">input_names</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">output_names</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;past_&quot;</span><span class="p">)]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_tokenizer_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the tokenizer parameters for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">!=</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Please use padding_side=&#39;right&#39; while initializing the tokenizer&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_full_batch_size</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the full batch size from the session&#39;s bindings or allowed shapes.</span>

<span class="sd">        Returns:</span>
<span class="sd">        full_batch_size: The full batch size fetched from the session&#39;s bindings or allowed shapes. If &quot;batch_index&quot; is not</span>
<span class="sd">        in the session&#39;s binding index map, full_batch_size will be None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">full_batch_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">&quot;batch_index&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">:</span>
                <span class="n">full_batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">full_batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span>
        <span class="k">return</span> <span class="n">full_batch_size</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_batch_size_prefill_seq_len</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the batch size and prefill sequence length from the session&#39;s bindings or allowed shapes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            batch_size: The batch size fetched from the session&#39;s bindings or allowed shapes.</span>
<span class="sd">            prefill_seq_len: The prefill sequence length fetched from the session&#39;s bindings or allowed shapes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">prefill_seq_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">prefill_seq_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span>
        <span class="k">return</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">prefill_seq_len</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_decode_seq_len</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the decode sequence length from the session&#39;s bindings or allowed shapes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            decode_seq_len: The decode sequence length fetched from the session&#39;s bindings or allowed shapes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">decode_seq_len</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">:</span>
            <span class="n">decode_seq_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">decode_seq_len</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_vocab_size</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the vocabulary size from the session&#39;s allowed shapes.</span>
<span class="sd">        Returns:</span>
<span class="sd">            vocab_size: The vocabulary size fetched from the session&#39;s allowed shapes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">allowed_shapes</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">binding_index_map</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_generation_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">max_gen_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the generation length for the model.</span>
<span class="sd">        Args:</span>
<span class="sd">            generation_len: The generation length provided. If None, the method uses max_gen_len.</span>
<span class="sd">            max_gen_len: The maximum allowed generation length.</span>

<span class="sd">        Returns:</span>
<span class="sd">            generation_len: The final generation length, which is either the provided generation_len (if it is not None and not greater than max_gen_len) or max_gen_len.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">generation_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;At least one of ctx_len or generation_len is needed&quot;</span><span class="p">)</span>
            <span class="n">generation_len</span> <span class="o">=</span> <span class="n">max_gen_len</span>
        <span class="k">elif</span> <span class="n">generation_len</span> <span class="o">&gt;</span> <span class="n">max_gen_len</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Passed generation_len is greater than allowed length. &quot;</span>
                <span class="s2">&quot;Make sure this model supports sliding window, such as Mistral&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_len</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;generation length should be greater than zero&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generation_len</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_decode_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function creates the decode inputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: The decode inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">decode_inputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="n">position_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_pos_ids</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_input_ids</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position_ids</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_input_ids</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_pos_ids</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_index</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">:</span>
                <span class="n">first_batch_lora_ids</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">)]</span>
                <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;lora_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">first_batch_lora_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="mi">1</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch_lora_ids</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
                <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;lora_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_lora_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decode_inputs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_next_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches the next token ID from the model&#39;s output logits.</span>
<span class="sd">        The method identifies the token with the highest probability using argmax along the last dimension.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs (dict): A dictionary containing the model&#39;s output logits. The key &quot;logits&quot; should map to a numpy array of shape (batch_size, sequence_length, vocab_size) or (batch_size, vocab_size).</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.ndarray: An array of the next token IDs for each sequence in the batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Get output token</span>
        <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">next_token_id</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_decode_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_prompts</span><span class="p">,</span> <span class="n">execution_batch_size</span><span class="p">,</span> <span class="n">max_gen_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize np arrays for storing the prefill output for all the decode batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_prompts</span><span class="p">,</span> <span class="n">max_gen_length</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">execution_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_pos_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">execution_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generation_len</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">execution_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_lora_id_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the LoRA ID mapping for prefill and decode phases.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt_to_lora_id_mapping (list): An iterable containing the mapping of prompts to LoRA IDs.</span>

<span class="sd">        Sets:</span>
<span class="sd">            self._prompt_to_lora_id_mapping_prefill (deque): A deque containing the prompt to LoRA ID mapping for the prefill phase.</span>
<span class="sd">            self._prompt_to_lora_id_mapping_decode (iterable or deque): The prompt to LoRA ID mapping for the decode phase. If full_batch_size is set, it uses the original iterable; otherwise, it converts it to a deque.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_prefill</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span> <span class="o">=</span> <span class="n">prompt_to_lora_id_mapping</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_decode_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">decode_batch_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the decode input with the generated values.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs (dict): The outputs of the model.</span>
<span class="sd">            position_ids (array): The position IDs.</span>
<span class="sd">            generation_len (int): The generation length.</span>
<span class="sd">            decode_batch_id (int, optional): The decode batch ID. If None, all values are updated. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            next_token_id (array): The next token ID.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">next_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_next_token_id</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="c1"># Store the generated values.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_input_ids</span><span class="p">[</span><span class="n">decode_batch_id</span> <span class="ow">or</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">=</span> <span class="n">next_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_pos_ids</span><span class="p">[</span><span class="n">decode_batch_id</span> <span class="ow">or</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">=</span> <span class="n">position_ids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[</span><span class="n">decode_batch_id</span> <span class="ow">or</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token_id</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generation_len</span><span class="p">[</span><span class="n">decode_batch_id</span> <span class="ow">or</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">=</span> <span class="n">generation_len</span>
        <span class="k">return</span> <span class="n">next_token_id</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_prefill_for_all_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt_queue</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs prefill for all inputs in the prompt queue and updates the decode input.</span>

<span class="sd">        Method iterates over the full batch size and for each decode batch ID, it pops the next prompt from the queue.  It then runs prefill for the next prompt and updates the decode input with the outputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt_queue (deque): The queue of prompts.</span>
<span class="sd">            generation_len (int): The generation length.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">decode_batch_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">):</span>
            <span class="n">next_prompt</span> <span class="o">=</span> <span class="n">prompt_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>

            <span class="c1"># run prefill for num_chunks</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_prefill</span><span class="p">(</span>
                <span class="n">next_prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">decode_batch_id</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_decode_input</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">decode_batch_id</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_prefill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prefill_logit_bs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">decode_batch_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs prefill for a given prompt and generation length.</span>

<span class="sd">        This method tokenize the prompt and calculates the padded length and number of chunks. Calculates the</span>
<span class="sd">        maximum generation length and fetches the generation length. If a batch index for prefill is provided, it sets the batch index in the inputs. The method then runs prefill for each chunk and updates the inputs and outputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (str): The prompt for which to run prefill.</span>
<span class="sd">            generation_len (int): The generation length.</span>
<span class="sd">            prefill_logit_bs (int, optional): The prefill logit batch size. Defaults to 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            outputs (dict): The outputs of the prefill.</span>
<span class="sd">            position_ids (array): The position IDs.</span>
<span class="sd">            generation_len (int): The generation length.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Run prefill</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">padded_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_chunks</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">padded_len</span> <span class="o">//</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span><span class="p">)</span>  <span class="c1"># ceil divide without float</span>
        <span class="n">padded_len</span> <span class="o">=</span> <span class="n">num_chunks</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span>  <span class="c1"># Convert to a multiple of prompt_len</span>

        <span class="c1"># Initialize variables specific to request</span>
        <span class="c1"># Calculate the max generation length.</span>
        <span class="n">max_gen_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span> <span class="o">-</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">generation_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_generation_len</span><span class="p">(</span><span class="n">generation_len</span><span class="p">,</span> <span class="n">max_gen_len</span><span class="p">)</span>

        <span class="c1"># Set the prefill logic buffer</span>
        <span class="n">logits_out_placeholder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">prefill_logit_bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_out_placeholder</span><span class="p">})</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">padded_len</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">padded_len</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">decode_batch_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">decode_batch_id</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_prefill</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">:</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;lora_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_prefill</span><span class="o">.</span><span class="n">popleft</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span>
                <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch_lora_ids</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_prefill</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;lora_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_lora_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks</span><span class="p">):</span>
            <span class="n">chunk_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span>
                <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span>
            <span class="p">]</span>
            <span class="n">chunk_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][</span>
                <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefill_seq_len</span>
            <span class="p">]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">chunk_inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">write_io_files</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span><span class="p">,</span> <span class="s2">&quot;prefill&quot;</span><span class="p">,</span> <span class="s2">&quot;aic_batch_io&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">outputs</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">,</span>
            <span class="n">generation_len</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_continuous_batching_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt_queue</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs continuous batching decode for the given prompt queue and generation length.</span>

<span class="sd">        Method sets up the initial conditions for decoding and preparing the decode inputs. Then enters a loop that continues as long as there are prompts in the queue or any decoding is ongoing. In each iteration of the loop, it runs the session with the current decode inputs, prepares the inputs for the next iteration and updates the decode inputs. If a prompt has been fully decoded, it runs prefill for the next prompt in the queue if available.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt_queue (deque): The queue of prompts to be decoded.</span>
<span class="sd">            generation_len (int): The generation length.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Set logits placeholder for decode</span>
        <span class="n">logits_out_placeholder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_out_placeholder</span><span class="p">})</span>
        <span class="c1"># Generate flag for tracking progress for each batch ID</span>
        <span class="n">current_decode_ongoing</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Generate an array for maintaining the tokens generated in each batch ID</span>
        <span class="n">generated_id_current_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="c1"># Generate a batch ID map for mapping the batch ID if input &gt; full_batch_size.</span>
        <span class="c1"># This ID map will be used for storing all generated tokens</span>
        <span class="n">batch_id_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">)}</span>
        <span class="n">decode_pause_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Prepare decode inputs inputs.</span>
        <span class="n">decode_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_decode_inputs</span><span class="p">()</span>

        <span class="k">while</span> <span class="n">prompt_queue</span> <span class="ow">or</span> <span class="n">current_decode_ongoing</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">)</span>

            <span class="c1"># Prepare inputs for next iteration</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">decode_batch_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_batch_size</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">next_token_id</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
                    <span class="ow">or</span> <span class="n">generated_id_current_index</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_len</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="n">prompt_queue</span><span class="p">:</span>
                        <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
                        <span class="c1"># run prefill for next prompt input.</span>
                        <span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_prefill</span><span class="p">(</span>
                            <span class="n">prompt_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">(),</span>
                            <span class="n">generation_len</span><span class="p">,</span>
                            <span class="n">decode_batch_id</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="p">)</span>

                        <span class="n">new_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_decode_input</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">decode_batch_id</span><span class="p">)</span>

                        <span class="n">batch_id_map</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">batch_id_map</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[</span><span class="n">batch_id_map</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">],</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token_id</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">generated_id_current_index</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_out_placeholder</span><span class="p">})</span>
                        <span class="n">decode_pause_time</span> <span class="o">+=</span> <span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span><span class="p">:</span>
                            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;lora_ids&quot;</span><span class="p">][</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_to_lora_id_mapping_decode</span><span class="p">[</span>
                                <span class="n">batch_id_map</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span>
                            <span class="p">]</span>

                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">current_decode_ongoing</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If the generated sequence is valid and within generation len prepare for next decode</span>
                    <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token_id</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[</span><span class="n">batch_id_map</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">],</span> <span class="n">generated_id_current_index</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">next_token_id</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="p">)</span>

                    <span class="n">generated_id_current_index</span><span class="p">[</span><span class="n">decode_batch_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">decode_pause_time</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decode_inputs</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">transformers</span><span class="o">.</span><span class="n">TextStreamer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default method for running decode. Executes the decoding process for a given set of inputs and a specified generation length.</span>

<span class="sd">        Enters a loop that continues until all sequences are finished or the maximum generation length is reached. In each iteration, it runs the session with the decode inputs, prepares the inputs for the next iteration and checks if all sequences are finished.</span>

<span class="sd">        Args:</span>
<span class="sd">            decode_inputs (dict): The initial inputs for decoding. This should be a dictionary containing &#39;input_ids&#39; and &#39;position_ids&#39;.</span>
<span class="sd">            generation_len (int): Max allowed length for generating tokens. The decoding process will be terminated  when generation length is reached.</span>
<span class="sd">            streamer (transformers.TextStreamer): TextStreamer object to print decoded tokens to console.</span>
<span class="sd">        Returns:</span>
<span class="sd">            num_token (int): The number of tokens processed in the decoding process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">logits_out_placeholder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_out_placeholder</span><span class="p">})</span>
        <span class="n">finished_sequences</span> <span class="o">=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">num_token</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">num_token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">streamer</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">write_io_files</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span><span class="p">,</span> <span class="s2">&quot;decode&quot;</span><span class="p">,</span> <span class="s2">&quot;aic_batch_io&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Prepare inputs for next iteration</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">num_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">finished_sequences</span> <span class="o">|=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>

            <span class="k">if</span> <span class="n">finished_sequences</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">num_token</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate_decode_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decode_inputs</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generator method for yielding decode tokens. Executes the decoding process for a given set of inputs and a specified generation length.</span>

<span class="sd">        Enters a loop that continues until all sequences are finished or the maximum generation length is reached. In each iteration, it runs the session with the decode inputs, prepares the inputs for the next iteration and checks if all sequences are finished.</span>

<span class="sd">        Args:</span>
<span class="sd">            decode_inputs (dict): The initial inputs for decoding. This should be a dictionary containing &#39;input_ids&#39; and &#39;position_ids&#39;.</span>
<span class="sd">            generation_len (int): Max allowed length for generating tokens. The decoding process will be terminated  when generation length is reached.</span>

<span class="sd">        Yields:</span>
<span class="sd">            token_id (int): The token generated in the decoding process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">finished_sequences</span> <span class="o">=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="k">for</span> <span class="n">num_token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">write_io_files</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span><span class="p">,</span> <span class="s2">&quot;decode&quot;</span><span class="p">,</span> <span class="s2">&quot;aic_batch_io&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_write_io_dir</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Prepare inputs for next iteration</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">num_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">finished_sequences</span> <span class="o">|=</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>

            <span class="k">if</span> <span class="n">finished_sequences</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="k">break</span>
        <span class="k">yield</span> <span class="n">decode_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>  <span class="c1"># yield the last token</span>


<span class="k">class</span><span class="w"> </span><span class="nc">TextGeneration</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">],</span>
        <span class="n">qpc_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_debug_logs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">write_io_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_tlm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span> <span class="o">=</span> <span class="n">QEffTextGenerationBase</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="p">,</span> <span class="n">qpc_path</span><span class="p">,</span> <span class="n">full_batch_size</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">enable_debug_logs</span><span class="p">,</span> <span class="n">write_io_dir</span><span class="p">,</span> <span class="n">is_tlm</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">full_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span> <span class="o">=</span> <span class="n">ctx_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_queue</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_text_streamer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">perf_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_model_execution_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method should be called to set/reset inputs</span>
<span class="sd">        Args:</span>
<span class="sd">            :prompt (List[str]): prompts for the model text generation</span>
<span class="sd">            :generation_len (Optional[int], optional): Number of tokens to be generated.</span>
<span class="sd">            :prompt_to_lora_id_mapping (Optional[List[int]], optional): Mapping to associate prompts with their respective LoRA adapter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">execution_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="p">)</span>
        <span class="n">max_gen_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">generation_len</span> <span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ctx_len</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">)</span>

        <span class="c1"># Create a prompt queue.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="c1"># Initialize np arrays for storing the prefill output for all the decode batch size.</span>
        <span class="n">num_prompts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_queue</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">initialize_lora_id_mapping</span><span class="p">(</span><span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">initialize_decode_inputs</span><span class="p">(</span><span class="n">num_prompts</span><span class="p">,</span> <span class="n">execution_batch_size</span><span class="p">,</span> <span class="n">max_gen_length</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_regular_model_execution</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Executes the model in regular mode.</span>
<span class="sd">        This method runs the prefill, prepares the decode inputs, and then runs the decode. The generated texts are decoded and optionally streamed. Latency metrics are calculated and returned.</span>
<span class="sd">        Args:</span>
<span class="sd">            :prompt (List[str]): The list of prompts for the model.</span>
<span class="sd">            :generation_len (Optional[int], optional): The generation length.</span>
<span class="sd">            :stream (Optional[bool], optional): Boolean flag to enable stream output to console.</span>
<span class="sd">            :prompt_to_lora_id_mapping (Optional[List[int]], optional): Mapping to associate prompts with their respective LoRA adapter.</span>

<span class="sd">        Returns:</span>
<span class="sd">        :tuple: A tuple containing performance metrics and generated texts.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_model_execution_inputs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stream</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_text_streamer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_text_streamer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TextStreamer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">run_prefill</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prefill_logit_bs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">update_decode_input</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">)</span>

        <span class="n">decode_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">prepare_decode_inputs</span><span class="p">()</span>

        <span class="n">loop_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>  <span class="c1"># Start decode loop timer</span>
        <span class="n">num_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">run_decode</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_text_streamer</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">generated_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">total_decode_tokens</span> <span class="o">=</span> <span class="n">num_token</span>
        <span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span> <span class="o">=</span> <span class="n">calculate_latency</span><span class="p">(</span>
            <span class="n">total_decode_tokens</span><span class="p">,</span> <span class="n">loop_start</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span> <span class="o">=</span> <span class="n">PerfMetrics</span><span class="p">(</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span><span class="p">,</span> <span class="n">generated_texts</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_continuous_batching_execution</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Executes the model using continuous batching.</span>
<span class="sd">        This method handles the execution of the model when continuous batching is enabled. It runs the prefill step for all inputs, performs continuous batching decode, and then decodes the generated texts. The texts are optionally streamed. Latency metrics are calculated and returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            :prompt (List[str]): The list of prompts for the model.</span>
<span class="sd">            :generation_len (Optional[int], optional): The generation length.</span>
<span class="sd">            :prompt_to_lora_id_mapping (Optional[List[int]], optional): Mapping to associate prompts with their respective LoRA adapter.</span>

<span class="sd">        Returns:</span>
<span class="sd">        :tuple: A tuple containing performance metrics and generated texts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_model_execution_inputs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">run_prefill_for_all_inputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_queue</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">)</span>

        <span class="n">loop_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>  <span class="c1"># Start decode loop timer</span>
        <span class="n">decode_pause_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">run_continuous_batching_decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt_queue</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">generated_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">total_decode_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span> <span class="o">=</span> <span class="n">calculate_latency</span><span class="p">(</span>
            <span class="n">total_decode_tokens</span><span class="p">,</span> <span class="n">loop_start</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">decode_pause_time</span>
        <span class="p">)</span>
        <span class="n">prefill_time</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>  <span class="c1"># Average prefill time for continuous batching</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span> <span class="o">=</span> <span class="n">PerfMetrics</span><span class="p">(</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span><span class="p">,</span> <span class="n">generated_texts</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate_stream_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Executes the model for a given list of prompts and a specified generation length.</span>
<span class="sd">        This method runs the prefill, prepares the decode inputs, and then runs the decode. The tokens are decoded and streamed as they are generated. Latency metrics are calculated and can be retrieved</span>
<span class="sd">        after all tokens are streamed.</span>

<span class="sd">        Args:</span>
<span class="sd">            :prompt (List[str]): The list of prompts for the model.</span>
<span class="sd">            :generation_len (Optional[int], optional): The generation length.</span>
<span class="sd">            :prompt_to_lora_id_mapping (Optional[List[int]], optional): Mapping to associate prompts with their respective LoRA adapter.</span>

<span class="sd">        Yields:</span>
<span class="sd">        :list: A list containing decoded tokens corresponding to each index of batch size.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Streaming tokens is currently unavailable for continuous batch execution.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_model_execution_inputs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">run_prefill</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prefill_logit_bs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">update_decode_input</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">)</span>

        <span class="n">decode_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">prepare_decode_inputs</span><span class="p">()</span>

        <span class="n">loop_start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>  <span class="c1"># Start decode loop timer</span>
        <span class="n">num_token</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">generate_decode_stream</span><span class="p">(</span><span class="n">decode_inputs</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">):</span>
            <span class="n">decoded_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="n">decoded_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_id</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="k">yield</span> <span class="n">decoded_tokens</span>
            <span class="n">num_token</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">total_decode_tokens</span> <span class="o">=</span> <span class="n">num_token</span>
        <span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span> <span class="o">=</span> <span class="n">calculate_latency</span><span class="p">(</span>
            <span class="n">total_decode_tokens</span><span class="p">,</span> <span class="n">loop_start</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perf_metrics</span> <span class="o">=</span> <span class="n">PerfMetrics</span><span class="p">(</span><span class="n">prefill_time</span><span class="p">,</span> <span class="n">decode_perf</span><span class="p">,</span> <span class="n">total_perf</span><span class="p">,</span> <span class="n">total_time</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prompt_to_lora_id_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Executes the model for a given list of prompts and a specified generation length.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (List[str]): The list of prompts for the model.</span>
<span class="sd">            generation_len (Optional[int], optional): The generation length.</span>
<span class="sd">            stream (Optional[bool], optional): Boolean flag to enable stream output to console.</span>
<span class="sd">            prompt_to_lora_id_mapping (Optional[List[int]], optional): Mapping to associate prompts with their respective LoRA adapter.</span>
<span class="sd">        Returns:</span>
<span class="sd">            latency_stats (tuple): A tuple containing the generated texts, performance metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Streamer is currently unavailable for continuous batch execution.&quot;</span><span class="p">)</span>
            <span class="n">perf_metrics</span><span class="p">,</span> <span class="n">generated_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continuous_batching_execution</span><span class="p">(</span>
                <span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Prompt : &quot;</span> <span class="o">+</span> <span class="n">prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Completion :&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">perf_metrics</span><span class="p">,</span> <span class="n">generated_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regular_model_execution</span><span class="p">(</span>
                <span class="n">prompt</span><span class="p">,</span> <span class="n">generation_len</span><span class="p">,</span> <span class="n">stream</span><span class="p">,</span> <span class="n">prompt_to_lora_id_mapping</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
            <span class="n">stream_start</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="k">else</span> <span class="mi">1</span>
            <span class="n">stream_end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">stream_start</span><span class="p">,</span> <span class="n">stream_end</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Prompt : &quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Completion : &quot;</span><span class="p">,</span> <span class="n">generated_texts</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="n">latency_stats</span> <span class="o">=</span> <span class="n">CloudAI100ExecInfo</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_batch_size</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">generated_texts</span><span class="o">=</span><span class="n">generated_texts</span><span class="p">,</span>
            <span class="n">generated_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_qaic_model</span><span class="o">.</span><span class="n">generated_ids</span><span class="p">,</span>
            <span class="n">perf_metrics</span><span class="o">=</span><span class="n">perf_metrics</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">latency_stats</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>