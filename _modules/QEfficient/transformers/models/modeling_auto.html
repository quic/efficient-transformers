<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QEfficient.transformers.models.modeling_auto &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/my_theme.css?v=547657ed" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Upgrade Efficient-Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/upgrade.html">Using GitHub Repository</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/quick_start.html">Transformed models and QPC storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/quick_start.html#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/quick_start.html#python-api">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Command Line Interface Use (CLI)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.infer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html#module-QEfficient.cloud.execute.main"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.execute</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html#qefficient-cloud-compile"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html#qefficient-cloud-export"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.export</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html#qefficient-cloud-finetune"><code class="docutils literal notranslate"><span class="pre">QEfficient.cloud.finetune</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/hl_api.html">High Level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/ll_api.html">Low Level API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">QEfficient.transformers.models.modeling_auto</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for QEfficient.transformers.models.modeling_auto</h1><div class="highlight"><pre>
<span></span><span class="c1"># -----------------------------------------------------------------------------</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="c1">#</span>
<span class="c1"># ----------------------------------------------------------------------------</span>

<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span>

<span class="kn">import</span> <span class="nn">QEfficient</span>
<span class="kn">from</span> <span class="nn">QEfficient.base.modeling_qeff</span> <span class="kn">import</span> <span class="n">QEFFBaseModel</span>
<span class="kn">from</span> <span class="nn">QEfficient.base.onnx_transforms</span> <span class="kn">import</span> <span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span>
<span class="kn">from</span> <span class="nn">QEfficient.generation.cloud_infer</span> <span class="kn">import</span> <span class="n">QAICInferenceSession</span>
<span class="kn">from</span> <span class="nn">QEfficient.transformers.models.pytorch_transforms</span> <span class="kn">import</span> <span class="n">CustomOpsTransform</span><span class="p">,</span> <span class="n">KVCacheTransform</span><span class="p">,</span> <span class="n">SpDTransform</span>
<span class="kn">from</span> <span class="nn">QEfficient.transformers.quantizers.auto</span> <span class="kn">import</span> <span class="n">QEFF_AUTO_QUANTIZATION_CONFIG_MAPPING</span><span class="p">,</span> <span class="n">with_replaced_quantizers</span>
<span class="kn">from</span> <span class="nn">QEfficient.transformers.quantizers.quant_transforms</span> <span class="kn">import</span> <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">GPTQToMatmulNbitsTransform</span>
<span class="kn">from</span> <span class="nn">QEfficient.utils</span> <span class="kn">import</span> <span class="n">constants</span><span class="p">,</span> <span class="n">get_padding_shape_from_config</span>
<span class="kn">from</span> <span class="nn">QEfficient.utils.cache</span> <span class="kn">import</span> <span class="n">to_hashable</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">QEFFTransformersBase</span><span class="p">(</span><span class="n">QEFFBaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parent class for models QEFF provides from transformers i.e. (AutoModel, AutoModelForCausalLM, AutoModelForAudioClassification etc.) from transformers/models/modeling_auto.py file.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span><span class="p">:</span> <span class="nb">type</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;quantization_config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_config</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">QEFF_AUTO_QUANTIZATION_CONFIG_MAPPING</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Please use `from_pretrained` method to load quantized models&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">is_tlm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">is_tlm</span><span class="o">=</span><span class="n">is_tlm</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">mname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEff&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;QEFF&quot;</span><span class="p">):</span>
            <span class="n">mname</span> <span class="o">=</span> <span class="n">mname</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mname</span>


<div class="viewcode-block" id="QEFFAutoModelForCausalLM"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM">[docs]</a><span class="k">class</span> <span class="nc">QEFFAutoModelForCausalLM</span><span class="p">(</span><span class="n">QEFFTransformersBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The QEFF class is designed for manipulating any causal language model from the HuggingFace hub.</span>
<span class="sd">    Although it is possible to initialize the class directly, we highly recommend using the ``from_pretrained`` method for initialization.</span>

<span class="sd">    ``Mandatory`` Args:</span>
<span class="sd">        :model (nn.Module):  PyTorch model</span>
<span class="sd">        :continuous_batching (bool): Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</span>
<span class="sd">        :is_tlm (bool): Whether this is a Speculative Decoding Target Language Model. If set to True, `num_logits_to_keep` input array will have to be fed to control the number of returned logits during prefill/decode.</span>


<span class="sd">    .. code-block:: python</span>

<span class="sd">        from QEfficient import QEFFAutoModelForCausalLM</span>
<span class="sd">        from transformers import AutoTokenizer</span>

<span class="sd">        model_name = &quot;gpt2&quot;</span>
<span class="sd">        model = QEFFAutoModelForCausalLM.from_pretrained(model_name, num_hidden_layers=2)</span>
<span class="sd">        model.compile(prefill_seq_len=128, ctx_len=256, num_cores=16, num_devices=1)</span>

<span class="sd">        tokenizer = AutoTokenizer.from_pretrained(model_name)</span>
<span class="sd">        model.generate(prompts=[&quot;Hi there!!&quot;], tokenizer=tokenizer)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">CustomOpsTransform</span><span class="p">,</span> <span class="n">KVCacheTransform</span><span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">continuous_batching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">is_tlm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">model_class_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">model_class_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;ForCausalLM&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">model_class_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;LMHeadModel&quot;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required pytorch module for CausalLM or LMHeadModel, got </span><span class="si">{</span><span class="n">model_class_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># TODO: remove from version 1.20</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">continuous_batching</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;full_batch_size argument is deprecated. Use continuous_batching=True instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="mi">2</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># Set use_cache=True to get KV values as output during ONNX export</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="o">=</span> <span class="n">continuous_batching</span>

        <span class="k">if</span> <span class="n">is_tlm</span><span class="p">:</span>
            <span class="c1"># TODO: It is possible to always apply this transform and make value of indices as last indices by default in PyTorch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">SpDTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="o">=</span> <span class="n">is_tlm</span>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.from_pretrained"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">continuous_batching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">is_tlm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method serves as the easiest entry point into using QEfficient. The interface is designed to be similar to transformers.AutoModelForCausalLM.</span>
<span class="sd">        Once the model is initialized, you can use other methods such as export, compile, and generate on the same object.</span>

<span class="sd">        Args:</span>
<span class="sd">            :pretrained_name_or_path (str): Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">            :continuous_batching (bool): Whether this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</span>
<span class="sd">            :is_tlm (bool): Whether this is a Speculative Decoding Target Language Model. If set to True, `num_logits_to_keep` input array will have to be fed to control the number of returned logits during prefill/decode.</span>
<span class="sd">            :args, kwargs: Additional arguments to pass to transformers.AutoModelForCausalLM.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from QEfficient import QEFFAutoModelForCausalLM</span>
<span class="sd">            from transformers import AutoTokenizer</span>

<span class="sd">            # Initialize the model using from_pretrained similar to transformers.AutoModelForCausalLM</span>
<span class="sd">            model_name = &quot;gpt2&quot;</span>
<span class="sd">            model = QEFFAutoModelForCausalLM.from_pretrained(model_name)</span>

<span class="sd">            # Now you can directly compile the model for Cloud AI 100</span>
<span class="sd">            model.compile(num_cores=16) # Considering you have a Cloud AI 100 Standard SKU</span>

<span class="sd">            # You can now execute the model</span>
<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(model_name)</span>
<span class="sd">            model.generate(prompts=[&quot;Hi there!!&quot;], tokenizer=tokenizer)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">continuous_batching</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;full_batch_size argument is deprecated. Use continuous_batching=True instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="mi">2</span>
            <span class="p">)</span>

        <span class="bp">self</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">is_tlm</span><span class="o">=</span><span class="n">is_tlm</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="o">=</span> <span class="n">continuous_batching</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># Compute the hash with: model_config, continuous_batching, transforms</span>
        <span class="n">mhash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">sha256</span><span class="p">()</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_diff_dict</span><span class="p">()))</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">({</span><span class="s2">&quot;continuous_batching&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">}))</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">({</span><span class="s2">&quot;is_tlm&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">}))</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform_names</span><span class="p">()))</span>
        <span class="n">mhash</span> <span class="o">=</span> <span class="n">mhash</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()[:</span><span class="mi">16</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mhash</span>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.export"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.export">[docs]</a>    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the model to ``ONNX`` format using ``torch.onnx.export``.</span>

<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            :export_dir (str, optional): The directory path to store ONNX-graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :str: Path of the generated ``ONNX`` graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span>
        <span class="n">fbs</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_FBS</span>
        <span class="n">kv_cache_shape</span> <span class="o">=</span> <span class="n">get_padding_shape_from_config</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span>
        <span class="p">)</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)],</span>
        <span class="p">}</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_cache_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># For GPTBigCode arch the pkv is 3d</span>
            <span class="n">pkv_dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;ctx_len&quot;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># pkv is 4d</span>
            <span class="n">pkv_dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
                <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;ctx_len&quot;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">kv_cache_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
                <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pkv_dynamic_axes</span>
                <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_RetainedState&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="n">nlk</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_NLK</span>  <span class="c1"># Number of Logits to Keep</span>
            <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nlk</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nlk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">}</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">output_names</span><span class="p">,</span>
            <span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModelForCausalLM.compile"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.compile">[docs]</a>    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefill_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">full_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv_cache_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mxint8_kv_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_qnn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">qnn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method compiles the exported ``ONNX`` model using the Cloud AI 100 Platform SDK compiler binary found at ``/opt/qti-aic/exec/qaic-exec`` and generates a ``qpc`` package.</span>
<span class="sd">        If the model has not been exported yet, this method will handle the export process.</span>
<span class="sd">        You can pass any other arguments that the `qaic-exec` takes as extra kwargs.</span>

<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            :onnx_path (str, optional): Path to pre-exported onnx model.</span>
<span class="sd">            :compile_dir (str, optional): Path for saving the qpc generated.</span>
<span class="sd">            :num_cores (int): Number of cores used to compile the model.</span>
<span class="sd">            :num_devices (int): Number of devices the model needs to be compiled for. Defaults to 1.</span>
<span class="sd">            :batch_size (int, optional): Batch size. ``Defaults to 1``.</span>
<span class="sd">            :prefill_seq_len (int, optional): The length of the Prefill prompt should be less that ``prefill_seq_len``. ``Defaults to 32``.</span>
<span class="sd">            :ctx_len (int, optional): Maximum ``ctx`` that the compiled model can remember. ``Defaults to 128``.</span>
<span class="sd">            :full_batch_size (int, optional): Continuous batching batch size.</span>
<span class="sd">            :mxfp6_matmul (bool, optional): Whether to use ``mxfp6`` compression for weights. ``Defaults to False``.</span>
<span class="sd">            :mxint8_kv_cache (bool, optional): Whether to use ``mxint8`` compression for KV cache. ``Defaults to False``.</span>
<span class="sd">            :num_speculative_tokens (int, optional): Number of speculative tokens to take as input for Speculative Decoding Target Language Model.</span>
<span class="sd">            :mos (int, optional): Effort level to reduce on-chip memory. Defaults to -1, meaning no effort. ``Defaults to -1``.</span>
<span class="sd">            :aic_enable_depth_first (bool, optional): Enables DFS with default memory size. ``Defaults to False``.</span>
<span class="sd">            :enable_qnn (bool): Enables QNN Compilation. ``Defaults to False.``</span>
<span class="sd">            :qnn_config (str): Path of QNN Config parameters file. ``Defaults to None.``</span>

<span class="sd">        Returns:</span>
<span class="sd">            :str: Path of the compiled ``qpc`` package.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">:</span>
            <span class="c1"># assert num_speculative_tokens cfg is acceptable if defined</span>
            <span class="k">if</span> <span class="n">num_speculative_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;missing required argument `num_speculative_tokens` as `is_tlm` is True.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_speculative_tokens</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_speculative_tokens</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`num_speculative_tokens` arg should be an integer greater than 1, got </span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">num_logits_to_keep</span> <span class="o">=</span> <span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">prefill_seq_len</span> <span class="o">&lt;</span> <span class="n">num_logits_to_keep</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;sequence length (</span><span class="si">{</span><span class="n">prefill_seq_len</span><span class="si">}</span><span class="s2">) must be at least `num_speculative_tokens+1` (</span><span class="si">{</span><span class="n">num_logits_to_keep</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="ow">and</span> <span class="n">full_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;missing required argument: &#39;full_batch_size&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kv_cache_batch_size</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">full_batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Prefix caching is enabled only for continuous batching as of now. Please pass `full_batch_size` argument and make sure you pass `continuous_batching=True` in the `from_pretrained` call&quot;</span>
            <span class="p">)</span>

        <span class="n">kv_cache_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kv_cache_batch_size</span> <span class="k">if</span> <span class="n">kv_cache_batch_size</span> <span class="k">else</span> <span class="p">(</span><span class="n">full_batch_size</span> <span class="k">if</span> <span class="n">full_batch_size</span> <span class="k">else</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Define prefill specialization</span>
        <span class="n">prefill_specialization</span> <span class="o">=</span> <span class="p">{</span>
            <span class="c1"># Prefill is always run with single BS for continuous batching.</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">prefill_seq_len</span><span class="p">,</span>
            <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="n">ctx_len</span><span class="p">,</span>
            <span class="c1"># TODO: should be renamed to kv_cache_batch_size in specialzation too</span>
        <span class="p">}</span>
        <span class="n">prefill_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="o">...</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">prefill_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">:</span> <span class="n">kv_cache_batch_size</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prefill_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">kv_cache_batch_size</span><span class="p">})</span>
        <span class="n">prefill_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;full_batch_exec_size&quot;</span><span class="p">:</span> <span class="n">full_batch_size</span><span class="p">})</span> <span class="k">if</span> <span class="n">full_batch_size</span> <span class="k">else</span> <span class="o">...</span>
        <span class="n">specializations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">prefill_specialization</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="c1"># Skip decode specialization if we are not in continuous batching and prefill_seq_len=1 as this repeats prefill specialization</span>
        <span class="k">if</span> <span class="n">prefill_seq_len</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">decode_specialization</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">full_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">batch_size</span><span class="p">,</span>
                <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
                <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="n">ctx_len</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
                <span class="n">decode_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;full_batch_size&quot;</span><span class="p">:</span> <span class="n">kv_cache_batch_size</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">decode_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">kv_cache_batch_size</span><span class="p">})</span>
            <span class="n">decode_specialization</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">:</span> <span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">})</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span> <span class="k">else</span> <span class="o">...</span>
            <span class="n">specializations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decode_specialization</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">enable_qnn</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">compiler_options</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Extra arguments to QNN compilation are supported via qnn_config.json only&quot;</span><span class="p">)</span>

            <span class="n">qpc_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qnn_compile</span><span class="p">(</span>
                <span class="n">onnx_path</span><span class="p">,</span>
                <span class="n">compile_dir</span><span class="p">,</span>
                <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
                <span class="n">prefill_seq_len</span><span class="o">=</span><span class="n">prefill_seq_len</span><span class="p">,</span>
                <span class="n">ctx_len</span><span class="o">=</span><span class="n">ctx_len</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">full_batch_size</span><span class="o">=</span><span class="n">full_batch_size</span><span class="p">,</span>
                <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
                <span class="n">num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
                <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
                <span class="n">mxint8_kv_cache</span><span class="o">=</span><span class="n">mxint8_kv_cache</span><span class="p">,</span>
                <span class="n">qnn_config</span><span class="o">=</span><span class="n">qnn_config</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Custom IO</span>
            <span class="n">custom_io</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="s2">&quot;mxint8&quot;</span> <span class="k">if</span> <span class="n">mxint8_kv_cache</span> <span class="k">else</span> <span class="s2">&quot;float16&quot;</span>
            <span class="k">for</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;_RetainedState&quot;</span><span class="p">]:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                        <span class="n">custom_io</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}{</span><span class="n">suffix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_dtype</span>

            <span class="n">qpc_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
                <span class="n">onnx_path</span><span class="p">,</span>
                <span class="n">compile_dir</span><span class="p">,</span>
                <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">retained_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
                <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
                <span class="n">custom_io</span><span class="o">=</span><span class="n">custom_io</span><span class="p">,</span>
                <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
                <span class="n">num_speculative_tokens</span><span class="o">=</span><span class="n">num_speculative_tokens</span><span class="p">,</span>
                <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
                <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">qpc_path</span></div>

    <span class="c1"># FIXME: Update this method to match with transformers AutoModelForCausalLM.generate</span>
<div class="viewcode-block" id="QEFFAutoModelForCausalLM.generate"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModelForCausalLM.generate">[docs]</a>    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizerFast</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span><span class="p">],</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method generates output until ``eos`` or ``generation_len`` by executing the compiled ``qpc`` on ``Cloud AI 100`` Hardware cards.</span>
<span class="sd">        This is a sequential execution based on the ``batch_size`` of the compiled model and the number of prompts passed.</span>
<span class="sd">        If the number of prompts cannot be divided by the ``batch_size``, the last unfulfilled batch will be dropped.</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :tokenizer (Union[PreTrainedTokenizerFast, PreTrainedTokenizer]): Pass tokenizer of the model.</span>
<span class="sd">            :prompts (List[str]): List of prompts to run the execution.</span>

<span class="sd">        ``optional`` Args:</span>
<span class="sd">            :device_id (List[int]): Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</span>
<span class="sd">            :runtime_ai100 (bool, optional): ``AI_100`` and ``PyTorch`` runtime is supported as of now. Defaults to ``True`` for ``AI_100`` runtime.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>
            <span class="n">generation_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;generation_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">cloud_ai_100_exec_kv</span><span class="p">(</span>
                <span class="n">tokenizer</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
                <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
                <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span><span class="p">,</span>
                <span class="n">is_tlm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_tlm</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only AI_100 runtime is supported right now via generate API&quot;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="QEFFAutoModel"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel">[docs]</a><span class="k">class</span> <span class="nc">QEFFAutoModel</span><span class="p">(</span><span class="n">QEFFTransformersBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The QEFFAutoModel class is designed for manipulating any transformer model from the HuggingFace hub.</span>
<span class="sd">    Although it is possible to initialize the class directly, we highly recommend using the ``from_pretrained`` method for initialization.</span>

<span class="sd">    ``Mandatory`` Args:</span>
<span class="sd">        :model (nn.Module): PyTorch model</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from QEfficient import QEFFAutoModel</span>
<span class="sd">        from transformers import AutoTokenizer</span>

<span class="sd">        # Initialize the model using from_pretrained similar to transformers.AutoModel.</span>
<span class="sd">        model = QEFFAutoModel.from_pretrained(&quot;model_name&quot;)</span>

<span class="sd">        # Now you can directly compile the model for Cloud AI 100</span>
<span class="sd">        model.compile(num_cores=16)  # Considering you have a Cloud AI 100 SKU</span>

<span class="sd">        #prepare input</span>
<span class="sd">        tokenizer = AutoTokenizer.from_pretrained(model_name)</span>
<span class="sd">        inputs = tokenizer(&quot;My name is&quot;, return_tensors=&quot;pt&quot;)</span>

<span class="sd">        # You can now execute the model</span>
<span class="sd">        model.generate(inputs)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_hf_auto_class</span> <span class="o">=</span> <span class="n">AutoModel</span>
    <span class="n">_pytorch_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">CustomOpsTransform</span><span class="p">,</span> <span class="n">AwqToMatmulNbitsTransform</span><span class="p">,</span> <span class="n">GPTQToMatmulNbitsTransform</span><span class="p">]</span>
    <span class="n">_onnx_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">FP16ClipTransform</span><span class="p">,</span> <span class="n">SplitTensorsTransform</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@with_replaced_quantizers</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method serves as the easiest entry point into using QEfficient. The interface is designed to be similar to transformers.AutoModel.</span>
<span class="sd">        Once the model is initialized, you can use other methods such as export, compile, and generate on the same object.</span>

<span class="sd">        Args:</span>
<span class="sd">            :pretrained_name_or_path (str): Model card name from HuggingFace or local path to model directory.</span>
<span class="sd">            :args, kwargs: Additional arguments to pass to transformers.AutoModel.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from QEfficient import QEFFAutoModel</span>
<span class="sd">            from transformers import AutoTokenizer</span>

<span class="sd">            # Initialize the model using from_pretrained similar to transformers.AutoModel.</span>
<span class="sd">            model = QEFFAutoModel.from_pretrained(&quot;model_name&quot;)</span>

<span class="sd">            # Now you can directly compile the model for Cloud AI 100</span>
<span class="sd">            model.compile(num_cores=16)  # Considering you have a Cloud AI 100 SKU</span>

<span class="sd">            #prepare input</span>
<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(model_name)</span>
<span class="sd">            inputs = tokenizer(&quot;My name is&quot;, return_tensors=&quot;pt&quot;)</span>

<span class="sd">            # You can now execute the model</span>
<span class="sd">            model.generate(inputs)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;eager&quot;</span><span class="p">}:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Updating attn_implementation=&quot;eager&quot;&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Updating low_cpu_mem_usage=False&quot;</span><span class="p">)</span>

        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;add_pooling_layer&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Removing pooling layer from the model if exist&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;add_pooling_layer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_hf_auto_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># NOTE: model_config.to_diff_dict() has &quot;_name_or_path&quot; attribute which is the model card name or path.</span>
        <span class="c1"># Using same card name will result in same hash. But, using a relative path for one run and</span>
        <span class="c1"># absolute path for another run will result in different hash.</span>
        <span class="c1"># The added complexity to resolve different paths to same location is not worth pursuing.</span>
        <span class="c1"># Instead, advise the user to always provide same relative paths or absolute paths for local models.</span>

        <span class="c1"># Compute the hash with: model_config, transforms</span>
        <span class="n">mhash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">sha256</span><span class="p">()</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_diff_dict</span><span class="p">()))</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform_names</span><span class="p">()))</span>
        <span class="n">mhash</span> <span class="o">=</span> <span class="n">mhash</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()[:</span><span class="mi">16</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mhash</span>

<div class="viewcode-block" id="QEFFAutoModel.export"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.export">[docs]</a>    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the model to ``ONNX`` format using ``torch.onnx.export``.</span>

<span class="sd">        ``Optional`` Args:</span>
<span class="sd">           :export_dir (str, optional): The directory path to store ONNX-graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :str: Path of the generated ``ONNX`` graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span>

        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">}}</span>

        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">output_names</span><span class="p">,</span>
            <span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModel.compile"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.compile">[docs]</a>    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">onnx_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># FIXME: Make this mandatory arg</span>
        <span class="n">mxfp6_matmul</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method compiles the exported ``ONNX`` model using the Cloud AI 100 Platform SDK compiler binary found at ``/opt/qti-aic/exec/qaic-exec`` and generates a ``qpc`` package.</span>
<span class="sd">        If the model has not been exported yet, this method will handle the export process.</span>
<span class="sd">        You can pass any other arguments that the `qaic-exec` takes as extra kwargs.</span>

<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            :onnx_path (str, optional): Path to pre-exported onnx model.</span>
<span class="sd">            :compile_dir (str, optional): Path for saving the qpc generated.</span>
<span class="sd">            :seq_len (int, optional): The length of the prompt should be less that ``seq_len``. ``Defaults to 32``.</span>
<span class="sd">            :batch_size (int, optional): Batch size. ``Defaults to 1``.</span>
<span class="sd">            :num_devices (int): Number of devices the model needs to be compiled for. Defaults to 1.</span>
<span class="sd">            :num_cores (int): Number of cores used to compile the model.</span>
<span class="sd">            :mxfp6_matmul (bool, optional): Whether to use ``mxfp6`` compression for weights. ``Defaults to False``.</span>
<span class="sd">            :aic_enable_depth_first (bool, optional): Enables DFS with default memory size. ``Defaults to False``.</span>
<span class="sd">            :allow_mxint8_mdp_io (bool, optional): Allows MXINT8 compression of MDP IO traffic. ``Defaults to False.``</span>
<span class="sd">        Returns:</span>
<span class="sd">            :str: Path of the compiled ``qpc`` package.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">specializations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">seq_len</span><span class="p">},</span>
        <span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">(</span>
            <span class="n">onnx_path</span><span class="p">,</span>
            <span class="n">compile_dir</span><span class="p">,</span>
            <span class="n">compile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">specializations</span><span class="o">=</span><span class="n">specializations</span><span class="p">,</span>
            <span class="n">convert_to_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">mxfp6_matmul</span><span class="o">=</span><span class="n">mxfp6_matmul</span><span class="p">,</span>
            <span class="n">mdp_ts_num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">aic_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="o">**</span><span class="n">compiler_options</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModel.generate"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.generate">[docs]</a>    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime_ai100</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method generates output by executing PyTorch runtime or the compiled ``qpc`` on ``Cloud AI 100`` Hardware cards.</span>
<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :inputs (Union[torch.Tensor, np.ndarray]): inputs to run the execution.</span>
<span class="sd">        ``optional`` Args:</span>
<span class="sd">            :device_id (List[int]): Ids of devices for running the qpc pass as [0] in case of normal model / [0, 1, 2, 3] in case of tensor slicing model</span>
<span class="sd">            :runtime_ai100 (bool, optional): ``AI_100`` and ``PyTorch`` runtime is supported as of now. Defaults to ``True`` for ``AI_100`` runtime.</span>
<span class="sd">        Returns:</span>
<span class="sd">            :dict: Output from the ``AI_100`` or ``PyTorch`` runtime.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># AI_100 runtime</span>
        <span class="k">if</span> <span class="n">runtime_ai100</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cloud_ai_100_feature_generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="c1"># PyTorch runtime</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pytorch_feature_generate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="QEFFAutoModel.cloud_ai_100_feature_generate"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.cloud_ai_100_feature_generate">[docs]</a>    <span class="k">def</span> <span class="nf">cloud_ai_100_feature_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates features with list of prompts using AI 100 runtime.</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :inputs (Union[torch.Tensor, np.ndarray]): inputs to run the execution.</span>
<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            device_ids (List[int], optional): A list of device IDs to use for the session. Defaults to [0].</span>

<span class="sd">        Returns:</span>
<span class="sd">           np.ndarray: A list of dictionaries containing the generated output features.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span> <span class="o">=</span> <span class="n">QAICInferenceSession</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Prepare input</span>
        <span class="n">input_ids_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">),</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">set_buffers</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">][:,</span> <span class="p">:</span><span class="n">input_ids_len</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">outputs</span></div>

<div class="viewcode-block" id="QEFFAutoModel.pytorch_feature_generate"><a class="viewcode-back" href="../../../../source/hl_api.html#QEfficient.transformers.models.modeling_auto.QEFFAutoModel.pytorch_feature_generate">[docs]</a>    <span class="k">def</span> <span class="nf">pytorch_feature_generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates features from a list of text prompts using a PyTorch model.</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :model: The transformed PyTorch model used for generating features.</span>
<span class="sd">            :inputs (Union[torch.Tensor, np.ndarray]): inputs to run the execution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: A list of output features generated by the model for each prompt.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>