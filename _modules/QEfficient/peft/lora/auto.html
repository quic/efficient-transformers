<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QEfficient.peft.lora.auto &mdash; efficient-transformers main documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/my_theme.css?v=547657ed" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=d01aebe5"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            efficient-transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/introduction.html">Introduction Qualcomm <code class="docutils literal notranslate"><span class="pre">efficient-transformers</span></code> library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html#sanity-check">Sanity Check</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Upgrade Efficient-Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/upgrade.html">Using GitHub Repository</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Cloud AI 100</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/cli_api.html">Command Line Interface Use (CLI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/python_api.html">Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">QAIC Finetune</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/finetune.html">Finetune Infra</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">efficient-transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">QEfficient.peft.lora.auto</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for QEfficient.peft.lora.auto</h1><div class="highlight"><pre>
<span></span><span class="c1"># -----------------------------------------------------------------------------</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="c1">#</span>
<span class="c1"># ----------------------------------------------------------------------------</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">hashlib</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">load_peft_weights</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">QEfficient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient</span><span class="w"> </span><span class="kn">import</span> <span class="n">QEFFAutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.peft.lora.pytorch_transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraModelInputsTransform</span><span class="p">,</span> <span class="n">TargetModulesTransform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">constants</span><span class="p">,</span> <span class="n">get_padding_shape_from_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils.cache</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_hashable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">QEfficient.utils.logging_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>


<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">QEffAutoLoraModelForCausalLM</span><span class="p">(</span><span class="n">QEFFAutoModelForCausalLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QEff class for loading models with multiple LoRA adapters. Currently only Mistral and Llama model are supported.</span>
<span class="sd">    Once exported and compiled, the qpc can perform mixed batch inference with provided `prompt_to_adapter_mapping`.</span>

<span class="sd">    Args:</span>
<span class="sd">        :model (nn.Module): PyTorch model</span>
<span class="sd">        :continuous_batching (bool): Weather this model will be used for continuous batching in future. If this is not set True here, the model can not be exported/compiled for continuous batching later.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from QEfficient.peft.lora import QEffAutoLoraModelForCausalLM</span>

<span class="sd">        m = QEffAutoPeftModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.1&quot;)</span>
<span class="sd">        m.load_adapter(&quot;predibase/gsm8k&quot;, &quot;gsm8k&quot;)</span>
<span class="sd">        m.load_adapter(&quot;predibase/magicoder&quot;, &quot;magicoder&quot;)</span>
<span class="sd">        m.compile(num_cores=16, device_group=[0])</span>

<span class="sd">        prompts=[&quot;code prompt&quot;, &quot;math prompt&quot;, &quot;generic&quot;]</span>
<span class="sd">        m.generate(prompts, device_group=[0], prompt_to_adapter_mapping=[&quot;magicoder&quot;,&quot;gsm8k_id&quot;,&quot;base&quot;])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">continuous_batching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">continuous_batching</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;QEffMistralForCausalLM&quot;</span><span class="p">,</span> <span class="s2">&quot;QEffLlamaForCausalLM&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only QEffMistralForCausalLM and QEffLlamaForCausalLM model are supported but get </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lora_rank</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_modules_for_all_adapters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">mhash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">sha256</span><span class="p">()</span>

        <span class="c1"># should use model config here</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_diff_dict</span><span class="p">()))</span>

        <span class="c1"># create active adapter config dict</span>
        <span class="n">active_adapter_configs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">adpt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">active_adapter_configs</span><span class="p">[</span><span class="n">adpt</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">adpt</span><span class="p">]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="n">active_adapter_configs</span><span class="p">))</span>

        <span class="c1"># create active adapter weight dict</span>
        <span class="n">active_adapter_weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">adpt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">active_adapter_weights</span><span class="p">[</span><span class="n">adpt</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="p">[</span><span class="n">adpt</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="n">active_adapter_weights</span><span class="p">))</span>

        <span class="c1"># ensure model will be exported again if order of adapters changes</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">))</span>

        <span class="c1"># noncb &amp; cb should have different onnx &amp; qpc</span>
        <span class="n">mhash</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_hashable</span><span class="p">({</span><span class="s2">&quot;continuous_batching&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">}))</span>

        <span class="n">mhash</span> <span class="o">=</span> <span class="n">mhash</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()[:</span><span class="mi">16</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mhash</span>

<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM.download_adapter"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.download_adapter">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">download_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">adapter_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PeftConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads a new adapter from huggingface hub or local path into CPU cache</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :adapter_model_id (str): Adapter model ID from huggingface hub or local path</span>
<span class="sd">            :adapter_name (str): Adapter name to be used to downloaded this adapter</span>
<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            :adapter_weight (dict): Adapter weight tensors in dictionary format</span>
<span class="sd">            :adapter_config (PeftConfig): Adapter config in the format of PeftConfig</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if adapter name already loaded</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="ow">and</span> <span class="p">(</span><span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> has been loaded. Skip download.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">adapter_weight</span> <span class="ow">and</span> <span class="n">adapter_config</span><span class="p">:</span>  <span class="c1"># if sufficiently get adapter weight and adpater config</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adapter_weight</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adapter_config</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># donwload with adapter_model_id</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">load_peft_weights</span><span class="p">(</span><span class="n">adapter_model_id</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">adapter_model_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM.load_adapter"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.load_adapter">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">adapter_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PeftConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load adapter into CPU cache and set it as active</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :adapter_model_id (str): Adapter model ID from huggingface hub or local path</span>
<span class="sd">            :adapter_name (str): Adapter name to be used to load this adapter</span>
<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            :adapter_weight (dict): Adapter weight tensors in dictionary format</span>
<span class="sd">            :adapter_config (PeftConfig): Adapter config in the format of PeftConfig</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check if adapter name already exist and activated</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> exists and activated. Please provide a different adapter_name.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">download_adapter</span><span class="p">(</span><span class="n">adapter_model_id</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">adapter_weight</span><span class="p">,</span> <span class="n">adapter_config</span><span class="p">)</span>

            <span class="c1"># starting from the second adapter_name, check if adapters has same target module and rank</span>
            <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span>
                <span class="o">!=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> must have same target_modules as </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">r</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">r</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> must have same rank as </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># set active adapter id to current max if adapter_name is new</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># reserve 0 for base</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span></div>

<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM.unload_adapter"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.unload_adapter">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">unload_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deactivate adpater and remove it from CPU cache</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :adapter_name (str): Adapter name to be unloaded</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># step1: remove from active list if it&#39;s there</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter name </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> is not set active yet&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>

        <span class="c1"># renumbering of active adapter id</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deleting </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> from active adapters.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">onnx_path</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Please redo compile_and_export() to reflect the active adapters changes.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">onnx_path</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># step2: delete from cache</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unloading </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> from CPU cache.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">True</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Set adapter is not supported in finite_adapters mode&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_load_adapter_weights_to_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Loads adapter weights to the model&#39;s multilora layer in a stacked format&quot;</span>

        <span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">target_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_modules_for_all_adapters</span><span class="p">:</span>
                <span class="c1"># stack all adapters weights</span>
                <span class="n">a_tensor_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">b_tensor_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">s_tensor_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

                <span class="k">for</span> <span class="n">lora_name</span><span class="p">,</span> <span class="n">lora_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">target_module</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">]:</span>
                        <span class="n">a_tensor_list</span><span class="p">[</span><span class="n">lora_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="p">[</span><span class="n">lora_name</span><span class="p">][</span>
                                <span class="sa">f</span><span class="s2">&quot;base_model.model.model.layers.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.self_attn.</span><span class="si">{</span><span class="n">target_module</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span>
                            <span class="p">]</span>
                        <span class="p">)</span>
                        <span class="n">b_tensor_list</span><span class="p">[</span><span class="n">lora_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">adapter_weights</span><span class="p">[</span><span class="n">lora_name</span><span class="p">][</span>
                                <span class="sa">f</span><span class="s2">&quot;base_model.model.model.layers.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.self_attn.</span><span class="si">{</span><span class="n">target_module</span><span class="si">}</span><span class="s2">.lora_B.weight&quot;</span>
                            <span class="p">]</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Target module not supported!!&quot;</span><span class="p">)</span>

                    <span class="n">s_tensor_list</span><span class="p">[</span><span class="n">lora_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">lora_name</span><span class="p">]</span><span class="o">.</span><span class="n">lora_alpha</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="p">[</span><span class="n">lora_name</span><span class="p">]</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="c1"># dummy zero tensor for base model</span>
                <span class="n">a_tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a_tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">b_tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b_tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">s_tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

                <span class="c1"># stack weight tensors</span>
                <span class="n">stacked_lora_a</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">a_tensor_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># &lt;num_loras, 1, in_feature, r&gt;</span>
                <span class="n">stacked_lora_b</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">b_tensor_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># &lt;num_loras, 1, r, out_feature&gt;</span>
                <span class="n">stacked_lora_s</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">s_tensor_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># &lt;num_loras, 1, 1, 1&gt;</span>

                <span class="c1"># stored weight to corresponding ops</span>
                <span class="k">if</span> <span class="n">target_module</span> <span class="o">==</span> <span class="s2">&quot;q_proj&quot;</span><span class="p">:</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">q_proj</span>
                <span class="k">elif</span> <span class="n">target_module</span> <span class="o">==</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">:</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">k_proj</span>
                <span class="k">elif</span> <span class="n">target_module</span> <span class="o">==</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">:</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">v_proj</span>
                <span class="k">elif</span> <span class="n">target_module</span> <span class="o">==</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">:</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">o_proj</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Target module not supported!!&quot;</span><span class="p">)</span>

                <span class="n">module</span><span class="o">.</span><span class="n">lora_a_weights</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">stacked_lora_a</span><span class="p">)</span>
                <span class="n">module</span><span class="o">.</span><span class="n">lora_b_weights</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">stacked_lora_b</span><span class="p">)</span>
                <span class="n">module</span><span class="o">.</span><span class="n">lora_scalings</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">stacked_lora_s</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_adapter_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize the fixed lora model with multiple adapter weigths standby&quot;</span>

        <span class="c1"># set lora rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_rank</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">r</span>

        <span class="c1"># do the module replacement</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">LoraModelInputsTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">target_modules_for_all_adapters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adapter_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">TargetModulesTransform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_modules_for_all_adapters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_rank</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># load_weight to model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_adapter_weights_to_model</span><span class="p">()</span>

<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM.export"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.export">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the model to ``ONNX`` format using ``torch.onnx.export``.</span>
<span class="sd">        We currently don&#39;t support exporting non-transformed models. Please refer to the ``convert_to_cloud_bertstyle`` function in the **Low-Level API** for a legacy function that supports this.&quot;</span>

<span class="sd">        ``Optional`` Args:</span>
<span class="sd">            does not any arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :str: Path of the generated ``ONNX`` graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># initialize the adapter model</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Please use load_adapter() to add at least one adapter; otherwise, refer to QEFFAutoModelForCausalLM for base model usage&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_adapter_model</span><span class="p">()</span>

        <span class="n">bs</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_BATCH_SIZE</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_SEQ_LEN</span>
        <span class="n">fbs</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">ONNX_EXPORT_EXAMPLE_FBS</span>
        <span class="n">kv_cache_shape</span> <span class="o">=</span> <span class="n">get_padding_shape_from_config</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">fbs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span>
        <span class="p">)</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)],</span>
            <span class="s2">&quot;lora_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
            <span class="s2">&quot;lora_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">},</span>
        <span class="p">}</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">kv</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">]:</span>
                <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">kv_cache_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
                <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;full_batch_size&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span> <span class="k">else</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
                    <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;ctx_len&quot;</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="n">output_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;past_</span><span class="si">{</span><span class="n">kv</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_RetainedState&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_batching</span><span class="p">:</span>
            <span class="n">example_inputs</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dynamic_axes</span><span class="p">[</span><span class="s2">&quot;batch_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">output_names</span><span class="p">,</span>
            <span class="n">dynamic_axes</span><span class="p">,</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="QEffAutoLoraModelForCausalLM.generate"><a class="viewcode-back" href="../../../../source/python_api.html#QEfficient.peft.lora.auto.QEffAutoLoraModelForCausalLM.generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizerFast</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span><span class="p">],</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">prompt_to_adapter_mapping</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runtime</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;AI_100&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method generates output until ``eos`` or ``generation_len`` by executing the compiled ``qpc`` on ``Cloud AI 100`` Hardware cards.</span>
<span class="sd">        This is a sequential execution based on the ``batch_size`` of the compiled model and the number of prompts passed.</span>
<span class="sd">        If the number of prompts cannot be divided by the ``batch_size``, the last unfulfilled batch will be dropped.</span>

<span class="sd">        ``Mandatory`` Args:</span>
<span class="sd">            :tokenizer (PreTrainedTokenizerFast or PreTrainedTokenizer): The tokenizer used in the inference</span>
<span class="sd">            :prompts (List[str]): List of prompts to run the execution.</span>
<span class="sd">            :prompt_to_adapter_mapping (List[str]): The sequence of the adapter names will be matched with sequence of prompts and corresponding adapters will be used for the prompts.&quot;base&quot; for base model (no adapter).</span>
<span class="sd">        ``optional`` Args:</span>
<span class="sd">            :device_id (List[int]): Device IDs to be used for execution. If ``len(device_id) &gt; 1``, it enables multiple card setup. If ``None``, auto-device-picker will be used. ``Defaults to None``.</span>
<span class="sd">            :runtime (str, optional): Only ``AI_100`` runtime is supported as of now; ``ONNXRT`` and ``PyTorch`` coming soon. Defaults to &quot;AI_100&quot;.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">runtime</span> <span class="o">!=</span> <span class="s2">&quot;AI_100&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only AI_100 runtime is supported right now via generate API&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Please run compile API first!&quot;</span><span class="p">)</span>
        <span class="n">generation_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;generation_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt_to_adapter_mapping</span><span class="p">:</span>
            <span class="n">prompt_to_adapter_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;base&quot;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">))]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_to_adapter_mapping</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of prompts should match number of prompt_to_adapter_mapping, got len(prompts) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2">, len(prompt_to_adapter_mapping) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_to_adapter_mapping</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">QEfficient</span><span class="o">.</span><span class="n">cloud_ai_100_exec_kv</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qpc_path</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
            <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
            <span class="n">generation_len</span><span class="o">=</span><span class="n">generation_len</span><span class="p">,</span>
            <span class="n">prompt_to_lora_id_mapping</span><span class="o">=</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter_to_id</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;base&quot;</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">prompt_to_adapter_mapping</span>
            <span class="p">],</span>
        <span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      Version: Main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      Versions
      <dl>
        <dd><a href="../index.html">main</a></dd>
        <dd><a href="release/v1.18/index.html">release/v1.18</a></dd>
      </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>