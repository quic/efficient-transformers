{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a341fa4-b4dc-4cea-a4b3-249aa5fc9394",
   "metadata": {},
   "source": [
    "### Demonstrate the LLM GPT2 Model OnBoarding on Cloud AI 100 Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eef7ea-3488-414c-9e36-e960abba30c9",
   "metadata": {},
   "source": [
    "##### Download the OpenSource GPT2 based HuggingFace Model and Save in local *Cache* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f82d5-17df-4fc9-a180-05edd032f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Orignal Transformer model\n",
    "import os\n",
    "\n",
    "from QEfficient import QEFFAutoModel\n",
    "\n",
    "# Please uncomment and use appropriate Cache Directory for transformers, in case you don't want to use default ~/.cache dir.\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/local/mnt/workspace/hf_cache\"\n",
    "\n",
    "#ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "#CACHE_DIR = os.path.join(ROOT_DIR, \"tmp\"), you can use a different location for just one model by passing this param as cache_dir in below API.\n",
    "\n",
    "# Model-Card name to be onboarded (This is HF Model Card name) : https://huggingface.co/gpt2-xl\n",
    "model_name = \"gpt2\"  # Similar, we can change model name and generate corresponding models, if we have added the support in the lib.\n",
    "\n",
    "qeff_model = QEFFAutoModel.from_pretrained(model_name, cache_dir=None)\n",
    "print(f\"{model_name} from hugging-face \\n\", qeff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dfa0a-d8fe-4472-bf00-55e563ae9058",
   "metadata": {},
   "source": [
    "##### Now we Modify the GPT2 Classes using the Optimized Software Library to generate model for Cloud AI 100.\n",
    "##### Here we generate models with below Optimizations:\n",
    "\n",
    "* RMS Norm Fixes for FP16 Overflows and Underflow\n",
    "* Causal Mask Fix\n",
    "* Handling FP16 Overflows.\n",
    "* KV Cache (Retention Changes).\n",
    "* Triu/Tril Ops support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4543b94-9b50-4bcc-90c6-484ab694c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QEfficient\n",
    "\n",
    "# Easy and minimal api to update the model\n",
    "model_transformed = QEfficient.transform(qeff_model, form_factor=\"cloud\")\n",
    "\n",
    "print(\"Model after Optimized transformations \\n\", model_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1795ac7-d92c-42bb-8148-cb9da87439a6",
   "metadata": {},
   "source": [
    "##### Export the Optimized Pytorch model to the Onnx Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4d6dd-9973-4608-b68b-ec6825cfef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QEfficient.utils import load_hf_tokenizer\n",
    "# We can now export the modified models to Onnx framework\n",
    "# This will generate single Onnx Model for both Prefill and Decode Variations which are optimized for\n",
    "# Cloud AI 100 Platform.\n",
    "\n",
    "# This will generate Onnx model, clip the overflow constants to fp16\n",
    "# Verify the model on Onnxruntime vs Pytorch\n",
    "# Then generate inputs and customio yaml file required for compilation.\n",
    "\n",
    "# We can generate the KV Style models with the flag \"kv\"\n",
    "# Bertstyle models do not have any optimization w.r.t KV cache changes and are unoptimized version.\n",
    "# It is recommended to use kv=True for better performance.\n",
    "tokenizer = load_hf_tokenizer(model_name, use_cache=True)\n",
    "base_path, onnx_path = QEfficient.export(\n",
    "    model_name=model_name,\n",
    "    model_kv=model_transformed,\n",
    "    tokenizer=tokenizer,\n",
    "    kv=True,\n",
    "    form_factor=\"cloud\",\n",
    "    return_path=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126872b-8bc0-412e-956f-50eb01e5b6de",
   "metadata": {},
   "source": [
    "##### Compile the Optimized KV Cache Single Model on Cloud AI 100 (**Config; 16C;32PL;128CTX;FP16**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48be5da-02a1-4d7e-9b5f-a6dcca141d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use platform SDk to Check num_cores for your card.\n",
    "\n",
    "generated_qpc_path = QEfficient.compile(\n",
    "    onnx_path=onnx_path,\n",
    "    num_cores=14,\n",
    "    qpc_path=os.path.dirname(base_path),\n",
    "    mxfp6=False,\n",
    "    device_group=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa240c-f40b-4bf8-a982-8ffff4ff3978",
   "metadata": {},
   "source": [
    "##### Execute the Optimized KV Model on H/W and Print the Latency Stats *(tok/sec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711fc74-aa5d-4e20-af0e-0d461d2e19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QEfficient.generation.text_generation_inference import cloud_ai_100_exec_kv, get_compilation_batch_size\n",
    "\n",
    "# post compilation, we can print the latency stats for the kv models, We provide API to print token and Latency stats on AI 100\n",
    "# We need the compiled prefill and decode qpc to compute the token generated, This is based on Greedy Sampling Approach\n",
    "batch_size = get_compilation_batch_size(generated_qpc_path)\n",
    "cloud_ai_100_exec_kv(batch_size=batch_size, tokenizer=tokenizer, qpc_path=generated_qpc_path, device_id=[0], prompt=[\"My name is\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
