{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a341fa4-b4dc-4cea-a4b3-249aa5fc9394",
   "metadata": {},
   "source": [
    "### Demonstrate the LLM MPT Model OnBoarding on Cloud AI 100 Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eef7ea-3488-414c-9e36-e960abba30c9",
   "metadata": {},
   "source": [
    "##### Download the OpenSource MPT based HuggingFace Model and Save in local *Cache* directory\n",
    "###### Now we Modify the MPT Classes using the Optimized Software Library to generate model for Cloud AI 100.\n",
    "###### Here we generate models with below Optimizations:\n",
    "\n",
    "* RMS Norm Fixes for FP16 Overflows and Underflow\n",
    "* Causal Mask Fix\n",
    "* Handling FP16 Overflows.\n",
    "* KV Cache (Retention Changes).\n",
    "* Triu/Tril Ops support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f82d5-17df-4fc9-a180-05edd032f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Original Transformer model\n",
    "# Initiate the tokenizer for transformers library\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from QEfficient import QEFFAutoModelForCausalLM as AutoModelForCausalLM\n",
    "\n",
    "# Please uncomment and use appropriate Cache Directory for transformers, in case you don't want to use default ~/.cache dir.\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/local/mnt/workspace/hf_cache\"\n",
    "\n",
    "# ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "# CACHE_DIR = os.path.join(ROOT_DIR, \"tmp\"), you can use a different location for just one model by passing this param as cache_dir in below API.\n",
    "\n",
    "# Model-Card name to be onboarded (This is HF Model Card name) : https://huggingface.co/gpt2-xl\n",
    "model_name = \"mosaicml/mpt-7b\"  # Similar, we can change model name and generate corresponding models, if we have added the support in the lib.\n",
    "qeff_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"{model_name} optimized for Cloud AI 100 \\n\", qeff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596ae75-31d5-4837-8a2d-e404d02e3269",
   "metadata": {},
   "source": [
    "##### Export the Optimized Pytorch model to the Onnx Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642fb9c-3ec4-43d3-81d7-bd997846624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now export the modified models to ONNX framework\n",
    "# This will generate single ONNX Model for both Prefill and Decode Variations which are optimized for\n",
    "# Cloud AI 100 Platform.\n",
    "\n",
    "# While generating the ONNX model, this will clip the overflow constants to fp16\n",
    "# Verify the model on Onnxruntime vs Pytorch\n",
    "\n",
    "# Then generate inputs and customio yaml file required for compilation.\n",
    "qeff_model.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1795ac7-d92c-42bb-8148-cb9da87439a6",
   "metadata": {},
   "source": [
    "##### Compile the Optimized KV Cache Single Model on Cloud AI 100 (**Config; 16C;32PL;128CTX;MXFP6**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4d6dd-9973-4608-b68b-ec6825cfef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model for provided compilation arguments\n",
    "# Please use platform SDK to Check num_cores for your card.\n",
    "\n",
    "qeff_model.compile(num_cores=14, mxfp6_matmul=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa240c-f40b-4bf8-a982-8ffff4ff3978",
   "metadata": {},
   "source": [
    "##### Execute the Optimized KV Model on H/W and Print the Latency Stats *(tok/sec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711fc74-aa5d-4e20-af0e-0d461d2e19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post compilation, we can print the latency stats for the kv models, We provide API to print token and Latency stats on Cloud AI 100\n",
    "# We need the compiled prefill and decode qpc to compute the token generated, This is based on Greedy Sampling Approach\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "qeff_model.generate(prompts=[\"My name is\"], tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
