# -----------------------------------------------------------------------------
#
# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.
# SPDX-License-Identifier: BSD-3-Clause
#
# -----------------------------------------------------------------------------

# Model configuration
model:
  model_type: "hf"  # Hugging Face model
  auto_class_name: "AutoModelForCausalLM"
  model_name: "HuggingFaceTB/SmolLM-135M"  # Pretrained model name
  use_peft: true
  peft_config:
    lora_r: 8
    lora_alpha: 16
    target_modules: ["q_proj", "v_proj"]
    task_type: "CAUSAL_LM"  # Options: CAUSAL_LM, SEQ_2_SEQ_LM, etc.
    peft_type: "LORA"  # Options: LORA, IA3, etc.

# Dataset configuration
dataset:
  dataset_type: "seq_completion"
  dataset_name: "knkarthick/samsum"
  prompt_template: "Summarize the following conversation:\n\n{'dialogue'}\n\nSummary:\n"
  completion_template: "{summary}"
  train_split: "train"

# Training configuration
training:
  type: "sft"
  output_dir: "./training_results"
  eval_strategy: "epoch"
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  max_steps: -1
  save_strategy: "epoch"
  torch_dtype: "fp16"

  # Uncomment if running in Notebook
  # disable_tqdm: True

  # Uncomment and populate to resume training
  # resume_from_checkpoint: "./abc"
  # restore_callback_states_from_checkpoint: True

  gradient_checkpointing: False
  torch_compile: True

# Optimizer configuration
optimizers:
  optimizer_name: "adamw"
  lr: 5e-5
  weight_decay: 0.01

scheduler:
  scheduler_name: "cosine"
  warmup_steps: 100   # warmup_steps or warmup_ratio

callbacks:
  early_stopping:
    early_stopping_patience: 3
    early_stopping_threshold: 0.001
  tensorboard:
