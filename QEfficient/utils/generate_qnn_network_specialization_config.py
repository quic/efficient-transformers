# -----------------------------------------------------------------------------
#
# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
#
# -----------------------------------------------------------------------------

import json
from typing import Optional

import onnx
import yaml
from onnx import helper

"""
    The network specilization file is generated by loading the onnx graph and fecthing the graph inputs and outputs.
"""


def fetch_nodes_info(
    onnx_graph_path: str,
    batch_size: int,
    sequence_length: int,
    context_length: int,
    file_path: str = "custom_io_config.yaml",
    full_batch_size: Optional[int] = None,
    kv_precision: Optional[str] = "float16",
    kv_cache_batch_size: Optional[int] = None,
) -> None:
    """
    Generates network specialization config custom IO file for convertor stage in QNN compilation.
    Reads onnx graph and creates a custom IO configuration file according to the passed parameters and
    save it as a yaml file provided in file_path argument.

    ``Mandatory`` Args:
        :onnx_graph_path (str): Generated ``ONNX`` Model Path.
        :batch_size (int): Batch size to compile the model for.
        :sequence_length (int): Sequence length for the model to compile.
        :context_length (int): Maximum context length to compile the model.

    ``Optional`` Args:
        :file_path (str): File path to save the generated custom IO config. ``Defaults to custom_io_config.yaml.``
        :full_batch_size (int): Set full batch size to enable continuous batching mode. ``Default to None``
        :kv_precision (str): Sets kv precision for compilation.  ``Defaults to float16.``
        :kv_cache_batch_size (int): kv_cache_batch_size for Prefix Caching. ``Defaults to None.``
    """

    # Load the ONNX model
    onnx_model = onnx.load(onnx_graph_path)

    input_nodes = []
    input_nodes_info = []
    final_dict = {}
    output_nodes = []
    output_nodes_info = []
    for node in onnx_model.graph.input:
        input_nodes.append(node.name)
        input_info = {}
        input_info["DataType"] = str(helper.tensor_dtype_to_np_dtype(node.type.tensor_type.elem_type))
        if "past_key" in node.name or "past_value" in node.name:
            input_info["DataType"] = kv_precision

        if "batch_index" in node.name:
            if full_batch_size:
                input_info["Shape"] = f"(1, 1), ({full_batch_size}, 1)"
            else:
                raise AttributeError(
                    "ERROR: Full batch size is required for populating batch_index in custom_io_config.yaml"
                )
        else:
            shapes = []
            for input_shape in node.type.tensor_type.shape.dim:
                if input_shape.HasField("dim_value"):
                    shape = input_shape.dim_value
                elif input_shape.HasField("dim_param"):
                    shape = input_shape.dim_param
                else:
                    shape = "shape_not_found"
                shapes.append(shape)

            if (
                ("batch_size" in shapes or "full_batch_size" in shapes)
                and ("ctx_len" in shapes or "max_context_len" in shapes)
                and len(shapes) >= 3
            ):
                shapeList = []
                for shape in shapes:
                    if isinstance(shape, str):
                        if "full_batch_size" in shape:
                            if ("past_key" in node.name or "past_value" in node.name) and kv_cache_batch_size:
                                shapeList.append(kv_cache_batch_size)
                            elif full_batch_size:
                                shapeList.append(full_batch_size)
                            else:
                                raise AttributeError(
                                    "ERROR: Full batch size is required to generate custom_io_config.yaml"
                                )
                        elif "batch_size" in shape:
                            shapeList.append(batch_size)
                        elif shape in ["ctx_len", "max_context_len"]:
                            shapeList.append(context_length)
                    else:
                        shapeList.append(shape)
                shape = str(shapeList).replace("[", "(").replace("]", ")")
            elif "batch_size" in shapes and ("seq_len" in shapes or "prompt_len" in shapes):
                shape_1 = (
                    str(
                        [
                            batch_size if isinstance(shape, str) and "batch_size" in shape else sequence_length
                            for shape in shapes
                        ]
                    )
                    .replace("[", "(")
                    .replace("]", ")")
                )
                if full_batch_size:
                    shape_2 = (
                        str(
                            [
                                full_batch_size if isinstance(shape, str) and "batch_size" in shape else 1
                                for shape in shapes
                            ]
                        )
                        .replace("[", "(")
                        .replace("]", ")")
                    )
                else:
                    shape_2 = (
                        str([batch_size if isinstance(shape, str) and "batch_size" in shape else 1 for shape in shapes])
                        .replace("[", "(")
                        .replace("]", ")")
                    )
                shape = shape_1 + "," + shape_2
            elif ("batch_size" in shapes or "full_batch_size" in shapes) and (
                "ctx_len" in shapes or "max_context_len" in shapes
            ):
                shape = (
                    str(
                        [
                            batch_size if isinstance(shape, str) and "batch_size" in shape else context_length
                            for shape in shapes
                        ]
                    )
                    .replace("[", "(")
                    .replace("]", ")")
                )
            input_info["Shape"] = shape
        input_nodes_info.append({"Name": node.name, "Desired Model Parameters": input_info})

    # Prepare output tensor configuration
    for output in onnx_model.graph.output:
        output_nodes.append(output.name)
        output_info = {}
        output_info["DataType"] = str(helper.tensor_dtype_to_np_dtype(output.type.tensor_type.elem_type))
        if "past_key" in output.name or "past_value" in output.name:
            output_info["DataType"] = kv_precision
        elif "logits" in output.name:
            output_info["DataType"] = "float32"
        output_nodes_info.append({"Name": output.name, "Desired Model Parameters": output_info})

    # Combine input and output configurations
    final_dict = {"Input Tensor Configuration": input_nodes_info, "Output Tensor Configuration": output_nodes_info}

    # Save the configuration to a YAML file
    try:
        with open(file_path, "w") as yaml_file:
            yaml.dump(final_dict, yaml_file, default_flow_style=False, sort_keys=False)
    except Exception as e:
        print(f"Failed to create YAML File for QNN Network Specialization Configuration{file_path}: {e}")


def generate_data_format_config(
    onnx_graph_path: str,
    *,
    data_format: Optional[str] = "QNN_TENSOR_DATA_FORMAT_MX",
    model_dlc_name: Optional[str] = "model",
    file_path: str = "qnn_data_format_config.json",
) -> None:
    """
    Generates data format config for context binary generation stage in QNN compilation path.
    It defines the tensor format for KV nodes when precision is set to mxint8.
    Reads onnx graph and creates a data format configuration file and save it as a json file provided in
    file_path argument.

    ``Mandatory`` Args:
        :onnx_graph_path (str): Generated ``ONNX`` Model Path.

    ``Optional`` Args:
        :data_format (str): Tensor format for KV nodes. ``Defaults to QNN_TENSOR_DATA_FORMAT_MX.``
        :model_dlc_name (str): DLC Name generated by the convertor stage in QNN Compilation. ``Defaults to model.``
        :file_path (str): File path to save the generated data format config. ``Defaults to qnn_data_format_config.json.``
    """

    # Load the ONNX model
    onnx_model = onnx.load(onnx_graph_path)

    kv_nodes: list = []

    for input in onnx_model.graph.input:
        if "past_key" in input.name or "past_value" in input.name:
            kv_nodes.append((input.name).replace(".", "_"))
    for output in onnx_model.graph.output:
        if "past_key" in output.name or "past_value" in output.name:
            kv_nodes.append((output.name).replace(".", "_"))
            kv_overrides = {}

    kv_overrides["graphs"] = [
        {
            "graph_name": model_dlc_name + "_configuration_1",
            "tensors": [{"tensor_name": node, "dataFormat": data_format} for node in kv_nodes],
        },
        {
            "graph_name": model_dlc_name + "_configuration_2",
            "tensors": [{"tensor_name": node, "dataFormat": data_format} for node in kv_nodes],
        },
    ]

    try:
        with open(file_path, "w") as json_file:
            json.dump(kv_overrides, json_file, indent=4)
    except Exception as e:
        print(f"Failed to create JSON File for QNN Data Format Configuration{file_path}: {e}")
