# -----------------------------------------------------------------------------
#
# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.
# SPDX-License-Identifier: BSD-3-Clause
#
# -----------------------------------------------------------------------------

import json
import os
from importlib import reload
from typing import List, Optional

import numpy as np
import onnx
import onnxruntime
import pytest
import torch
import transformers
from datasets import load_dataset
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor

from QEfficient.transformers.models.modeling_auto import QEFFAutoModelForSpeechSeq2Seq
from QEfficient.transformers.quantizers.auto import replace_transformers_quantizers
from QEfficient.utils import get_padding_shape_from_config, hf_download
from QEfficient.utils._utils import create_json, load_hf_processor
from QEfficient.utils.constants import Constants, QnnConstants
from QEfficient.utils.device_utils import get_available_device_id

CONFIG_PATH = "tests/configs/speech_seq2seq_model_configs.json"

with open(CONFIG_PATH, "r") as f:
    config_data = json.load(f)
    test_models = config_data["speech_seq2seq_models"]


def load_seq2seq_model(model_config):
    """
    Function to load model from huggingface and transform to KV model
    --------

    :model_config: Dict

    :return model_hf, params
    """
    model_path = hf_download(
        repo_id=model_config["model_name"],
        ignore_patterns=["*.onnx", "*.ot", "*.md", "*.tflite", "*.pdf", "*.h5", "*.msgpack"],
    )
    model_hf = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_path,
        use_cache=True,
        num_hidden_layers=model_config["n_layer"],
        attn_implementation="eager",
        low_cpu_mem_usage=False,
    )  # Run models for single layers only
    params = sum(p.numel() for p in model_hf.parameters())
    model_hf.eval()
    return model_hf, params


def run_seq2seq_pytorch_hf(
    model, processor: AutoProcessor, inputs: np.ndarray, sample_rate: int, generation_len: int
) -> List[str]:
    """
    Run pytorch inference on model

    ``Mandatory`` Args:
        :model: The transformed PyTorch model used for generating transcripts
        :processor: autoprocessor to process inputs and decode logits
        :inputs (np.ndarray): inputs to run the execution.
        :sample_rate (int): sampling rate at which input audio is stored in inputs (needed for processor)
        :generation_len (int): length upto which to generate

    Returns:
        torch.Tensor: A list of output features generated by the model for each prompt.
    """
    seq_len = 1
    batch_size = 1

    # prepare inputs
    input_features = processor(inputs, sampling_rate=sample_rate, return_tensors="pt").input_features
    decoder_input_ids = torch.ones((batch_size, seq_len), dtype=torch.int64) * model.config.decoder_start_token_id
    decoder_position_ids = torch.arange(seq_len, dtype=torch.int64).view(1, seq_len).repeat(batch_size, 1)

    model_inputs = dict(
        input_features=input_features,
        decoder_input_ids=decoder_input_ids,
        decoder_position_ids=decoder_position_ids,
    )

    # TODO: temporary hack to nullify effect of KVCacheTransform add this as setup_module in pytest
    reload(transformers.cache_utils)
    # encoder run
    outputs = model(**model_inputs)

    # array to hold generated tokens
    generated_ids = np.full((batch_size, generation_len + 1), processor.tokenizer.pad_token_id)
    generated_ids[:, 0] = [model.config.decoder_start_token_id]
    logits = outputs["logits"]
    next_token = logits.argmax(-1)
    generated_ids[:, 1] = next_token.squeeze(1)

    model_inputs["encoder_outputs"] = outputs["encoder_last_hidden_state"]
    model_inputs["past_key_values"] = outputs["past_key_values"]

    for num_tokens in range(generation_len):
        outputs = model(**model_inputs)
        logits = outputs["logits"]
        next_token = logits.argmax(-1)
        generated_ids[:, num_tokens + 1] = next_token.squeeze(1)

        if next_token[0][0] == processor.tokenizer.eos_token_id:
            break

        model_inputs["decoder_input_ids"] = next_token
        model_inputs["decoder_position_ids"] += 1
        model_inputs["past_key_values"] = outputs["past_key_values"]

    return generated_ids[0]


def run_seq2seq_pytorch_with_kv(
    model, processor: AutoProcessor, inputs: np.ndarray, sample_rate: int, generation_len: int
) -> List[str]:
    """
    Run pytorch inference on model

    ``Mandatory`` Args:
        :model: The transformed PyTorch model used for generating transcripts
        :processor: autoprocessor to process inputs and decode logits
        :inputs (np.ndarray): inputs to run the execution.
        :sample_rate (int): sampling rate at which input audio is stored in inputs (needed for processor)
        :generation_len (int): length upto which to generate

    Returns:
        torch.Tensor: A list of output features generated by the model for each prompt.
    """
    seq_len = 1
    batch_size = 1
    config = model.model.config

    # prepare inputs
    input_features = processor(inputs, sampling_rate=sample_rate, return_tensors="pt").input_features
    decoder_input_ids = torch.ones((batch_size, seq_len), dtype=torch.int64) * config.decoder_start_token_id
    decoder_position_ids = torch.arange(seq_len, dtype=torch.int64).view(1, seq_len).repeat(batch_size, 1)

    model_inputs = dict(
        input_features=input_features,
        input_ids=decoder_input_ids,
        position_ids=decoder_position_ids,
        past_key_values=[[] for _ in range(config.num_hidden_layers)],
    )

    # prepare dummy past kvs and cross kvs
    kv_cache_shape = get_padding_shape_from_config(config, batch_size, generation_len)
    kv_cross_cache_shape = get_padding_shape_from_config(config, batch_size, config.max_source_positions)

    for i in range(config.num_hidden_layers):
        for self_cross in ["self", "cross"]:
            for kv in ["key", "value"]:
                model_inputs["past_key_values"][i].append(
                    torch.zeros(kv_cache_shape if self_cross == "self" else kv_cross_cache_shape, dtype=torch.float32)
                )

    # encoder run
    outputs = model.model(**model_inputs)

    # array to hold generated tokens
    generated_ids = np.full((batch_size, generation_len + 1), processor.tokenizer.pad_token_id)
    generated_ids[:, 0] = [config.decoder_start_token_id]
    logits = outputs["logits"]
    next_token = logits.argmax(-1)
    generated_ids[:, 1] = next_token.squeeze(1)

    model_inputs["input_features"] = torch.tensor(np.zeros((batch_size, config.num_mel_bins, 1)).astype(np.float32))
    model_inputs["past_key_values"] = outputs["past_key_values"]

    for num_tokens in range(generation_len):
        outputs = model.model(**model_inputs)
        logits = outputs["logits"]
        next_token = logits.argmax(-1)
        generated_ids[:, num_tokens + 1] = next_token.squeeze(1)

        if next_token[0][0] == processor.tokenizer.eos_token_id:
            break

        model_inputs["input_ids"] = next_token
        model_inputs["position_ids"] += 1
        model_inputs["past_key_values"] = outputs["past_key_values"]

    return generated_ids[0]


def run_seq2seq_ort(
    onnx_path, config, processor: AutoProcessor, inputs: np.ndarray, sample_rate: int, generation_len: int
) -> List[str]:
    """
    Run onnxruntime inference on model

    ``Mandatory`` Args:
        :model: The transformed PyTorch model used for generating transcripts
        :processor: autoprocessor to process inputs and decode logits
        :inputs (np.ndarray): inputs to run the execution.
        :sample_rate (int): sampling rate at which input audio is stored in inputs (needed for processor)
        :generation_len (int): length upto which to generate

    Returns:
        torch.Tensor: A list of output features generated by the model for each prompt.
    """
    seq_len = 1
    batch_size = 1

    # Replace invalid index value for INT32 max to 0 using add_initializer
    m = onnx.load(onnx_path, load_external_data=False)
    # NOTE: OrtValue objects should be kept around until the session is run, hence this dict is required
    added_initializers = {}
    for node in m.graph.node:
        if node.op_type == "Constant":
            np_tensor = onnx.numpy_helper.to_array(node.attribute[0].t, os.path.dirname(onnx_path))
            if len(np_tensor.shape) == 0 and np_tensor.item() == 2147483647:
                added_initializers[node.output[0]] = onnxruntime.OrtValue.ortvalue_from_numpy(
                    np.array(0, np_tensor.dtype)
                )

    session_options = onnxruntime.SessionOptions()
    for name, value in added_initializers.items():
        session_options.add_initializer(name, value)

    session = onnxruntime.InferenceSession(onnx_path, session_options)

    # prepare inputs
    input_features = processor(inputs, sampling_rate=sample_rate, return_tensors="pt").input_features
    decoder_input_ids = torch.ones((batch_size, seq_len), dtype=torch.int64) * config.decoder_start_token_id
    decoder_position_ids = torch.arange(seq_len, dtype=torch.int64).view(1, seq_len).repeat(batch_size, 1)

    model_inputs = dict(
        input_features=input_features,
        input_ids=decoder_input_ids,
        position_ids=decoder_position_ids,
    )

    # prepare dummy past kvs and cross kvs
    kv_cache_shape = get_padding_shape_from_config(config, batch_size, generation_len)
    kv_cross_cache_shape = get_padding_shape_from_config(config, batch_size, config.max_source_positions)

    pkv_names = []
    for i in range(config.num_hidden_layers):
        for self_cross in ["self", "cross"]:
            for kv in ["key", "value"]:
                pkv_names.append(f"past_{kv}_{self_cross}.{i}_RetainedState")
                model_inputs[f"past_{kv}_{self_cross}.{i}"] = torch.zeros(
                    kv_cache_shape if self_cross == "self" else kv_cross_cache_shape, dtype=torch.float32
                )

    output_names = ["logits"] + pkv_names

    # encoder run
    outputs = session.run(output_names, {k: v.detach().numpy() for k, v in model_inputs.items()})

    # array to hold generated tokens
    generated_ids = np.full((batch_size, generation_len + 1), processor.tokenizer.pad_token_id)
    generated_ids[:, 0] = [config.decoder_start_token_id]
    logits = outputs[0]
    next_token = logits.argmax(-1)
    generated_ids[:, 1] = next_token.squeeze(1)

    model_inputs["input_features"] = torch.tensor(np.zeros((batch_size, config.num_mel_bins, 1)).astype(np.float32))
    for i, name in enumerate(pkv_names):
        model_inputs[name.split("_RetainedState")[0]] = outputs[1 + i]

    for num_tokens in range(generation_len):
        outputs = session.run(
            output_names, {k: (v.detach().numpy() if type(v) is torch.Tensor else v) for k, v in model_inputs.items()}
        )
        logits = outputs[0]
        next_token = logits.argmax(-1)
        generated_ids[:, num_tokens + 1] = next_token.squeeze(1)

        if next_token[0][0] == processor.tokenizer.eos_token_id:
            break

        model_inputs["input_ids"] = next_token
        model_inputs["position_ids"] += 1
        for i, name in enumerate(pkv_names):
            model_inputs[name.split("_RetainedState")[0]] = outputs[1 + i]

    return generated_ids[0]


def check_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100(
    model_name: str,
    ctx_len: int = Constants.CTX_LEN,
    n_layer: int = 1,
    enable_qnn: Optional[bool] = False,
    qnn_config: Optional[str] = None,
):
    """
    Validate the PyTorch model, the PyTorch model after KV changes, ONNX model and the Cloud AI 100 model
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``whisper``
        :ctx_len (int): Maximum context length to compile the model.
        :n_layers (int): Number of layers for the Model.
    """
    replace_transformers_quantizers()
    model_config = {"model_name": model_name}
    model_config["n_layer"] = n_layer

    model_hf, _ = load_seq2seq_model(model_config)

    processor = load_hf_processor(pretrained_model_name_or_path=model_name)
    batch_size = 1

    ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
    data = ds[0]["audio"]["array"]
    data = data.reshape(-1)
    sample_rate = ds[0]["audio"]["sampling_rate"]

    pytorch_hf_tokens = run_seq2seq_pytorch_hf(model_hf, processor, data, sample_rate, ctx_len)

    qeff_model = QEFFAutoModelForSpeechSeq2Seq(model_hf, pretrained_model_name_or_path=model_name)

    pytorch_kv_tokens = run_seq2seq_pytorch_with_kv(qeff_model, processor, data, sample_rate, ctx_len)

    assert (pytorch_hf_tokens == pytorch_kv_tokens).all(), (
        "Tokens don't match for HF PyTorch model output and KV PyTorch model output"
    )

    qeff_model.export()

    ort_tokens = run_seq2seq_ort(qeff_model.onnx_path, qeff_model.model.config, processor, data, sample_rate, ctx_len)

    assert (pytorch_kv_tokens == ort_tokens).all(), "Tokens don't match for pytorch output and ort output"

    if not get_available_device_id():
        pytest.skip("No available devices to run model on Cloud AI 100")

    qeff_model.compile(
        ctx_len=ctx_len,
        num_cores=16,
        batch_size=batch_size,
        enable_qnn=enable_qnn,
        qnn_config=qnn_config,
    )

    exec_info = qeff_model.generate(
        inputs=processor(data, sampling_rate=sample_rate, return_tensors="pt"), generation_len=ctx_len
    )
    cloud_ai_100_tokens = exec_info.generated_ids[0]  # Because we always run for single input and single batch size
    assert (pytorch_kv_tokens == cloud_ai_100_tokens).all(), (
        "Tokens don't match for pytorch output and Cloud AI 100 output."
    )
    assert os.path.isfile(os.path.join(os.path.dirname(qeff_model.qpc_path), "qconfig.json"))


@pytest.mark.on_qaic
@pytest.mark.llm_model
@pytest.mark.parametrize("model_name", test_models)
def test_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100(model_name):
    """
    Test function to validate the PyTorch model, the PyTorch model after KV changes, the ONNX model, and the Cloud AI 100 model, both with and without continuous batching.
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``gpt2``
    """
    check_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100(model_name=model_name, n_layer=4)


@pytest.mark.on_qaic
@pytest.mark.llm_model
@pytest.mark.qnn
@pytest.mark.skip(reason="Whisper is currently not supported on QNN")
@pytest.mark.parametrize("model_name", test_models)
def test_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100_qnn(model_name):
    """
    QNN Compilation path test.
    Test function to validate the PyTorch model, the PyTorch model after KV changes, the ONNX model, and the Cloud AI 100 model, both with and without continuous batching.
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``gpt2``
    """
    qnn_config_json_path = os.path.join(os.getcwd(), "qnn_config.json")
    create_json(qnn_config_json_path, QnnConstants.QNN_SAMPLE_CONFIG)

    check_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100(
        model_name=model_name, n_layer=4, enable_qnn=True, qnn_config=qnn_config_json_path
    )
