# -----------------------------------------------------------------------------
#
# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.
# SPDX-License-Identifier: BSD-3-Clause
#
# -----------------------------------------------------------------------------

import json
import os
from typing import List, Optional

import numpy as np
import onnx
import onnxruntime
import pytest
import torch
from datasets import load_dataset
from transformers import AutoModelForCTC, AutoProcessor

from QEfficient.transformers.models.modeling_auto import QEFFAutoModelForCTC
from QEfficient.transformers.quantizers.auto import replace_transformers_quantizers
from QEfficient.utils import hf_download
from QEfficient.utils._utils import create_json, load_hf_processor
from QEfficient.utils.constants import WAV2VEC2_MAX_SEQ_LEN, QnnConstants
from QEfficient.utils.device_utils import get_available_device_id

CONFIG_PATH = "tests/configs/embedding_model_configs.json"

with open(CONFIG_PATH, "r") as f:
    config_data = json.load(f)
    test_models = config_data["audio_embedding_models"]


def load_ctc_model(model_config):
    """
    Function to load model from huggingface
    --------

    :model_config: Dict

    :return model_hf, params
    """
    model_path = hf_download(
        repo_id=model_config["model_name"],
        ignore_patterns=["*.onnx", "*.ot", "*.md", "*.tflite", "*.pdf", "*.h5", "*.msgpack"],
    )
    model_hf = AutoModelForCTC.from_pretrained(
        model_path,
        attn_implementation="eager",
        low_cpu_mem_usage=False,
    )  # Run models for single layers only
    params = sum(p.numel() for p in model_hf.parameters())
    model_hf.eval()
    return model_hf, params


def run_ctc_pytorch_hf(model, processor: AutoProcessor, inputs: np.ndarray, sample_rate: int) -> List[str]:
    """
    Run pytorch inference on model

    ``Mandatory`` Args:
        :model: The transformed PyTorch model used for generating transcripts
        :processor: autoprocessor to process inputs and decode logits
        :inputs (np.ndarray): inputs to run the execution.
        :sample_rate (int): the sample rate for the audio file



    Returns:
        torch.Tensor: A list of output features generated by the model for each prompt.
    """
    seq_len = WAV2VEC2_MAX_SEQ_LEN

    # prepare inputs
    input_features = processor(
        inputs[0], return_tensors="pt", max_length=seq_len, truncating=True, padding="max_length"
    ).input_values

    model_inputs = dict(
        input_values=input_features,
    )
    outputs = torch.tensor(model(**model_inputs).logits)
    return outputs


def run_ctc_ort(onnx_path, config, processor: AutoProcessor, inputs: np.ndarray, sample_rate: int) -> List[str]:
    """
    Run onnxruntime inference on model

    ``Mandatory`` Args:
        :model: The transformed PyTorch model used for generating transcripts
        :processor: autoprocessor to process inputs and decode logits
        :inputs (np.ndarray): inputs to run the execution.
        :sample_rate (int): sampling rate at which input audio is stored in inputs (needed for processor)

    Returns:
        torch.Tensor: A list of output features generated by the model for each prompt.
    """
    seq_len = 480000

    # Replace invalid index value for INT32 max to 0 using add_initializer
    m = onnx.load(onnx_path, load_external_data=False)
    # NOTE: OrtValue objects should be kept around until the session is run, hence this dict is required
    added_initializers = {}
    for node in m.graph.node:
        if node.op_type == "Constant":
            np_tensor = onnx.numpy_helper.to_array(node.attribute[0].t, os.path.dirname(onnx_path))
            if len(np_tensor.shape) == 0 and np_tensor.item() == 2147483647:
                added_initializers[node.output[0]] = onnxruntime.OrtValue.ortvalue_from_numpy(
                    np.array(0, np_tensor.dtype)
                )

    session_options = onnxruntime.SessionOptions()
    for name, value in added_initializers.items():
        session_options.add_initializer(name, value)

    session = onnxruntime.InferenceSession(onnx_path, session_options)

    # prepare inputs
    input_features = processor(
        inputs[0], return_tensors="pt", max_length=seq_len, truncation=True, padding="max_length"
    ).input_values

    model_inputs = dict(input_values=(input_features).numpy())
    outputs = session.run(None, model_inputs)
    logits = torch.tensor(outputs[0])
    return logits


def check_ctc_pytorch_vs_kv_vs_ort_vs_ai100(
    model_name: str,
    n_layer: int = 1,
    enable_qnn: Optional[bool] = False,
    qnn_config: Optional[str] = None,
):
    """
    Validate the PyTorch model, the PyTorch model after ONNX model and the Cloud AI 100 model
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``whisper``
        :n_layers (int): Number of layers for the Model.
    """
    replace_transformers_quantizers()
    model_config = {"model_name": model_name}
    model_config["n_layer"] = n_layer

    model_hf, _ = load_ctc_model(model_config)

    processor = load_hf_processor(pretrained_model_name_or_path=model_name)
    batch_size = 1

    ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
    data = ds[0]["audio"]["array"]
    data = torch.tensor(data).unsqueeze(0).numpy()
    sample_rate = ds[0]["audio"]["sampling_rate"]
    pytorch_tokens = run_ctc_pytorch_hf(model_hf, processor, data, sample_rate)
    predicted_ids = torch.argmax(pytorch_tokens, dim=-1)
    pytorch_output = processor.batch_decode(predicted_ids)

    qeff_model = QEFFAutoModelForCTC(model_hf, pretrained_model_name_or_path=model_name)
    qeff_model.export()
    ort_tokens = run_ctc_ort(qeff_model.onnx_path, qeff_model.model.config, processor, data, sample_rate)
    predicted_ids = torch.argmax(ort_tokens, dim=-1)
    ort_output = processor.batch_decode(predicted_ids)
    assert pytorch_output == ort_output, "Tokens don't match for pytorch output and ORT output."
    if not get_available_device_id():
        pytest.skip("No available devices to run model on Cloud AI 100")
    qeff_model.compile(
        num_cores=16,
        batch_size=batch_size,
        enable_qnn=enable_qnn,
        qnn_config=qnn_config,
    )
    cloud_ai_100_output = qeff_model.generate(processor, data)
    assert pytorch_output == cloud_ai_100_output, "Tokens don't match for pytorch output and Cloud AI 100 output."
    assert os.path.isfile(os.path.join(os.path.dirname(qeff_model.qpc_path), "qconfig.json"))


@pytest.mark.on_qaic
@pytest.mark.llm_model
@pytest.mark.parametrize("model_name", test_models)
def test_ctc_pytorch_vs_kv_vs_ort_vs_ai100(model_name):
    """
    Test function to validate the PyTorch model, the PyTorch model the ONNX model, and the Cloud AI 100 model.
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``gpt2``
    """
    check_ctc_pytorch_vs_kv_vs_ort_vs_ai100(model_name=model_name, n_layer=4)


@pytest.mark.on_qaic
@pytest.mark.llm_model
@pytest.mark.qnn
@pytest.mark.skip(reason="Wav2Vec2 is currently not supported on QNN")
@pytest.mark.parametrize("model_name", test_models)
def test_ctc_pytorch_vs_kv_vs_ort_vs_ai100_qnn(model_name):
    """
    QNN Compilation path test.
    Test function to validate the PyTorch model, the PyTorch model after the ONNX model, and the Cloud AI 100 model.
    ``Mandatory`` Args:
        :model_name (str): Hugging Face Model Card name, Example: ``gpt2``
    """
    qnn_config_json_path = os.path.join(os.getcwd(), "qnn_config.json")
    create_json(qnn_config_json_path, QnnConstants.QNN_SAMPLE_CONFIG)

    check_ctc_pytorch_vs_kv_vs_ort_vs_ai100(
        model_name=model_name, n_layer=4, enable_qnn=True, qnn_config=qnn_config_json_path
    )
