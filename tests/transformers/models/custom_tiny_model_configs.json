[
  {
    "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "model_type": "llama",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32000,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "gpt2",
    "model_type": "gpt2",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 50257,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "Salesforce/codegen-350M-mono",
    "model_type": "codegen",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 4,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 51200,
      "num_key_value_heads": 1,
      "rotary_dim": 16
    }
  },
  
  {
    "model_name": "microsoft/Phi-3-mini-4k-instruct",
    "model_type": "phi3",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32064,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "tiiuae/falcon-7b",
    "model_type": "falcon",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 65024,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
    "model_type": "qwen3_moe",
    "additional_params": {
      "hidden_size": 256,
      "intermediate_size": 256,
      "max_position_embeddings": 128,
      "max_window_layers": 48,
      "moe_intermediate_size": 768,
      "num_attention_heads": 2,
      "num_experts": 4,
      "num_experts_per_tok": 2,
      "num_hidden_layers": 1,
      "num_key_value_heads": 1,
      "vocab_size": 151936
    }
  },
  {
    "model_name": "Qwen/Qwen2-0.5B",
    "model_type": "qwen2",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 151936,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "bigcode/starcoder2-3b",
    "model_type": "starcoder2",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 49152,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "Felladrin/Minueza-32M-Base",
    "model_type": "mistral",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32002,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "wtang06/mpt-125m-c4",
    "model_type": "mpt",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 50368
    }
  },
  {
    "model_name": "hakurei/gpt-j-random-tinier",
    "model_type": "gptj",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 50400,
      "num_key_value_heads": 1,
      "rotary_dim": 16
    }
  },
  {
    "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "model_type": "mixtral",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32000,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "meta-llama/Llama-3.2-1B",
    "model_type": "llama",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 128256,
      "num_key_value_heads": 1,
      "rope_scaling": {
        "factor": 32.0,
        "high_freq_factor": 4.0,
        "low_freq_factor": 1.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
      }
    }
  },
  {
    "model_name": "unsloth/gemma-2b",
    "model_type": "gemma",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 256000,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "unsloth/gemma-2-2b",
    "model_type": "gemma2",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 256000,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ",
    "model_type": "llama",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32003
    }
  },
  {
    "model_name": "TheBloke/Llama-2-7B-GPTQ",
    "model_type": "llama",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 32000
    }
  },
  {
    "model_name": "ibm-granite/granite-20b-code-base",
    "model_type": "gpt_bigcode",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 49152,
      "num_key_value_heads": 1,
      "activation_function": "gelu",
      "architectures": [
        "GPTBigCodeForCausalLM"
      ]
    }
  },
  {
    "model_name": "neuralmagic/Llama-3.2-3B-Instruct-FP8",
    "model_type": "llama",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 128256
    }
  },
  {
    "model_name": "neuralmagic/Qwen2-0.5B-Instruct-FP8",
    "model_type": "qwen2",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 151936
    }
  },
  {
    "model_name": "ibm-granite/granite-3.1-2b-instruct",
    "model_type": "granite",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 49155,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "ibm-granite/granite-guardian-3.1-2b",
    "model_type": "granite",
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 49155,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "hpcai-tech/grok-1",
    "model_type": null,
    "additional_params":{
      "max_position_embeddings": 128,
      "num_hidden_layers": 1,
      "num_attention_heads": 2,
      "hidden_size": 64,
      "intermediate_size": 256,
      "vocab_size": 131072,
      "num_key_value_heads": 1
    }
  },
  {
    "model_name": "Snowflake/Llama-3.1-SwiftKV-8B-Instruct",
    "model_type": null,
    "additional_params": {
      "max_position_embeddings": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 2,
      "hidden_size": 256,
      "intermediate_size": 256,
      "vocab_size": 128256,
      "num_key_value_layers": 1,
      "num_key_value_heads": 1,
      "rope_scaling": {
      "factor": 8.0,
      "high_freq_factor": 4.0,
      "low_freq_factor": 1.0,
      "original_max_position_embeddings": 8192,
      "rope_type": "llama3"
      }
    }
  }
]
