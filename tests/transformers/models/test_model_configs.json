{
  "causal_lm_models": [
    {
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "gpt2",
      "model_type": "gpt2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50257,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "allenai/OLMo-2-0425-1B",
      "model_type": "olmo2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 100352,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Salesforce/codegen-350M-mono",
      "model_type": "codegen",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 4,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 51200,
        "num_key_value_heads": 1,
        "rotary_dim": 16
      }
    },
    
    {
      "model_name": "microsoft/Phi-3-mini-4k-instruct",
      "model_type": "phi3",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32064,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "tiiuae/falcon-7b",
      "model_type": "falcon",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 65024,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "model_type": "qwen3_moe",
      "additional_params": {
        "hidden_size": 256,
        "intermediate_size": 256,
        "max_position_embeddings": 128,
        "max_window_layers": 48,
        "moe_intermediate_size": 768,
        "num_attention_heads": 2,
        "num_experts": 4,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 1,
        "num_key_value_heads": 1,
        "vocab_size": 151936
      }
    },
    {
      "model_name": "Qwen/Qwen2-0.5B",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "bigcode/starcoder2-3b",
      "model_type": "starcoder2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49152,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Felladrin/Minueza-32M-Base",
      "model_type": "mistral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32002,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "wtang06/mpt-125m-c4",
      "model_type": "mpt",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50368
      }
    },
    {
      "model_name": "hakurei/gpt-j-random-tinier",
      "model_type": "gptj",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50400,
        "num_key_value_heads": 1,
        "rotary_dim": 16
      }
    },
    {
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "model_type": "mixtral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "meta-llama/Llama-3.2-1B",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_heads": 1,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        }
      }
    },
    {
      "model_name": "unsloth/gemma-2b",
      "model_type": "gemma",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "unsloth/gemma-2-2b",
      "model_type": "gemma2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32003
      }
    },
    {
      "model_name": "TheBloke/Llama-2-7B-GPTQ",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000
      }
    },
    {
      "model_name": "ibm-granite/granite-20b-code-base",
      "model_type": "gpt_bigcode",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49152,
        "num_key_value_heads": 1,
        "activation_function": "gelu",
        "architectures": [
          "GPTBigCodeForCausalLM"
        ]
      }
    },
    {
      "model_name": "neuralmagic/Llama-3.2-3B-Instruct-FP8",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256
      }
    },
    {
      "model_name": "neuralmagic/Qwen2-0.5B-Instruct-FP8",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 2,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936
      }
    },
    {
      "model_name": "ibm-granite/granite-3.1-2b-instruct",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "ibm-granite/granite-guardian-3.1-2b",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "hpcai-tech/grok-1",
      "model_type": null,
      "additional_params":{
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 131072,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Snowflake/Llama-3.1-SwiftKV-8B-Instruct",
      "model_type": null,
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 2,
        "num_attention_heads": 2,
        "hidden_size": 256,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_layers": 1,
        "num_key_value_heads": 1,
        "rope_scaling": {
        "factor": 8.0,
        "high_freq_factor": 4.0,
        "low_freq_factor": 1.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
        }
      }
    }
  ],
  "multimodal_models": [
    {
      "model_name": "llava-hf/llava-1.5-7b-hf",
      "model_type": "llava",
      "batch_size": 1,
      "prompt_len": 784,
      "ctx_len": 1024,
      "img_size": 336,
      "img_url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg",
      "text_prompt": "What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud",
      "num_layers": 1,
      "additional_params": {}
    },
    {
      "model_name": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "model_type": "llama4",
      "batch_size": 1,
      "prompt_len": 32,
      "ctx_len": 3072,
      "img_size": 336,
      "img_url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg",
      "text_prompt": "What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud",
      "num_layers": 4,
      "additional_params": {}
    },
    {
      "model_name": "google/gemma-3-4b-it",
      "model_type": "gemma3",
      "batch_size": 1,
      "prompt_len": 128,
      "ctx_len": 3072,
      "img_size": 896,
      "img_url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png",
      "text_prompt": "Can you describe the image in detail.",
      "num_layers": 1,
      "additional_params": {}
    },
    {
      "model_name": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "model_type": "mistral3",
      "batch_size": 1,
      "prompt_len": 128,
      "ctx_len": 4096,
      "img_size": 1540,
      "img_url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png",
      "text_prompt": "Can you describe the image in detail.",
      "num_layers": 1,
      "additional_params": {}
    },
    {
      "model_name": "Qwen/Qwen2.5-VL-3B-Instruct",
      "model_type": "qwen2_5_vl",
      "batch_size": 1,
      "prompt_len": 128,
      "ctx_len": 4096,
      "img_size": 1540,
      "img_url": "https://picsum.photos/id/237/536/354",
      "text_prompt": "Can you describe the image in detail.",
      "num_layers": 1,
      "additional_params": {}
    },
    {
      "model_name": "allenai/Molmo-7B-D-0924",
      "model_type": "molmo",
      "batch_size": 1,
      "prompt_len": 128,
      "ctx_len": 4096,
      "img_size": null,
      "img_url": "https://picsum.photos/id/237/536/354",
      "text_prompt": "Can you describe the image in detail.",
      "num_layers": 2,
      "additional_params": {}
    },
    {
      "model_name": "OpenGVLab/InternVL2_5-1B",
      "model_type": "internvl_chat",
      "batch_size": 1,
      "prompt_len": 384,
      "ctx_len": 512,
      "img_size": null,
      "img_url": "https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-1-2048.jpg",
      "text_prompt": "Please describe the image in detail.",
      "num_layers": 2,
      "additional_params": {}
    },
    {
      "model_name": "OpenGVLab/InternVL3_5-1B",
      "model_type": "internvl_chat",
      "batch_size": 1,
      "prompt_len": 384,
      "ctx_len": 512,
      "img_size": null,
      "img_url": "https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-1-2048.jpg",
      "text_prompt": "Please describe the image in detail.",
      "num_layers": 2,
      "additional_params": {}
    },
    {
      "model_name": "meta-llama/Llama-3.2-11B-Vision-Instruct",
      "model_type": "mllama",
      "batch_size": 1,
      "prompt_len": 32,
      "ctx_len": 512,
      "img_size": 560,
      "img_url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg",
      "text_prompt": "Explain this image",
      "num_layers": 7,
      "additional_params": {}
    }

  ],
  "speech_seq2seq_models": [
    "openai/whisper-tiny"
  ],
  "embedding_models": [
    {"model_name": "jinaai/jina-embeddings-v2-base-code", "pooling": "mean"},
    {"model_name": "sentence-transformers/nli-bert-base-cls-pooling", "pooling": "cls"}
  ],
  "spd_causal_lm_models": [
    {
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Qwen/Qwen2-0.5B",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936,
        "num_key_value_heads": 1
      }
    }
  ],
  "qnn_causal_lm_models": [
    {
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "model_type": "mixtral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "meta-llama/Llama-3.2-1B",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_heads": 1,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        }
      }
    },
    {
      "model_name": "unsloth/gemma-2b",
      "model_type": "gemma",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "ibm-granite/granite-guardian-3.1-2b",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    }
  ],
  "prefix_caching_models": [
    {
      "model_name": "gpt2",
      "model_type": "gpt2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50257,
        "num_key_value_heads": 1
      }
    }
  ],
  "audio_embedding_models": [
    "facebook/wav2vec2-base-960h"
  ]
}
