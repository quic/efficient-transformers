{
  "causal_lm_models": [
    {
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "gpt2",
      "model_type": "gpt2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50257,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "allenai/OLMo-2-0425-1B",
      "model_type": "olmo2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 100352,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Salesforce/codegen-350M-mono",
      "model_type": "codegen",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 4,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 51200,
        "num_key_value_heads": 1,
        "rotary_dim": 16
      }
    },
    
    {
      "model_name": "microsoft/Phi-3-mini-4k-instruct",
      "model_type": "phi3",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32064,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "tiiuae/falcon-7b",
      "model_type": "falcon",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 65024,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "model_type": "qwen3_moe",
      "additional_params": {
        "hidden_size": 256,
        "intermediate_size": 256,
        "max_position_embeddings": 128,
        "max_window_layers": 48,
        "moe_intermediate_size": 768,
        "num_attention_heads": 2,
        "num_experts": 4,
        "num_experts_per_tok": 2,
        "num_hidden_layers": 1,
        "num_key_value_heads": 1,
        "vocab_size": 151936
      }
    },
    {
      "model_name": "Qwen/Qwen2-0.5B",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "bigcode/starcoder2-3b",
      "model_type": "starcoder2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49152,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Felladrin/Minueza-32M-Base",
      "model_type": "mistral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32002,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "wtang06/mpt-125m-c4",
      "model_type": "mpt",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50368
      }
    },
    {
      "model_name": "hakurei/gpt-j-random-tinier",
      "model_type": "gptj",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50400,
        "num_key_value_heads": 1,
        "rotary_dim": 16
      }
    },
    {
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "model_type": "mixtral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "meta-llama/Llama-3.2-1B",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_heads": 1,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        }
      }
    },
    {
      "model_name": "unsloth/gemma-2b",
      "model_type": "gemma",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "unsloth/gemma-2-2b",
      "model_type": "gemma2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32003
      }
    },
    {
      "model_name": "TheBloke/Llama-2-7B-GPTQ",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000
      }
    },
    {
      "model_name": "ibm-granite/granite-20b-code-base",
      "model_type": "gpt_bigcode",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49152,
        "num_key_value_heads": 1,
        "activation_function": "gelu",
        "architectures": [
          "GPTBigCodeForCausalLM"
        ]
      }
    },
    {
      "model_name": "neuralmagic/Llama-3.2-3B-Instruct-FP8",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256
      }
    },
    {
      "model_name": "neuralmagic/Qwen2-0.5B-Instruct-FP8",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 2,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936
      }
    },
    {
      "model_name": "ibm-granite/granite-3.1-2b-instruct",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "ibm-granite/granite-guardian-3.1-2b",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "hpcai-tech/grok-1",
      "model_type": null,
      "additional_params":{
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 131072,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Snowflake/Llama-3.1-SwiftKV-8B-Instruct",
      "model_type": null,
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 2,
        "num_attention_heads": 2,
        "hidden_size": 256,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_layers": 1,
        "num_key_value_heads": 1,
        "rope_scaling": {
        "factor": 8.0,
        "high_freq_factor": 4.0,
        "low_freq_factor": 1.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
        }
      }
    }
  ],

  "spd_causal_lm_models": [
    {
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "Qwen/Qwen2-0.5B",
      "model_type": "qwen2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 151936,
        "num_key_value_heads": 1
      }
    }
  ],
  
  "qnn_causal_lm_models": [
    {
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "model_type": "mixtral",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 32000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "meta-llama/Llama-3.2-1B",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_heads": 1,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        }
      }
    },
    {
      "model_name": "unsloth/gemma-2b",
      "model_type": "gemma",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 256000,
        "num_key_value_heads": 1
      }
    },
    {
      "model_name": "ibm-granite/granite-guardian-3.1-2b",
      "model_type": "granite",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 49155,
        "num_key_value_heads": 1
      }
    }
  ],
  
  "prefix_caching_models": [
    {
      "model_name": "gpt2",
      "model_type": "gpt2",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 50257,
        "num_key_value_heads": 1
      }
    }
  ],
  "blockedKV_causal_lm_models":[
    {
      "model_name": "meta-llama/Llama-3.2-1B",
      "model_type": "llama",
      "additional_params": {
        "max_position_embeddings": 128,
        "num_hidden_layers": 1,
        "num_attention_heads": 2,
        "hidden_size": 64,
        "intermediate_size": 256,
        "vocab_size": 128256,
        "num_key_value_heads": 1,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        }
      }
    }
  ]
}